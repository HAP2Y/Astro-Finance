{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "QzbljiVMGCEd",
    "cLtxVnm8GuPJ"
   ],
   "mount_file_id": "1YavSeQpGOvhpIqQnllxN3dF4t9j-QLZI",
   "authorship_tag": "ABX9TyPqlrkB5cu+aYwiOzZ0h5dB",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HAP2Y/Astro-Finance/blob/main/AstroFinanceProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "cell-0000"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PHASE 1 - \ud83d\udd2d Data Acquisition & Alignment"
   ],
   "metadata": {
    "id": "QzbljiVMGCEd"
   },
   "id": "cell-0001"
  },
  {
   "cell_type": "code",
   "source": "# Cell 1: Financial Data Acquisition (Phase 1 - Part 1 of 3)\n# ================================================================\n\nimport logging\nimport sys\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 1: FINANCIAL DATA ACQUISITION\")\nlogger.info(\"Phase 1 Progress: Part 1 of 3 (Financial Data)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Install Required Libraries\n# ============================================================================\nlogger.info(\"\\n[Installing Libraries]\")\nlogger.info(\"  \u2192 Installing yfinance, tabulate, pyswisseph...\")\n\n!pip install -q yfinance tabulate pyswisseph\n\nlogger.info(\"  \u2713 All libraries installed successfully\")\n\n# ============================================================================\n# STEP 2: Import Libraries\n# ============================================================================\nlogger.info(\"\\n[1/4] Importing libraries...\")\n\nimport yfinance as yf\nimport pandas as pd\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport time\nimport os\nfrom google.colab import drive\n\nlogger.info(\"  \u2713 Libraries imported\")\n\n# ============================================================================\n# STEP 3: Setup Google Drive and Folder Structure\n# ============================================================================\nlogger.info(\"\\n[2/4] Setting up Google Drive and project folders...\")\n\ntry:\n    drive.mount('/content/drive', force_remount=False)\n    logger.info(\"  \u2713 Google Drive mounted\")\n\n    # Define project paths\n    BASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\n    FINANCIAL_DATA_PATH = os.path.join(BASE_PATH, 'financial_data')\n    ASTRO_DATA_PATH = os.path.join(BASE_PATH, 'astro_data')\n\n    # Create directories\n    os.makedirs(FINANCIAL_DATA_PATH, exist_ok=True)\n    os.makedirs(ASTRO_DATA_PATH, exist_ok=True)\n\n    logger.info(f\"  \u2713 Project root: {BASE_PATH}\")\n    logger.info(f\"  \u2713 Financial data folder: financial_data/\")\n    logger.info(f\"  \u2713 Astro data folder: astro_data/\")\n\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not mount Google Drive\")\n    logger.critical(f\"  Error: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# STEP 4: Configure Tickers and Download Parameters\n# ============================================================================\nlogger.info(\"\\n[3/4] Configuring market data parameters...\")\n\n# Ticker configuration\nTICKER_INFO = {\n    # Indian Markets (Indices)\n    '^NSEI':                {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY 50'},\n    '^NSEBANK':             {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY BANK'},\n    'NIFTY_FIN_SERVICE.NS': {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY FIN SERVICES'},\n    '^CNXIT':               {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY IT'},\n    '^CNXPHARMA':           {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY PHARMA'},\n    '^CNXAUTO':             {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY AUTO'},\n    '^CNXMETAL':            {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY METAL'},\n    '^CNXFMCG':             {'currency': 'INR', 'volume_unit': 'shares', 'name': 'NIFTY FMCG'},\n    '^INDIAVIX':            {'currency': 'INR', 'volume_unit': 'points', 'name': 'INDIA VIX'},\n\n    # Indian Markets (Key Stocks)\n    'RELIANCE.NS':          {'currency': 'INR', 'volume_unit': 'shares', 'name': 'Reliance Industries'},\n    'TCS.NS':               {'currency': 'INR', 'volume_unit': 'shares', 'name': 'TCS'},\n    'HDFCBANK.NS':          {'currency': 'INR', 'volume_unit': 'shares', 'name': 'HDFC Bank'},\n\n    # US Markets (Indices)\n    '^GSPC':                {'currency': 'USD', 'volume_unit': 'points', 'name': 'S&P 500'},\n    '^DJI':                 {'currency': 'USD', 'volume_unit': 'points', 'name': 'Dow Jones'},\n    '^NDX':                 {'currency': 'USD', 'volume_unit': 'points', 'name': 'NASDAQ 100'},\n    '^RUT':                 {'currency': 'USD', 'volume_unit': 'points', 'name': 'Russell 2000'},\n    '^VIX':                 {'currency': 'USD', 'volume_unit': 'points', 'name': 'VIX'},\n    '^TNX':                 {'currency': 'USD', 'volume_unit': 'points', 'name': '10Y Treasury'},\n\n    # US Markets (Key Stocks)\n    'AAPL':                 {'currency': 'USD', 'volume_unit': 'shares', 'name': 'Apple'},\n    'MSFT':                 {'currency': 'USD', 'volume_unit': 'shares', 'name': 'Microsoft'},\n    'NVDA':                 {'currency': 'USD', 'volume_unit': 'shares', 'name': 'NVIDIA'},\n\n    # Global Markets (Indices)\n    '^N225':                {'currency': 'JPY', 'volume_unit': 'points', 'name': 'Nikkei 225'},\n    '^FTSE':                {'currency': 'GBP', 'volume_unit': 'points', 'name': 'FTSE 100'},\n    '^GDAXI':               {'currency': 'EUR', 'volume_unit': 'points', 'name': 'DAX'},\n    '000001.SS':            {'currency': 'CNY', 'volume_unit': 'shares', 'name': 'SSE Composite'},\n    '^HSI':                 {'currency': 'HKD', 'volume_unit': 'points', 'name': 'Hang Seng'},\n\n    # Commodities\n    'GC=F':                 {'currency': 'USD', 'volume_unit': 'contracts', 'name': 'Gold'},\n    'CL=F':                 {'currency': 'USD', 'volume_unit': 'contracts', 'name': 'Crude Oil'},\n    'SI=F':                 {'currency': 'USD', 'volume_unit': 'contracts', 'name': 'Silver'},\n\n    # Currencies & DXY\n    'DX-Y.NYB':             {'currency': 'USD', 'volume_unit': 'points', 'name': 'US Dollar Index'},\n    'USDINR=X':             {'currency': 'INR', 'volume_unit': 'rate', 'name': 'USD/INR'},\n    'EURUSD=X':             {'currency': 'USD', 'volume_unit': 'rate', 'name': 'EUR/USD'},\n}\n\nTICKERS = list(TICKER_INFO.keys())\nSTART_DATE = '2000-01-01'\nEND_DATE = datetime.now().strftime('%Y-%m-%d')\nWAIT_TIME_SECONDS = 1\n\nlogger.info(f\"  \u2713 Configured {len(TICKERS)} tickers\")\nlogger.info(f\"  \u2713 Date range: {START_DATE} to {END_DATE}\")\nlogger.info(f\"  \u2713 Rate limit: {WAIT_TIME_SECONDS}s between requests\")\n\n# ============================================================================\n# STEP 5: Download Financial Data\n# ============================================================================\nlogger.info(\"\\n[4/4] Downloading financial data...\")\nlogger.info(\"  (This may take several minutes)\")\nlogger.info(\"\")\n\nfinancial_results_summary = []\nsuccess_count = 0\nfailed_count = 0\nskipped_count = 0\n\nfor i, ticker in enumerate(TICKERS):\n    safe_ticker_name = ticker.replace('^', '').replace('=X', '').replace('=F', '').replace('-','_').replace('.','_')\n    ticker_display_name = TICKER_INFO[ticker]['name']\n\n    logger.info(f\"  [{i+1}/{len(TICKERS)}] {ticker_display_name} ({ticker})...\", end=\" \")\n\n    try:\n        filename = os.path.join(FINANCIAL_DATA_PATH, f\"financial_data_{safe_ticker_name}.parquet\")\n\n        # Download data (auto_adjust=False to suppress FutureWarning)\n        data = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False, auto_adjust=False)\n\n        if data.empty:\n            message = \"No data available\"\n            logger.warning(f\"\u26a0 SKIPPED ({message})\")\n            financial_results_summary.append({\n                'Ticker': ticker,\n                'Name': ticker_display_name,\n                'Status': 'Skipped',\n                'Details': message\n            })\n            skipped_count += 1\n            continue\n\n        # Flatten MultiIndex columns if present (yfinance creates MultiIndex for single tickers)\n        if isinstance(data.columns, pd.MultiIndex):\n            data.columns = data.columns.get_level_values(0)\n\n        # Add metadata\n        data['currency'] = TICKER_INFO[ticker]['currency']\n        data['volume_unit'] = TICKER_INFO[ticker]['volume_unit']\n\n        # Standardize columns\n        data.reset_index(inplace=True)\n        data.rename(columns={\n            'Date': 'date',\n            'Open': 'open',\n            'High': 'high',\n            'Low': 'low',\n            'Close': 'close',\n            'Adj Close': 'adj_close',\n            'Volume': 'volume'\n        }, inplace=True)\n\n        data['date'] = pd.to_datetime(data['date']).dt.date\n\n        # Reorder columns\n        column_order = ['date', 'open', 'high', 'low', 'close', 'volume',\n                       'currency', 'volume_unit', 'adj_close']\n        final_columns = [col for col in column_order if col in data.columns]\n        data = data[final_columns]\n\n        # Save to parquet\n        data.to_parquet(filename, index=False)\n\n        # Summary info\n        first_date = data['date'].min()\n        last_date = data['date'].max()\n        row_count = len(data)\n\n        message = f\"{row_count} rows | {first_date} to {last_date}\"\n        logger.info(f\"\u2713 ({message})\")\n\n        financial_results_summary.append({\n            'Ticker': ticker,\n            'Name': ticker_display_name,\n            'Status': 'Success',\n            'Details': message\n        })\n        success_count += 1\n\n    except Exception as e:\n        message = str(e)[:60]\n        logger.error(f\"\u2717 FAILED ({message})\")\n        financial_results_summary.append({\n            'Ticker': ticker,\n            'Name': ticker_display_name,\n            'Status': 'Failed',\n            'Details': message\n        })\n        failed_count += 1\n\n    finally:\n        time.sleep(WAIT_TIME_SECONDS)\n\n# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DOWNLOAD SUMMARY\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  \u2713 Successful: {success_count}\")\nlogger.warning(f\"  \u26a0 Skipped: {skipped_count}\")\nlogger.error(f\"  \u2717 Failed: {failed_count}\")\nlogger.info(f\"  Total: {len(TICKERS)}\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DETAILED RESULTS\")\nlogger.info(\"=\" * 70)\n\nsummary_df = pd.DataFrame(financial_results_summary)\nprint(\"\\n\" + tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 1 (FINANCIAL DATA) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(\"  1. \u25b6 Run Cell 2: Generate Vedic ephemeris data\")\nlogger.info(\"  2. \u25b6 Run Cell 3: Align financial + astro data by date\")\nlogger.info(\"  3. Then proceed to Phase 2: Feature Engineering\")\n\nlogger.info(f\"\\n\ud83d\udcc2 Output Location:\")\nlogger.info(f\"  {FINANCIAL_DATA_PATH}\")\nlogger.info(f\"  ({success_count} parquet files saved)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "aKK7JHfU9a2w",
    "outputId": "ccca252f-7643-4f8f-a771-303f88c33284"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 45) (ipython-input-2994976661.py, line 45)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2994976661.py\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    print(f\"  \u00e2\u0153\" {msg}\")\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 45)\n"
     ]
    }
   ],
   "id": "cell-0002"
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Vedic Astrological Data Generation (Phase 1 - Part 2 of 3)\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport swisseph as swe\nfrom datetime import datetime, timedelta\nfrom tabulate import tabulate\nimport requests\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 1: VEDIC EPHEMERIS GENERATION\")\nlogger.info(\"Phase 1 Progress: Part 2 of 3 (Astrological Data)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/6] Setting up directories...\")\n\n# Ephemeris files (local runtime)\nEPHE_DIR = \"/content/ephe_data\"\nos.makedirs(EPHE_DIR, exist_ok=True)\nlogger.info(f\"  \u2713 Ephemeris directory: {EPHE_DIR}\")\n\n# Output directory (Google Drive)\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nASTRO_DATA_PATH = os.path.join(BASE_PATH, 'astro_data')\nos.makedirs(ASTRO_DATA_PATH, exist_ok=True)\nlogger.info(f\"  \u2713 Output directory: {ASTRO_DATA_PATH}\")\n\n# ============================================================================\n# STEP 2: Download Ephemeris Files from VERIFIED WORKING SOURCES\n# ============================================================================\nlogger.info(\"\\n[2/6] Downloading ephemeris files...\")\nlogger.info(\"  Source: GitHub aloistr/swisseph (official Swiss Ephemeris)\")\n\n# The correct GitHub repository is aloistr/swisseph\nGITHUB_BASE = \"https://raw.githubusercontent.com/aloistr/swisseph/master/ephe/\"\n\n# Files needed for 2000-2025 (these cover 1800-2399 CE)\nEPHE_FILES = {\n    \"semo_18.se1\": \"Moon ephemeris 1800-2399\",\n    \"sepl_18.se1\": \"Planets ephemeris 1800-2399\",\n}\n\ndownload_success = True\ndownloaded_files = []\n\nfor filename, description in EPHE_FILES.items():\n    file_path = os.path.join(EPHE_DIR, filename)\n    url = GITHUB_BASE + filename\n\n    try:\n        logger.info(f\"  \u2192 {filename} ({description})...\", end=\" \")\n        response = requests.get(url, timeout=60, allow_redirects=True)\n\n        if response.status_code == 200 and len(response.content) > 1000:\n            with open(file_path, 'wb') as f:\n                f.write(response.content)\n\n            # Verify file was written correctly\n            if os.path.exists(file_path) and os.path.getsize(file_path) > 1000:\n                size_kb = os.path.getsize(file_path) / 1024\n                logger.info(f\"\u2713 ({size_kb:.1f} KB)\")\n                downloaded_files.append(filename)\n            else:\n                logger.error(\"\u2717 FAILED (file too small)\")\n                download_success = False\n        else:\n            logger.error(f\"\u2717 FAILED (HTTP {response.status_code})\")\n            download_success = False\n\n    except Exception as e:\n        logger.error(f\"\u2717 FAILED ({str(e)[:60]})\")\n        download_success = False\n\n# ============================================================================\n# BACKUP: Try Dropbox if GitHub fails\n# ============================================================================\nif not download_success or len(downloaded_files) < 2:\n    logger.info(\"\\n  GitHub download incomplete. Trying backup source...\")\n    logger.info(\"  Source: Dropbox (Alois Treindl's public folder)\")\n\n    # Dropbox direct download links\n    DROPBOX_FILES = {\n        \"semo_18.se1\": \"https://www.dropbox.com/scl/fo/y3naz62gy6f6qfrhquu7u/h/semo_18.se1?rlkey=ejltdhb262zglm7eo6yfj2940&dl=1\",\n        \"sepl_18.se1\": \"https://www.dropbox.com/scl/fo/y3naz62gy6f6qfrhquu7u/h/sepl_18.se1?rlkey=ejltdhb262zglm7eo6yfj2940&dl=1\",\n    }\n\n    downloaded_files = []  # Reset\n    download_success = True\n\n    for filename, url in DROPBOX_FILES.items():\n        file_path = os.path.join(EPHE_DIR, filename)\n\n        try:\n            logger.info(f\"  \u2192 {filename}...\", end=\" \")\n            response = requests.get(url, timeout=60, allow_redirects=True)\n\n            if response.status_code == 200 and len(response.content) > 1000:\n                with open(file_path, 'wb') as f:\n                    f.write(response.content)\n\n                if os.path.exists(file_path) and os.path.getsize(file_path) > 1000:\n                    size_kb = os.path.getsize(file_path) / 1024\n                    logger.info(f\"\u2713 ({size_kb:.1f} KB)\")\n                    downloaded_files.append(filename)\n                else:\n                    logger.error(\"\u2717 FAILED\")\n                    download_success = False\n            else:\n                logger.error(f\"\u2717 FAILED (HTTP {response.status_code})\")\n                download_success = False\n\n        except Exception as e:\n            logger.error(f\"\u2717 FAILED ({str(e)[:60]})\")\n            download_success = False\n\n# ============================================================================\n# STEP 3: Fatal Error Check\n# ============================================================================\nif not download_success or len(downloaded_files) < 2:\n    logger.info(\"\\n\" + \"!\" * 70)\n    logger.critical(\"FATAL ERROR: Ephemeris file download failed from all sources!\")\n    logger.info(\"!\" * 70)\n    logger.info(\"\\nDiagnostics:\")\n    logger.info(f\"  - Required files: 2\")\n    logger.info(f\"  - Successfully downloaded: {len(downloaded_files)}\")\n    logger.info(f\"  - Files in directory: {os.listdir(EPHE_DIR)}\")\n\n    logger.info(\"\\n\ud83d\udd27 MANUAL WORKAROUND - Run these commands in a NEW cell:\")\n    logger.info(\"```python\")\n    logger.info(\"# Method 1: Direct wget from GitHub\")\n    logger.info(\"!wget -P /content/ephe_data https://raw.githubusercontent.com/aloistr/swisseph/master/ephe/semo_18.se1\")\n    logger.info(\"!wget -P /content/ephe_data https://raw.githubusercontent.com/aloistr/swisseph/master/ephe/sepl_18.se1\")\n    logger.info(\"\")\n    logger.info(\"# Method 2: If GitHub blocked, use curl from Dropbox\")\n    logger.info(\"!curl -L -o /content/ephe_data/semo_18.se1 'https://www.dropbox.com/scl/fo/y3naz62gy6f6qfrhquu7u/h/semo_18.se1?rlkey=ejltdhb262zglm7eo6yfj2940&dl=1'\")\n    logger.info(\"!curl -L -o /content/ephe_data/sepl_18.se1 'https://www.dropbox.com/scl/fo/y3naz62gy6f6qfrhquu7u/h/sepl_18.se1?rlkey=ejltdhb262zglm7eo6yfj2940&dl=1'\")\n    logger.info(\"```\")\n    logger.info(\"\\nThen re-run this cell.\")\n\n    raise SystemExit(1)\n\nlogger.info(f\"\\n  \u2713 Successfully downloaded {len(downloaded_files)} ephemeris files\")\n\n# ============================================================================\n# STEP 4: Configure PySwisseph for Vedic Calculations\n# ============================================================================\nlogger.info(\"\\n[3/6] Configuring PySwisseph for Vedic (Lahiri) mode...\")\n\nswe.set_ephe_path(EPHE_DIR)\nswe.set_sid_mode(swe.SIDM_LAHIRI)\n\nlogger.info(f\"  \u2713 Ephemeris path: {EPHE_DIR}\")\nlogger.info(f\"  \u2713 Ayanamsha: Lahiri (Vedic)\")\n\n# Quick test to ensure files are working\ntry:\n    test_jd = swe.julday(2000, 1, 1, 12.0)\n    test_pos, _ = swe.calc_ut(test_jd, swe.SUN, swe.FLG_SIDEREAL)\n    logger.info(f\"  \u2713 Verification: Sun position on 2000-01-01 = {test_pos[0]:.2f}\u00b0\")\nexcept Exception as e:\n    logger.error(f\"  \u2717 WARNING: Test calculation failed: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# STEP 5: Generate Full Ephemeris Data (2000-01-01 to 2025-10-29)\n# ============================================================================\nlogger.info(\"\\n[4/6] Calculating planetary positions for full date range...\")\nlogger.info(\"  Date range: 2000-01-01 to 2025-10-29\")\nlogger.info(\"  (This will take several minutes - ~9,400 days to calculate)\")\n\nstart_date = datetime(2000, 1, 1)\nend_date = datetime(2025, 10, 29)\ntotal_days = (end_date - start_date).days + 1\n\nlogger.info(f\"  Total days: {total_days}\")\n\nPLANETS = {\n    'Sun': swe.SUN,\n    'Moon': swe.MOON,\n    'Mercury': swe.MERCURY,\n    'Venus': swe.VENUS,\n    'Mars': swe.MARS,\n    'Jupiter': swe.JUPITER,\n    'Saturn': swe.SATURN,\n    'Rahu': swe.MEAN_NODE,\n}\n\nephemeris_data = []\ncalculation_warnings = []\n\n# Progress tracking\ncurrent_date = start_date\ndays_processed = 0\nprogress_interval = 500  # Print progress every 500 days\n\nlogger.info(\"\\n  Progress:\")\n\nwhile current_date <= end_date:\n    jd = swe.julday(current_date.year, current_date.month, current_date.day, 12.0)\n\n    day_data = {\n        'date': current_date.strftime('%Y-%m-%d'),\n        'julian_day': jd\n    }\n\n    for planet_name, planet_id in PLANETS.items():\n        try:\n            position, ret_flag = swe.calc_ut(jd, planet_id, swe.FLG_SIDEREAL | swe.FLG_SPEED)\n\n            if position is not None and len(position) >= 4:\n                day_data[f'{planet_name.lower()}_longitude'] = round(position[0], 6)\n                day_data[f'{planet_name.lower()}_speed'] = round(position[3], 6)\n            else:\n                day_data[f'{planet_name.lower()}_longitude'] = None\n                day_data[f'{planet_name.lower()}_speed'] = None\n                calculation_warnings.append(\n                    f\"{planet_name} on {current_date.strftime('%Y-%m-%d')}: returned None\"\n                )\n        except Exception as e:\n            day_data[f'{planet_name.lower()}_longitude'] = None\n            day_data[f'{planet_name.lower()}_speed'] = None\n            calculation_warnings.append(\n                f\"{planet_name} on {current_date.strftime('%Y-%m-%d')}: {str(e)}\"\n            )\n\n    ephemeris_data.append(day_data)\n    current_date += timedelta(days=1)\n    days_processed += 1\n\n    # Progress indicator\n    if days_processed % progress_interval == 0 or days_processed == total_days:\n        progress_pct = (days_processed / total_days) * 100\n        logger.info(f\"    [{days_processed}/{total_days}] {progress_pct:.1f}% complete\")\n\ndf_ephemeris = pd.DataFrame(ephemeris_data)\n\nlogger.info(f\"\\n  \u2713 Calculated {len(df_ephemeris)} days\")\nlogger.info(f\"  \u2713 DataFrame shape: {df_ephemeris.shape}\")\nlogger.info(f\"  \u2713 Columns: {len(df_ephemeris.columns)}\")\n\n# ============================================================================\n# STEP 6: Validation & Quality Check\n# ============================================================================\nlogger.info(\"\\n[5/6] Validating data quality...\")\n\nif calculation_warnings:\n    logger.warning(f\"\\n  \u26a0 CALCULATION WARNINGS: {len(calculation_warnings)} warnings detected\")\n    for warning in calculation_warnings[:5]:\n        logger.info(f\"    \u2022 {warning}\")\n    if len(calculation_warnings) > 5:\n        logger.info(f\"    \u2022 ... and {len(calculation_warnings) - 5} more warnings\")\n\nnull_counts = df_ephemeris.isnull().sum()\ntotal_nulls = null_counts.sum()\n\nif total_nulls > 0:\n    logger.warning(f\"\\n  \u26a0 NULL VALUES: {total_nulls} cells contain null values\")\n    logger.info(\"    Columns with nulls:\")\n    for col, count in null_counts[null_counts > 0].items():\n        logger.info(f\"      \u2022 {col}: {count}\")\nelse:\n    logger.info(\"\\n  \u2713 DATA QUALITY: No null values detected\")\n\n# Sanity checks on the data\nlogger.info(\"\\n  Data Validation:\")\nsun_longs = df_ephemeris['sun_longitude'].dropna()\nif len(sun_longs) > 0:\n    logger.info(f\"    \u2713 Sun longitude range: {sun_longs.min():.2f}\u00b0 to {sun_longs.max():.2f}\u00b0\")\n    if sun_longs.min() >= 0 and sun_longs.max() <= 360:\n        logger.info(\"    \u2713 All longitudes within valid range (0-360\u00b0)\")\n    else:\n        logger.warning(\"    \u26a0 WARNING: Some longitudes outside valid range!\")\n\nmoon_speeds = df_ephemeris['moon_speed'].dropna()\nif len(moon_speeds) > 0:\n    logger.info(f\"    \u2713 Moon speed range: {moon_speeds.min():.4f}\u00b0 to {moon_speeds.max():.4f}\u00b0 per day\")\n    if 11 < moon_speeds.mean() < 15:\n        logger.info(f\"    \u2713 Moon speed looks reasonable (avg: {moon_speeds.mean():.2f}\u00b0/day)\")\n\n# ============================================================================\n# STEP 7: Save Results to Google Drive\n# ============================================================================\nlogger.info(\"\\n[6/6] Saving ephemeris data to Google Drive...\")\n\nOUTPUT_FILE = os.path.join(ASTRO_DATA_PATH, 'vedic_ephemeris_2000_2025.parquet')\ndf_ephemeris.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow')\n\nfile_size_kb = os.path.getsize(OUTPUT_FILE) / 1024\nlogger.info(f\"  \u2713 Saved: vedic_ephemeris_2000_2025.parquet\")\nlogger.info(f\"  \u2713 Size: {file_size_kb:.1f} KB\")\n\n# ============================================================================\n# STEP 8: Display Sample Results\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE RESULTS: PLANETARY LONGITUDES (First 3 Days)\")\nlogger.info(\"=\" * 70)\n\nsample_cols = ['date', 'sun_longitude', 'moon_longitude', 'mercury_longitude',\n               'venus_longitude', 'mars_longitude', 'jupiter_longitude',\n               'saturn_longitude', 'rahu_longitude']\n\nsample_df = df_ephemeris[sample_cols].head(3)\nprint(\"\\n\" + tabulate(sample_df, headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE RESULTS: PLANETARY SPEEDS (First 3 Days)\")\nlogger.info(\"=\" * 70)\n\nspeed_cols = ['date', 'sun_speed', 'moon_speed', 'mercury_speed',\n              'venus_speed', 'mars_speed', 'jupiter_speed',\n              'saturn_speed', 'rahu_speed']\n\nspeed_df = df_ephemeris[speed_cols].head(3)\nprint(\"\\n\" + tabulate(speed_df, headers='keys', tablefmt='grid', showindex=False, floatfmt=\".4f\"))\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 1 (VEDIC EPHEMERIS) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(\"  1. \u2713 Financial data acquired (Cell 1 complete)\")\nlogger.info(\"  2. \u2713 Vedic ephemeris generated (Cell 2 complete)\")\nlogger.info(\"  3. \u25b6 Run Cell 3: Align & merge datasets by date\")\nlogger.info(\"  4. Then proceed to Phase 2: Feature Engineering\")\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files:\")\nlogger.info(f\"  \u2022 Ephemeris files: {EPHE_DIR}\")\nlogger.info(f\"  \u2022 Vedic data: {ASTRO_DATA_PATH}\")\nlogger.info(f\"  \u2022 File: vedic_ephemeris_2000_2025.parquet ({len(df_ephemeris)} rows)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "id": "acbYT15QGBfu"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0003"
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Data Alignment & Merging (Phase 1 - Part 3 of 3)\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport glob\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 1: DATA ALIGNMENT & MERGING\")\nlogger.info(\"Phase 1 Progress: Part 3 of 3 (Dataset Integration)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/5] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nFINANCIAL_DATA_PATH = os.path.join(BASE_PATH, 'financial_data')\nASTRO_DATA_PATH = os.path.join(BASE_PATH, 'astro_data')\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\n\n# Create aligned data directory\nos.makedirs(ALIGNED_DATA_PATH, exist_ok=True)\n\nlogger.info(f\"  \u2713 Financial data: {FINANCIAL_DATA_PATH}\")\nlogger.info(f\"  \u2713 Astro data: {ASTRO_DATA_PATH}\")\nlogger.info(f\"  \u2713 Output directory: {ALIGNED_DATA_PATH}\")\n\n# ============================================================================\n# STEP 2: Load Vedic Ephemeris Data\n# ============================================================================\nlogger.info(\"\\n[2/5] Loading Vedic ephemeris data...\")\n\nEPHEMERIS_FILE = os.path.join(ASTRO_DATA_PATH, 'vedic_ephemeris_2000_2025.parquet')\n\ntry:\n    df_ephemeris = pd.read_parquet(EPHEMERIS_FILE)\n    df_ephemeris['date'] = pd.to_datetime(df_ephemeris['date']).dt.date\n\n    logger.info(f\"  \u2713 Loaded ephemeris data\")\n    logger.info(f\"  \u2713 Shape: {df_ephemeris.shape}\")\n    logger.info(f\"  \u2713 Date range: {df_ephemeris['date'].min()} to {df_ephemeris['date'].max()}\")\n    logger.info(f\"  \u2713 Columns: {len(df_ephemeris.columns)}\")\n\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not load ephemeris data\")\n    logger.critical(f\"  Error: {e}\")\n    logger.info(f\"\\n  Make sure Cell 2 has been run successfully!\")\n    raise SystemExit(1)\n\n# ============================================================================\n# STEP 3: Load and Align Financial Data\n# ============================================================================\nlogger.info(\"\\n[3/5] Loading and aligning financial data with ephemeris...\")\n\n# Find all financial data parquet files\nfinancial_files = glob.glob(os.path.join(FINANCIAL_DATA_PATH, 'financial_data_*.parquet'))\n\nif len(financial_files) == 0:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: No financial data files found\")\n    logger.info(f\"  Make sure Cell 1 has been run successfully!\")\n    raise SystemExit(1)\n\nlogger.info(f\"\\n  Found {len(financial_files)} financial data files\")\n\n# Debug: Check first file to see its structure\nlogger.info(\"\\n  Debugging first file structure...\")\nfirst_file = financial_files[0]\ndf_test = pd.read_parquet(first_file)\nlogger.info(f\"  \u2022 Columns: {list(df_test.columns)}\")\nlogger.info(f\"  \u2022 Date column type: {df_test['date'].dtype if 'date' in df_test.columns else 'Column not found!'}\")\nlogger.info(f\"  \u2022 Sample dates: {df_test['date'].head(3).tolist() if 'date' in df_test.columns else 'N/A'}\")\nlogger.info(f\"  \u2022 Shape: {df_test.shape}\")\n\nlogger.info(\"\\n  Processing all tickers...\")\n\nalignment_results = []\nsuccess_count = 0\nfailed_count = 0\n\nfor i, file_path in enumerate(financial_files):\n    # Extract ticker name from filename\n    filename = os.path.basename(file_path)\n    ticker_name = filename.replace('financial_data_', '').replace('.parquet', '')\n\n    logger.info(f\"\\n  [{i+1}/{len(financial_files)}] Processing {ticker_name}...\", end=\" \")\n\n    try:\n        # Load financial data\n        df_financial = pd.read_parquet(file_path)\n\n        # Ensure date column exists and convert to date type\n        if 'date' not in df_financial.columns:\n            raise ValueError(\"'date' column not found in financial data\")\n\n        # Convert date to datetime.date for consistent merging\n        df_financial['date'] = pd.to_datetime(df_financial['date']).dt.date\n\n        original_rows = len(df_financial)\n\n        # Merge with ephemeris data on date\n        df_aligned = pd.merge(\n            df_financial,\n            df_ephemeris,\n            on='date',\n            how='inner'  # Only keep dates that exist in both datasets\n        )\n\n        aligned_rows = len(df_aligned)\n        date_range = f\"{df_aligned['date'].min()} to {df_aligned['date'].max()}\"\n\n        # Save aligned data\n        output_file = os.path.join(ALIGNED_DATA_PATH, f'aligned_{ticker_name}.parquet')\n        df_aligned.to_parquet(output_file, index=False, engine='pyarrow')\n\n        logger.info(f\"\u2713 ({aligned_rows} rows | {date_range})\")\n\n        alignment_results.append({\n            'Ticker': ticker_name,\n            'Original_Rows': original_rows,\n            'Aligned_Rows': aligned_rows,\n            'Match_Rate': f\"{(aligned_rows/original_rows)*100:.1f}%\",\n            'Date_Range': date_range,\n            'Status': 'Success'\n        })\n\n        success_count += 1\n\n    except KeyError as e:\n        error_msg = f\"Missing column: {str(e)}\"\n        logger.error(f\"\u2717 FAILED ({error_msg})\")\n\n        alignment_results.append({\n            'Ticker': ticker_name,\n            'Original_Rows': 0,\n            'Aligned_Rows': 0,\n            'Match_Rate': '0%',\n            'Date_Range': 'N/A',\n            'Status': f'Failed: {error_msg}'\n        })\n\n        failed_count += 1\n\n    except Exception as e:\n        error_msg = str(e)[:50]\n        logger.error(f\"\u2717 FAILED ({error_msg})\")\n\n        alignment_results.append({\n            'Ticker': ticker_name,\n            'Original_Rows': 0,\n            'Aligned_Rows': 0,\n            'Match_Rate': '0%',\n            'Date_Range': 'N/A',\n            'Status': f'Failed: {error_msg}'\n        })\n\n        failed_count += 1\n\n# ============================================================================\n# STEP 4: Create Master Aligned Dataset (Optional)\n# ============================================================================\nlogger.info(\"\\n\\n[4/5] Creating master aligned dataset...\")\nlogger.info(\"  (Combining all tickers into single file for convenience)\")\n\n# Create a wide-format master dataset with all tickers\nmaster_data = df_ephemeris.copy()\n\nfor file_path in glob.glob(os.path.join(ALIGNED_DATA_PATH, 'aligned_*.parquet')):\n    filename = os.path.basename(file_path)\n    ticker_name = filename.replace('aligned_', '').replace('.parquet', '')\n\n    try:\n        df_ticker = pd.read_parquet(file_path)\n\n        # Select only financial columns (not ephemeris columns to avoid duplication)\n        financial_cols = ['date', 'open', 'high', 'low', 'close', 'volume', 'adj_close']\n        df_ticker_subset = df_ticker[financial_cols].copy()\n\n        # Rename columns to include ticker name\n        rename_dict = {\n            'open': f'{ticker_name}_open',\n            'high': f'{ticker_name}_high',\n            'low': f'{ticker_name}_low',\n            'close': f'{ticker_name}_close',\n            'volume': f'{ticker_name}_volume',\n            'adj_close': f'{ticker_name}_adj_close'\n        }\n        df_ticker_subset.rename(columns=rename_dict, inplace=True)\n\n        # Merge into master dataset\n        master_data = pd.merge(master_data, df_ticker_subset, on='date', how='left')\n\n    except Exception as e:\n        logger.warning(f\"  \u26a0 Warning: Could not add {ticker_name} to master dataset: {e}\")\n\n# Save master dataset\nmaster_file = os.path.join(ALIGNED_DATA_PATH, 'master_aligned_dataset.parquet')\nmaster_data.to_parquet(master_file, index=False, engine='pyarrow')\n\nfile_size_mb = os.path.getsize(master_file) / (1024 * 1024)\nlogger.info(f\"  \u2713 Created master dataset\")\nlogger.info(f\"  \u2713 Shape: {master_data.shape}\")\nlogger.info(f\"  \u2713 Size: {file_size_mb:.2f} MB\")\nlogger.info(f\"  \u2713 Saved: master_aligned_dataset.parquet\")\n\n# ============================================================================\n# STEP 5: Generate Summary Statistics\n# ============================================================================\nlogger.info(\"\\n[5/5] Generating summary statistics...\")\n\nsummary_df = pd.DataFrame(alignment_results)\n\n# ============================================================================\n# DISPLAY RESULTS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"ALIGNMENT SUMMARY\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  \u2713 Successful: {success_count}\")\nlogger.error(f\"  \u2717 Failed: {failed_count}\")\nlogger.info(f\"  Total: {len(financial_files)}\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DETAILED ALIGNMENT RESULTS\")\nlogger.info(\"=\" * 70)\n\nprint(\"\\n\" + tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))\n\n# ============================================================================\n# SAMPLE DATA PREVIEW\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: MASTER ALIGNED DATASET (First 3 Rows)\")\nlogger.info(\"=\" * 70)\n\n# Show a subset of columns for readability\npreview_cols = ['date', 'sun_longitude', 'moon_longitude', 'mercury_longitude']\n\n# Add first ticker's financial data to preview\nfirst_ticker_cols = [col for col in master_data.columns if '_close' in col][:3]\npreview_cols.extend(first_ticker_cols)\n\n# Make sure columns exist\npreview_cols = [col for col in preview_cols if col in master_data.columns]\n\nif len(preview_cols) > 0:\n    sample_data = master_data[preview_cols].head(3)\n    print(\"\\n\" + tabulate(sample_data, headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))\nelse:\n    logger.info(\"\\n  (No data to preview)\")\n\n# ============================================================================\n# DATA QUALITY METRICS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DATA QUALITY METRICS\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  Total aligned records: {len(master_data)}\")\nlogger.info(f\"  Date range: {master_data['date'].min()} to {master_data['date'].max()}\")\nlogger.info(f\"  Total columns: {len(master_data.columns)}\")\n\n# Count null values in planetary data\nplanetary_cols = [col for col in master_data.columns if any(\n    planet in col for planet in ['sun_', 'moon_', 'mercury_', 'venus_', 'mars_', 'jupiter_', 'saturn_', 'rahu_']\n)]\nplanetary_nulls = master_data[planetary_cols].isnull().sum().sum()\n\nlogger.info(f\"  Planetary data nulls: {planetary_nulls}\")\n\n# Count completeness of financial data\nfinancial_cols = [col for col in master_data.columns if '_close' in col]\nif len(financial_cols) > 0:\n    financial_completeness = (1 - master_data[financial_cols].isnull().sum().sum() /\n                             (len(master_data) * len(financial_cols))) * 100\n    logger.info(f\"  Financial data completeness: {financial_completeness:.1f}%\")\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 1 (DATA ALIGNMENT) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Phase 1 Summary:\")\nlogger.info(\"  \u2713 Cell 1: Financial data acquired (32 tickers)\")\nlogger.info(\"  \u2713 Cell 2: Vedic ephemeris generated (9,434 days)\")\nlogger.info(\"  \u2713 Cell 3: Datasets aligned and merged\")\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files:\")\nlogger.info(f\"  \u2022 Individual aligned files: {ALIGNED_DATA_PATH}/aligned_*.parquet\")\nlogger.info(f\"  \u2022 Master dataset: {ALIGNED_DATA_PATH}/master_aligned_dataset.parquet\")\nlogger.info(f\"  \u2022 Total files created: {success_count + 1}\")\n\nlogger.info(\"\\n\ud83c\udfaf Ready for Phase 2: Feature Engineering\")\nlogger.info(\"  The aligned datasets are now ready for:\")\nlogger.info(\"  \u2022 Aspect calculations (conjunctions, trines, squares, etc.)\")\nlogger.info(\"  \u2022 Transit analysis (planet-to-planet relationships)\")\nlogger.info(\"  \u2022 Nakshatra mapping (lunar mansion positions)\")\nlogger.info(\"  \u2022 Dasha period calculations\")\nlogger.info(\"  \u2022 Technical indicators integration\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "id": "qDZbY7SO9Uo0"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0004"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PHASE 2 - \u2699\ufe0f Feature Engineering"
   ],
   "metadata": {
    "id": "cLtxVnm8GuPJ"
   },
   "id": "cell-0005"
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Planetary Aspect Calculations (Phase 2 - Part 1 of 5)\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport itertools\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 2: FEATURE ENGINEERING\")\nlogger.info(\"Phase 2 Progress: Part 1 of 5 (Planetary Aspects)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/7] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\nFEATURE_DATA_PATH = os.path.join(BASE_PATH, 'feature_data')\n\n# Create feature data directory\nos.makedirs(FEATURE_DATA_PATH, exist_ok=True)\n\nINPUT_FILE = os.path.join(ALIGNED_DATA_PATH, 'master_aligned_dataset.parquet')\nOUTPUT_FILE = os.path.join(FEATURE_DATA_PATH, 'aspects_features.parquet')\n\nlogger.info(f\"  \u2713 Input: {ALIGNED_DATA_PATH}/master_aligned_dataset.parquet\")\nlogger.info(f\"  \u2713 Output: {FEATURE_DATA_PATH}/aspects_features.parquet\")\n\n# ============================================================================\n# STEP 2: Load Master Aligned Data\n# ============================================================================\nlogger.info(\"\\n[2/7] Loading master aligned dataset...\")\n\nif not os.path.exists(INPUT_FILE):\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Input file not found\")\n    logger.info(\"  Please run Cell 3 first to generate aligned data.\")\n    raise SystemExit(1)\n\ndf = pd.read_parquet(INPUT_FILE)\ndf['date'] = pd.to_datetime(df['date'])\n\nlogger.info(f\"  \u2713 Loaded dataset\")\nlogger.info(f\"  \u2713 Shape: {df.shape}\")\nlogger.info(f\"  \u2713 Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n\n# ============================================================================\n# STEP 3: Define Astrological Constants\n# ============================================================================\nlogger.info(\"\\n[3/7] Setting up astrological parameters...\")\n\n# Planets (must match column names from Cell 2)\nPLANETS = ['sun', 'moon', 'mercury', 'venus', 'mars',\n           'jupiter', 'saturn', 'rahu']\n\n# Planet pairs for aspects (28 combinations)\nPLANET_PAIRS = list(itertools.combinations(PLANETS, 2))\n\n# Major aspects with traditional orbs (per aspect type)\n# Format: {aspect_name: (angle, base_orb)}\nASPECT_DEFINITIONS = {\n    'conjunction': (0, 10),      # Most powerful, widest orb\n    'opposition': (180, 10),     # Very powerful\n    'trine': (120, 8),           # Harmonious, medium orb\n    'square': (90, 8),           # Challenging, medium orb\n    'sextile': (60, 6),          # Mild, smaller orb\n}\n\n# Tight orb threshold for \"exact\" aspects\nTIGHT_ORB = 2.0\n\n# Planet importance for orb adjustments (Luminaries get more)\nPLANET_IMPORTANCE = {\n    'sun': 1.2,      # Luminary boost\n    'moon': 1.2,     # Luminary boost\n    'mercury': 1.0,\n    'venus': 1.0,\n    'mars': 1.0,\n    'jupiter': 1.0,\n    'saturn': 1.0,\n    'rahu': 0.8,     # Node, slightly reduced\n}\n\nlogger.info(f\"  \u2713 Configured {len(PLANETS)} planets\")\nlogger.info(f\"  \u2713 Generated {len(PLANET_PAIRS)} planet pairs\")\nlogger.info(f\"  \u2713 Defined {len(ASPECT_DEFINITIONS)} major aspects\")\nlogger.info(f\"  \u2713 Tight orb threshold: {TIGHT_ORB}\u00b0\")\n\n# ============================================================================\n# STEP 4: Define Calculation Functions\n# ============================================================================\nlogger.info(\"\\n[4/7] Defining calculation functions...\")\n\ndef normalize_angle(angle):\n    \"\"\"Normalize angle to 0-360 range.\"\"\"\n    return angle % 360.0\n\ndef angular_distance(lon1, lon2):\n    \"\"\"\n    Calculate shortest angular distance between two longitudes.\n    Returns value between 0 and 180 degrees.\n    \"\"\"\n    diff = np.abs(lon1 - lon2)\n    # Use modulo to handle wrap-around\n    diff = np.where(diff > 180, 360.0 - diff, diff)\n    return diff\n\ndef get_aspect_orb(planet1, planet2, aspect_name):\n    \"\"\"\n    Calculate adjusted orb for a planet pair and aspect.\n    Luminaries (Sun/Moon) get wider orbs.\n    \"\"\"\n    base_orb = ASPECT_DEFINITIONS[aspect_name][1]\n\n    # Apply importance multipliers\n    p1_mult = PLANET_IMPORTANCE.get(planet1, 1.0)\n    p2_mult = PLANET_IMPORTANCE.get(planet2, 1.0)\n\n    # Use the larger multiplier\n    max_mult = max(p1_mult, p2_mult)\n\n    return base_orb * max_mult\n\ndef calculate_aspect_strength(distance_from_exact, orb):\n    \"\"\"\n    Calculate aspect strength (0 to 1).\n    1.0 = exact aspect\n    0.0 = at orb limit\n    Uses cosine curve for smooth falloff.\n    Vectorized to handle arrays.\n    \"\"\"\n    # Ensure inputs are arrays\n    distance_from_exact = np.asarray(distance_from_exact)\n\n    # Cosine curve: 1 at center, 0 at orb\n    # Only calculate for values within orb\n    strength = np.cos((distance_from_exact / orb) * (np.pi / 2))\n\n    # Clip to ensure values stay in [0, 1] range\n    strength = np.clip(strength, 0.0, 1.0)\n\n    return strength\n\ndef determine_applying_separating(p1_lon, p1_speed, p2_lon, p2_speed, aspect_angle):\n    \"\"\"\n    Determine if aspect is applying (forming) or separating (dissolving).\n\n    Logic:\n    - Calculate current distance to aspect\n    - Estimate future distance using speeds\n    - If future distance < current distance \u2192 applying\n    - If future distance > current distance \u2192 separating\n    \"\"\"\n    # Current angular distance\n    current_dist = angular_distance(p1_lon, p2_lon)\n\n    # Estimate positions in ~1 day (speeds are in degrees/day)\n    p1_future = normalize_angle(p1_lon + p1_speed)\n    p2_future = normalize_angle(p2_lon + p2_speed)\n\n    # Future angular distance\n    future_dist = angular_distance(p1_future, p2_future)\n\n    # Calculate distance from exact aspect angle\n    current_from_exact = np.abs(current_dist - aspect_angle)\n    current_from_exact = np.minimum(current_from_exact, 360 - current_from_exact)\n\n    future_from_exact = np.abs(future_dist - aspect_angle)\n    future_from_exact = np.minimum(future_from_exact, 360 - future_from_exact)\n\n    # Applying if getting closer to exact aspect\n    is_applying = future_from_exact < current_from_exact\n\n    return is_applying\n\nlogger.info(\"  \u2713 Angular distance calculation\")\nlogger.info(\"  \u2713 Dynamic orb adjustment\")\nlogger.info(\"  \u2713 Aspect strength scoring\")\nlogger.info(\"  \u2713 Applying/separating detection\")\n\n# ============================================================================\n# STEP 5: Calculate Aspect Features (Vectorized)\n# ============================================================================\nlogger.info(\"\\n[5/7] Calculating aspect features...\")\nlogger.info(f\"  Processing {len(PLANET_PAIRS)} planet pairs \u00d7 {len(ASPECT_DEFINITIONS)} aspects\")\nlogger.info(f\"  Expected features: ~{len(PLANET_PAIRS) * len(ASPECT_DEFINITIONS) * 5}\")\n\nstart_time = datetime.now()\n\n# Initialize feature tracking\naspect_features = {}\nfeature_count = 0\n\n# Progress tracking\ntotal_calculations = len(PLANET_PAIRS) * len(ASPECT_DEFINITIONS)\ncalc_count = 0\nprogress_interval = 20\n\nfor p1, p2 in PLANET_PAIRS:\n    # Get column names\n    p1_lon_col = f'{p1}_longitude'\n    p1_speed_col = f'{p1}_speed'\n    p2_lon_col = f'{p2}_longitude'\n    p2_speed_col = f'{p2}_speed'\n\n    # Extract values as numpy arrays for speed\n    p1_lon = df[p1_lon_col].values\n    p1_speed = df[p1_speed_col].values\n    p2_lon = df[p2_lon_col].values\n    p2_speed = df[p2_speed_col].values\n\n    # Calculate angular distance between planets\n    ang_dist = angular_distance(p1_lon, p2_lon)\n\n    for aspect_name, (aspect_angle, base_orb) in ASPECT_DEFINITIONS.items():\n        calc_count += 1\n\n        # Get adjusted orb for this planet pair\n        orb = get_aspect_orb(p1, p2, aspect_name)\n\n        # Calculate distance from exact aspect\n        dist_from_exact = np.abs(ang_dist - aspect_angle)\n\n        # Handle 360\u00b0 wrap (e.g., 359\u00b0 is close to 0\u00b0)\n        dist_from_exact = np.minimum(dist_from_exact, 360 - dist_from_exact)\n\n        # --- Feature 1: Is aspect active? (within orb) ---\n        is_active = (dist_from_exact <= orb).astype(np.int8)\n\n        # --- Feature 2: Is it a tight/exact aspect? ---\n        is_tight = (dist_from_exact <= TIGHT_ORB).astype(np.int8)\n\n        # --- Feature 3: Aspect strength (0 to 1) ---\n        strength = np.zeros_like(dist_from_exact, dtype=np.float32)\n        active_mask = dist_from_exact <= orb\n        strength[active_mask] = calculate_aspect_strength(\n            dist_from_exact[active_mask],\n            orb\n        )\n\n        # --- Feature 4: Distance from exact (in degrees) ---\n        exact_distance = dist_from_exact.astype(np.float32)\n\n        # --- Feature 5: Is applying (vs separating)? ---\n        is_applying = determine_applying_separating(\n            p1_lon, p1_speed, p2_lon, p2_speed, aspect_angle\n        ).astype(np.int8)\n\n        # Store features with descriptive names\n        base_name = f'{p1}_{p2}_{aspect_name}'\n        aspect_features[f'{base_name}_active'] = is_active\n        aspect_features[f'{base_name}_tight'] = is_tight\n        aspect_features[f'{base_name}_strength'] = strength\n        aspect_features[f'{base_name}_exact_dist'] = exact_distance\n        aspect_features[f'{base_name}_applying'] = is_applying\n\n        feature_count += 5\n\n        # Progress indicator\n        if calc_count % progress_interval == 0 or calc_count == total_calculations:\n            progress_pct = (calc_count / total_calculations) * 100\n            logger.info(f\"    [{calc_count}/{total_calculations}] {progress_pct:.0f}% complete\")\n\n# Convert to DataFrame\ndf_aspects = pd.DataFrame(aspect_features)\n\n# Add date column for merging\ndf_aspects['date'] = df['date'].values\n\nend_time = datetime.now()\nelapsed = (end_time - start_time).total_seconds()\n\nlogger.info(f\"\\n  \u2713 Calculations complete in {elapsed:.1f} seconds\")\nlogger.info(f\"  \u2713 Generated {feature_count} aspect features\")\n\n# ============================================================================\n# STEP 6: Calculate Aggregate Aspect Metrics\n# ============================================================================\nlogger.info(\"\\n[6/7] Calculating aggregate aspect metrics...\")\n\n# Count total active aspects per day\naspect_active_cols = [col for col in df_aspects.columns if col.endswith('_active')]\ndf_aspects['total_aspects_active'] = df_aspects[aspect_active_cols].sum(axis=1)\n\n# Count tight aspects per day\naspect_tight_cols = [col for col in df_aspects.columns if col.endswith('_tight')]\ndf_aspects['total_aspects_tight'] = df_aspects[aspect_tight_cols].sum(axis=1)\n\n# Average aspect strength (for active aspects)\naspect_strength_cols = [col for col in df_aspects.columns if col.endswith('_strength')]\ndf_aspects['avg_aspect_strength'] = df_aspects[aspect_strength_cols].mean(axis=1)\n\n# Count applying aspects (momentum indicator)\naspect_applying_cols = [col for col in df_aspects.columns if col.endswith('_applying')]\ndf_aspects['total_aspects_applying'] = df_aspects[aspect_applying_cols].sum(axis=1)\n\n# Count by aspect type\nfor aspect_name in ASPECT_DEFINITIONS.keys():\n    type_cols = [col for col in aspect_active_cols if f'_{aspect_name}_active' in col]\n    df_aspects[f'count_{aspect_name}s'] = df_aspects[type_cols].sum(axis=1)\n\nlogger.info(f\"  \u2713 Added {5 + len(ASPECT_DEFINITIONS)} aggregate metrics\")\n\n# ============================================================================\n# STEP 7: Save and Validate\n# ============================================================================\nlogger.info(\"\\n[7/7] Saving aspect features...\")\n\ntry:\n    df_aspects.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow')\n    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n    logger.info(f\"  \u2713 Saved: aspects_features.parquet\")\n    logger.info(f\"  \u2713 File size: {file_size_mb:.2f} MB\")\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not save file\")\n    logger.critical(f\"  Error: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# SAMPLE OUTPUT & VALIDATION\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"VALIDATION: Sun-Moon Aspects Analysis\")\nlogger.info(\"=\" * 70)\n\n# Check if Sun-Moon aspects ever occur\nsun_moon_cols = [col for col in df_aspects.columns if col.startswith('sun_moon_')]\nsun_moon_active = df_aspects[[col for col in sun_moon_cols if col.endswith('_active')]].sum()\n\nlogger.info(\"\\nSun-Moon Aspect Occurrence (Total Days Active):\")\nfor col in sun_moon_active.index:\n    aspect_type = col.replace('sun_moon_', '').replace('_active', '')\n    count = sun_moon_active[col]\n    pct = (count / len(df_aspects)) * 100\n    logger.info(f\"  \u2022 {aspect_type.capitalize()}: {count} days ({pct:.1f}%)\")\n\n# Find a day with active Sun-Moon aspects\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Days WITH Sun-Moon Aspects\")\nlogger.info(\"=\" * 70)\n\n# Find days where ANY sun-moon aspect is active\nany_sun_moon_active = df_aspects[[col for col in sun_moon_cols if col.endswith('_active')]].sum(axis=1) > 0\ndays_with_aspects = df_aspects[any_sun_moon_active]\n\nif len(days_with_aspects) > 0:\n    logger.info(f\"\\nFound {len(days_with_aspects)} days with Sun-Moon aspects\")\n    logger.info(\"\\nShowing first 5 days with active aspects:\")\n\n    sample_cols = ['date',\n                   'sun_moon_conjunction_active',\n                   'sun_moon_conjunction_tight',\n                   'sun_moon_conjunction_strength',\n                   'sun_moon_opposition_active',\n                   'sun_moon_square_active',\n                   'sun_moon_trine_active']\n\n    existing_cols = [col for col in sample_cols if col in days_with_aspects.columns]\n    print(\"\\n\" + tabulate(days_with_aspects[existing_cols].head(5),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".3f\"))\nelse:\n    logger.warning(\"\\n\u26a0 WARNING: No Sun-Moon aspects found in dataset!\")\n    logger.info(\"  This may indicate an issue with aspect calculations.\")\n\n# Show actual Sun-Moon positions for verification\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"VERIFICATION: Sun & Moon Positions (First 5 Days)\")\nlogger.info(\"=\" * 70)\n\n# Get original planetary data\nverify_cols = ['date', 'sun_longitude', 'moon_longitude']\nif all(col in df.columns for col in verify_cols):\n    df_verify = df[verify_cols].head(5).copy()\n\n    # Calculate angular distance manually\n    df_verify['angular_distance'] = angular_distance(\n        df['sun_longitude'].head(5).values,\n        df['moon_longitude'].head(5).values\n    )\n\n    print(\"\\n\" + tabulate(df_verify, headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".2f\"))\n\n    logger.info(\"\\nExpected aspects based on angular distance:\")\n    logger.info(\"  \u2022 Conjunction (0\u00b0): distance < 12\u00b0 (Sun/Moon orb)\")\n    logger.info(\"  \u2022 Opposition (180\u00b0): |distance - 180\u00b0| < 12\u00b0\")\n    logger.info(\"  \u2022 Square (90\u00b0): |distance - 90\u00b0| < 9.6\u00b0\")\n    logger.info(\"  \u2022 Trine (120\u00b0): |distance - 120\u00b0| < 9.6\u00b0\")\n    logger.info(\"  \u2022 Sextile (60\u00b0): |distance - 60\u00b0| < 7.2\u00b0\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"AGGREGATE METRICS (All Days)\")\nlogger.info(\"=\" * 70)\n\nagg_cols = ['total_aspects_active', 'total_aspects_tight',\n            'avg_aspect_strength', 'count_conjunctions', 'count_squares',\n            'count_trines', 'count_oppositions', 'count_sextiles']\nexisting_agg = [col for col in agg_cols if col in df_aspects.columns]\n\nagg_stats = df_aspects[existing_agg].describe()\nprint(\"\\n\" + tabulate(agg_stats, headers='keys', tablefmt='grid', floatfmt=\".2f\"))\n\n# Validation statistics\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DATA QUALITY VALIDATION\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  Dataset shape: {df_aspects.shape}\")\nlogger.info(f\"  Total features: {len(df_aspects.columns)}\")\nlogger.info(f\"  Date range: {df_aspects['date'].min().date()} to {df_aspects['date'].max().date()}\")\n\n# Check for nulls\nnull_counts = df_aspects.isnull().sum().sum()\nlogger.info(f\"  Null values: {null_counts} ({null_counts / df_aspects.size * 100:.2f}%)\")\n\n# Sample statistics\nlogger.info(f\"\\n  Average active aspects per day: {df_aspects['total_aspects_active'].mean():.1f}\")\nlogger.info(f\"  Max active aspects on single day: {df_aspects['total_aspects_active'].max()}\")\nlogger.info(f\"  Days with tight aspects: {(df_aspects['total_aspects_tight'] > 0).sum()} ({(df_aspects['total_aspects_tight'] > 0).sum() / len(df_aspects) * 100:.1f}%)\")\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 2 (ASPECT FEATURES) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Integration Points for Downstream Phases:\")\nlogger.info(\"  \u2192 Phase 3 (Model Training):\")\nlogger.info(f\"    \u2022 {feature_count} aspect features ready for ML input\")\nlogger.info(\"    \u2022 Binary flags for classification, strength scores for regression\")\nlogger.info(\"  \u2192 Phase 4 (Backtesting):\")\nlogger.info(\"    \u2022 Use '_active' columns to filter specific aspect events\")\nlogger.info(\"    \u2022 Example: days_with_mars_saturn_square = df[df['mars_saturn_square_active'] == 1]\")\nlogger.info(\"  \u2192 Phase 5 (Insight Extraction):\")\nlogger.info(\"    \u2022 Feature names follow '{planet1}_{planet2}_{aspect}_{metric}' pattern\")\nlogger.info(\"    \u2022 Use for SHAP analysis and feature importance ranking\")\n\nlogger.info(\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(\"  1. \u2713 Planetary aspects calculated (Cell 4 complete)\")\nlogger.info(\"  2. \u25b6 Run Cell 5: Transit & Positional Features\")\nlogger.info(\"  3. \u25b6 Run Cell 6: Cyclic & Temporal Features\")\nlogger.info(\"  4. \u25b6 Run Cell 7: Advanced Astrological Indicators\")\nlogger.info(\"  5. \u25b6 Run Cell 8: Feature Integration & Final Dataset\")\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files:\")\nlogger.info(f\"  {OUTPUT_FILE}\")\nlogger.info(f\"  ({len(df_aspects)} rows \u00d7 {len(df_aspects.columns)} features)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "id": "iJ6FzdRg3Wk8"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0006"
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Transit & Positional Features (Phase 2 - Part 2 of 5)\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom tabulate import tabulate\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 2: FEATURE ENGINEERING\")\nlogger.info(\"Phase 2 Progress: Part 2 of 5 (Transits & Positions)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/6] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\nFEATURE_DATA_PATH = os.path.join(BASE_PATH, 'feature_data')\n\nINPUT_FILE = os.path.join(ALIGNED_DATA_PATH, 'master_aligned_dataset.parquet')\nOUTPUT_FILE = os.path.join(FEATURE_DATA_PATH, 'transit_features.parquet')\n\nlogger.info(f\"  \u2713 Input: master_aligned_dataset.parquet\")\nlogger.info(f\"  \u2713 Output: transit_features.parquet\")\n\n# ============================================================================\n# STEP 2: Load Master Aligned Data\n# ============================================================================\nlogger.info(\"\\n[2/6] Loading master aligned dataset...\")\n\nif not os.path.exists(INPUT_FILE):\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Input file not found\")\n    logger.info(\"  Please run Cell 3 first.\")\n    raise SystemExit(1)\n\ndf = pd.read_parquet(INPUT_FILE)\ndf['date'] = pd.to_datetime(df['date'])\n\nlogger.info(f\"  \u2713 Loaded dataset\")\nlogger.info(f\"  \u2713 Shape: {df.shape}\")\nlogger.info(f\"  \u2713 Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n\n# ============================================================================\n# STEP 3: Define Astrological Constants\n# ============================================================================\nlogger.info(\"\\n[3/6] Defining astrological constants...\")\n\n# Planets to process\nPLANETS = ['sun', 'moon', 'mercury', 'venus', 'mars',\n           'jupiter', 'saturn', 'rahu']\n\n# Zodiac signs (Vedic/Sidereal)\nZODIAC_SIGNS = [\n    'Aries', 'Taurus', 'Gemini', 'Cancer', 'Leo', 'Virgo',\n    'Liberia', 'Scorpio', 'Sagittarius', 'Capricorn', 'Aquarius', 'Pisces'\n]\n\n# Nakshatras (27 lunar mansions, 13\u00b020' each)\nNAKSHATRAS = [\n    'Ashwini', 'Bharani', 'Krittika', 'Rohini', 'Mrigashira', 'Ardra',\n    'Punarvasu', 'Pushya', 'Ashlesha', 'Magha', 'Purva Phalguni', 'Uttara Phalguni',\n    'Hasta', 'Chitra', 'Swati', 'Vishakha', 'Anuradha', 'Jyeshtha',\n    'Mula', 'Purva Ashadha', 'Uttara Ashadha', 'Shravana', 'Dhanishta', 'Shatabhisha',\n    'Purva Bhadrapada', 'Uttara Bhadrapada', 'Revati'\n]\n\n# Planetary dignities (exaltation/debilitation points in degrees)\nDIGNITY_POINTS = {\n    'sun': {'exalted': 10, 'debilitated': 190},      # Exalted in Aries 10\u00b0, Debilitated in Libra 10\u00b0\n    'moon': {'exalted': 33, 'debilitated': 213},     # Exalted in Taurus 3\u00b0, Debilitated in Scorpio 3\u00b0\n    'mercury': {'exalted': 165, 'debilitated': 345}, # Exalted in Virgo 15\u00b0, Debilitated in Pisces 15\u00b0\n    'venus': {'exalted': 357, 'debilitated': 177},   # Exalted in Pisces 27\u00b0, Debilitated in Virgo 27\u00b0\n    'mars': {'exalted': 298, 'debilitated': 118},    # Exalted in Capricorn 28\u00b0, Debilitated in Cancer 28\u00b0\n    'jupiter': {'exalted': 95, 'debilitated': 275},  # Exalted in Cancer 5\u00b0, Debilitated in Capricorn 5\u00b0\n    'saturn': {'exalted': 200, 'debilitated': 20},   # Exalted in Libra 20\u00b0, Debilitated in Aries 20\u00b0\n}\n\n# Retrograde speed thresholds (approximately when planet appears stationary)\nRETROGRADE_STATIONARY_THRESHOLD = 0.05  # degrees/day\n\nlogger.info(f\"  \u2713 Configured {len(PLANETS)} planets\")\nlogger.info(f\"  \u2713 Defined {len(ZODIAC_SIGNS)} zodiac signs\")\nlogger.info(f\"  \u2713 Defined {len(NAKSHATRAS)} nakshatras\")\nlogger.info(f\"  \u2713 Dignity points for 7 planets\")\n\n# ============================================================================\n# STEP 4: Calculate Zodiac Sign Features\n# ============================================================================\nlogger.info(\"\\n[4/6] Calculating zodiac sign positions...\")\n\ntransit_features = {'date': df['date'].values}\nfeature_count = 0\n\nfor planet in PLANETS:\n    lon_col = f'{planet}_longitude'\n\n    if lon_col not in df.columns:\n        logger.warning(f\"  \u26a0 Skipping {planet} - longitude column not found\")\n        continue\n\n    longitude = df[lon_col].values\n\n    # Calculate zodiac sign (0-11, where 0=Aries, 1=Taurus, etc.)\n    # Each sign is 30 degrees\n    sign_index = (longitude // 30).astype(np.int8)\n\n    # Calculate degrees within sign (0-29.99)\n    degrees_in_sign = longitude % 30\n\n    # Detect sign ingress (planet just entered new sign)\n    # Check if degrees_in_sign < previous day's degrees (wrapped around)\n    ingress = np.zeros(len(longitude), dtype=np.int8)\n    if len(longitude) > 1:\n        ingress[1:] = (degrees_in_sign[1:] < degrees_in_sign[:-1]).astype(np.int8)\n\n    # Store features\n    transit_features[f'{planet}_sign'] = sign_index\n    transit_features[f'{planet}_degrees_in_sign'] = degrees_in_sign.astype(np.float32)\n    transit_features[f'{planet}_sign_ingress'] = ingress\n\n    feature_count += 3\n\nlogger.info(f\"  \u2713 Created {feature_count} zodiac sign features\")\n\n# ============================================================================\n# STEP 5: Calculate Nakshatra Features\n# ============================================================================\nlogger.info(\"\\n[5/6] Calculating nakshatra positions...\")\n\n# Focus on Moon and Sun (most important for nakshatras)\nfor planet in ['moon', 'sun']:\n    lon_col = f'{planet}_longitude'\n\n    if lon_col not in df.columns:\n        continue\n\n    longitude = df[lon_col].values\n\n    # Each nakshatra is 13.333... degrees (360/27)\n    nakshatra_width = 360.0 / 27\n    nakshatra_index = (longitude / nakshatra_width).astype(np.int8)\n\n    # Nakshatra pada (quarter): 1-4\n    # Each nakshatra has 4 padas of 3\u00b020' each\n    pada_within_nakshatra = ((longitude % nakshatra_width) / (nakshatra_width / 4))\n    pada = (pada_within_nakshatra.astype(np.int8) + 1).clip(1, 4)  # 1-4\n\n    # Degrees within nakshatra\n    degrees_in_nakshatra = longitude % nakshatra_width\n\n    # Detect nakshatra change\n    nakshatra_change = np.zeros(len(longitude), dtype=np.int8)\n    if len(longitude) > 1:\n        nakshatra_change[1:] = (nakshatra_index[1:] != nakshatra_index[:-1]).astype(np.int8)\n\n    # Store features\n    transit_features[f'{planet}_nakshatra'] = nakshatra_index\n    transit_features[f'{planet}_nakshatra_pada'] = pada\n    transit_features[f'{planet}_degrees_in_nakshatra'] = degrees_in_nakshatra.astype(np.float32)\n    transit_features[f'{planet}_nakshatra_change'] = nakshatra_change\n\n    feature_count += 4\n\nlogger.info(f\"  \u2713 Created nakshatra features for Moon and Sun\")\n\n# ============================================================================\n# STEP 6: Calculate Speed & Motion Features\n# ============================================================================\nlogger.info(\"\\n[6/6] Calculating planetary motion features...\")\n\nfor planet in PLANETS:\n    if planet == 'rahu':  # Rahu is always retrograde by definition\n        continue\n\n    speed_col = f'{planet}_speed'\n\n    if speed_col not in df.columns:\n        continue\n\n    speed = df[speed_col].values\n\n    # Retrograde flag (speed < 0)\n    is_retrograde = (speed < 0).astype(np.int8)\n\n    # Stationary flag (speed near 0, within threshold)\n    is_stationary = (np.abs(speed) < RETROGRADE_STATIONARY_THRESHOLD).astype(np.int8)\n\n    # Speed category: -1 (retrograde), 0 (stationary), 1 (direct)\n    speed_category = np.zeros_like(speed, dtype=np.int8)\n    speed_category[speed < -RETROGRADE_STATIONARY_THRESHOLD] = -1  # Retrograde\n    speed_category[speed > RETROGRADE_STATIONARY_THRESHOLD] = 1    # Direct\n    # Stationary = 0 (default)\n\n    # Detect station (change in direction)\n    # Station occurs when speed crosses zero\n    station = np.zeros(len(speed), dtype=np.int8)\n    if len(speed) > 1:\n        # Check for sign change in speed\n        station[1:] = ((speed[:-1] * speed[1:]) < 0).astype(np.int8)\n\n    # Store features\n    transit_features[f'{planet}_retrograde'] = is_retrograde\n    transit_features[f'{planet}_stationary'] = is_stationary\n    transit_features[f'{planet}_speed_category'] = speed_category\n    transit_features[f'{planet}_station'] = station\n\n    feature_count += 4\n\nlogger.info(f\"  \u2713 Created motion features for {len(PLANETS)-1} planets\")\n\n# ============================================================================\n# STEP 7: Calculate Dignity Features\n# ============================================================================\nlogger.info(\"\\n[7/7] Calculating planetary dignity features...\")\n\nfor planet, dignity_data in DIGNITY_POINTS.items():\n    lon_col = f'{planet}_longitude'\n\n    if lon_col not in df.columns:\n        continue\n\n    longitude = df[lon_col].values\n\n    exalted_point = dignity_data['exalted']\n    debilitated_point = dignity_data['debilitated']\n\n    # Calculate angular distance to exaltation point\n    dist_to_exalted = np.abs(longitude - exalted_point)\n    dist_to_exalted = np.minimum(dist_to_exalted, 360 - dist_to_exalted)\n\n    # Calculate angular distance to debilitation point\n    dist_to_debilitated = np.abs(longitude - debilitated_point)\n    dist_to_debilitated = np.minimum(dist_to_debilitated, 360 - dist_to_debilitated)\n\n    # Exalted flag (within 5 degrees of exaltation point)\n    is_exalted = (dist_to_exalted <= 5).astype(np.int8)\n\n    # Debilitated flag (within 5 degrees of debilitation point)\n    is_debilitated = (dist_to_debilitated <= 5).astype(np.int8)\n\n    # Dignity score: positive near exaltation, negative near debilitation\n    # Scale: +1.0 at exaltation point, -1.0 at debilitation point, 0 neutral\n    dignity_score = np.zeros_like(longitude, dtype=np.float32)\n\n    # Positive contribution from exaltation (0 to 1)\n    exalted_contribution = np.maximum(0, 1 - (dist_to_exalted / 30))\n\n    # Negative contribution from debilitation (0 to -1)\n    debilitated_contribution = -np.maximum(0, 1 - (dist_to_debilitated / 30))\n\n    dignity_score = exalted_contribution + debilitated_contribution\n\n    # Store features\n    transit_features[f'{planet}_is_exalted'] = is_exalted\n    transit_features[f'{planet}_is_debilitated'] = is_debilitated\n    transit_features[f'{planet}_dignity_score'] = dignity_score\n\n    feature_count += 3\n\nlogger.info(f\"  \u2713 Created dignity features for 7 planets\")\n\n# ============================================================================\n# STEP 8: Create DataFrame and Save\n# ============================================================================\nlogger.info(f\"\\n[8/8] Saving transit features...\")\n\ndf_transit = pd.DataFrame(transit_features)\n\ntry:\n    df_transit.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow')\n    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n    logger.info(f\"  \u2713 Saved: transit_features.parquet\")\n    logger.info(f\"  \u2713 File size: {file_size_mb:.2f} MB\")\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not save file\")\n    logger.critical(f\"  Error: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# VALIDATION & SAMPLE OUTPUT\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Moon Positional Features (First 5 Days)\")\nlogger.info(\"=\" * 70)\n\nsample_cols = ['date', 'moon_sign', 'moon_degrees_in_sign',\n               'moon_nakshatra', 'moon_nakshatra_pada',\n               'moon_retrograde']\n\nexisting_cols = [col for col in sample_cols if col in df_transit.columns]\nsample_data = df_transit[existing_cols].head(5).copy()\n\n# Add sign names for readability\nif 'moon_sign' in sample_data.columns:\n    sample_data['moon_sign_name'] = sample_data['moon_sign'].apply(lambda x: ZODIAC_SIGNS[x] if 0 <= x < 12 else 'Unknown')\n\n# Add nakshatra names\nif 'moon_nakshatra' in sample_data.columns:\n    sample_data['moon_nakshatra_name'] = sample_data['moon_nakshatra'].apply(lambda x: NAKSHATRAS[x] if 0 <= x < 27 else 'Unknown')\n\nprint(\"\\n\" + tabulate(sample_data, headers='keys', tablefmt='grid',\n                     showindex=False, floatfmt=\".2f\"))\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Retrograde Planets (Days with Retrogrades)\")\nlogger.info(\"=\" * 70)\n\n# Find days with any retrograde planets\nretrograde_cols = [col for col in df_transit.columns if col.endswith('_retrograde')]\nany_retrograde = df_transit[retrograde_cols].sum(axis=1) > 0\ndays_with_rx = df_transit[any_retrograde]\n\nif len(days_with_rx) > 0:\n    logger.info(f\"\\nFound {len(days_with_rx)} days with retrograde planets ({len(days_with_rx)/len(df_transit)*100:.1f}%)\")\n\n    rx_sample_cols = ['date'] + [col for col in retrograde_cols if col in days_with_rx.columns][:5]\n    logger.info(\"\\nFirst 5 days with retrogrades:\")\n    print(\"\\n\" + tabulate(days_with_rx[rx_sample_cols].head(5),\n                         headers='keys', tablefmt='grid', showindex=False))\nelse:\n    logger.info(\"\\n  No retrograde periods found in dataset\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Planetary Dignity (Jupiter Exalted)\")\nlogger.info(\"=\" * 70)\n\nif 'jupiter_is_exalted' in df_transit.columns:\n    jupiter_exalted_days = df_transit[df_transit['jupiter_is_exalted'] == 1]\n\n    if len(jupiter_exalted_days) > 0:\n        logger.info(f\"\\nJupiter in exaltation: {len(jupiter_exalted_days)} days ({len(jupiter_exalted_days)/len(df_transit)*100:.1f}%)\")\n\n        dignity_cols = ['date', 'jupiter_is_exalted', 'jupiter_dignity_score']\n        existing_dignity = [col for col in dignity_cols if col in jupiter_exalted_days.columns]\n\n        logger.info(\"\\nFirst 5 days:\")\n        print(\"\\n\" + tabulate(jupiter_exalted_days[existing_dignity].head(5),\n                             headers='keys', tablefmt='grid',\n                             showindex=False, floatfmt=\".3f\"))\n    else:\n        logger.info(\"\\n  Jupiter not in exaltation during this period\")\n\n# ============================================================================\n# DATA QUALITY VALIDATION\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DATA QUALITY VALIDATION\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  Dataset shape: {df_transit.shape}\")\nlogger.info(f\"  Total features: {len(df_transit.columns)}\")\nlogger.info(f\"  Date range: {df_transit['date'].min().date()} to {df_transit['date'].max().date()}\")\n\nnull_counts = df_transit.isnull().sum().sum()\nlogger.info(f\"  Null values: {null_counts} ({null_counts / df_transit.size * 100:.2f}%)\")\n\n# Feature category counts\nlogger.info(\"\\n  Feature breakdown:\")\nsign_features = len([col for col in df_transit.columns if '_sign' in col])\nnakshatra_features = len([col for col in df_transit.columns if '_nakshatra' in col])\nmotion_features = len([col for col in df_transit.columns if any(x in col for x in ['_retrograde', '_stationary', '_station'])])\ndignity_features = len([col for col in df_transit.columns if any(x in col for x in ['_exalted', '_debilitated', '_dignity'])])\n\nlogger.info(f\"    \u2022 Zodiac sign features: {sign_features}\")\nlogger.info(f\"    \u2022 Nakshatra features: {nakshatra_features}\")\nlogger.info(f\"    \u2022 Motion features: {motion_features}\")\nlogger.info(f\"    \u2022 Dignity features: {dignity_features}\")\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 2 (TRANSIT FEATURES) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Integration Points for Downstream Phases:\")\nlogger.info(\"  \u2192 Phase 3 (Model Training):\")\nlogger.info(\"    \u2022 Sign/nakshatra as categorical features\")\nlogger.info(\"    \u2022 Retrograde/dignity as binary flags\")\nlogger.info(\"    \u2022 Dignity scores as continuous features\")\nlogger.info(\"  \u2192 Phase 4 (Backtesting):\")\nlogger.info(\"    \u2022 Filter on '_ingress' columns for sign change events\")\nlogger.info(\"    \u2022 Filter on '_retrograde' for Mercury Rx periods\")\nlogger.info(\"  \u2192 Phase 5 (Insight Extraction):\")\nlogger.info(\"    \u2022 Analyze sector sensitivity to retrogrades\")\nlogger.info(\"    \u2022 Identify most influential nakshatras\")\n\nlogger.info(\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(\"  1. \u2713 Planetary aspects calculated (Cell 4)\")\nlogger.info(\"  2. \u2713 Transit & positional features (Cell 5 complete)\")\nlogger.info(\"  3. \u25b6 Run Cell 6: Cyclic & Temporal Features\")\nlogger.info(\"  4. \u25b6 Run Cell 7: Advanced Astrological Indicators\")\nlogger.info(\"  5. \u25b6 Run Cell 8: Feature Integration & Final Dataset\")\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files:\")\nlogger.info(f\"  {OUTPUT_FILE}\")\nlogger.info(f\"  ({len(df_transit)} rows \u00d7 {len(df_transit.columns)} features)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "id": "gj31Khp06jEt"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0007"
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Cyclic & Temporal Features (Phase 2 - Part 3 of 5)\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom tabulate import tabulate\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 2: FEATURE ENGINEERING\")\nlogger.info(\"Phase 2 Progress: Part 3 of 5 (Cyclic & Temporal)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/6] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\nFEATURE_DATA_PATH = os.path.join(BASE_PATH, 'feature_data')\n\nINPUT_FILE = os.path.join(ALIGNED_DATA_PATH, 'master_aligned_dataset.parquet')\nOUTPUT_FILE = os.path.join(FEATURE_DATA_PATH, 'temporal_features.parquet')\n\nlogger.info(f\"  \u2713 Input: master_aligned_dataset.parquet\")\nlogger.info(f\"  \u2713 Output: temporal_features.parquet\")\n\n# ============================================================================\n# STEP 2: Load Master Aligned Data\n# ============================================================================\nlogger.info(\"\\n[2/6] Loading master aligned dataset...\")\n\nif not os.path.exists(INPUT_FILE):\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Input file not found\")\n    logger.info(\"  Please run Cell 3 first.\")\n    raise SystemExit(1)\n\ndf = pd.read_parquet(INPUT_FILE)\ndf['date'] = pd.to_datetime(df['date'])\n\nlogger.info(f\"  \u2713 Loaded dataset\")\nlogger.info(f\"  \u2713 Shape: {df.shape}\")\nlogger.info(f\"  \u2713 Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n\n# ============================================================================\n# STEP 3: Calculate Lunar Cycle Features\n# ============================================================================\nlogger.info(\"\\n[3/6] Calculating lunar cycle features...\")\n\n# Get Sun and Moon longitudes\nsun_lon = df['sun_longitude'].values\nmoon_lon = df['moon_longitude'].values\n\n# Calculate Moon phase angle (elongation from Sun)\n# Phase angle = Moon longitude - Sun longitude\n# 0\u00b0 = New Moon, 90\u00b0 = First Quarter, 180\u00b0 = Full Moon, 270\u00b0 = Last Quarter\nmoon_phase_angle = (moon_lon - sun_lon) % 360.0\n\n# Categorize moon phase (8 phases)\n# New Moon: 0\u00b0 \u00b1 22.5\u00b0 (337.5\u00b0 - 22.5\u00b0)\n# Waxing Crescent: 22.5\u00b0 - 67.5\u00b0\n# First Quarter: 67.5\u00b0 - 112.5\u00b0\n# Waxing Gibbous: 112.5\u00b0 - 157.5\u00b0\n# Full Moon: 157.5\u00b0 - 202.5\u00b0\n# Waning Gibbous: 202.5\u00b0 - 247.5\u00b0\n# Last Quarter: 247.5\u00b0 - 292.5\u00b0\n# Waning Crescent: 292.5\u00b0 - 337.5\u00b0\n\ndef get_moon_phase_category(phase_angle):\n    \"\"\"Convert phase angle to category (0-7)\"\"\"\n    # Adjust so New Moon is centered at 0\n    adjusted = (phase_angle + 22.5) % 360\n    category = int(adjusted / 45)\n    return category\n\nmoon_phase_category = np.array([get_moon_phase_category(angle) for angle in moon_phase_angle], dtype=np.int8)\n\n# Phase names for reference\nPHASE_NAMES = [\n    'New Moon', 'Waxing Crescent', 'First Quarter', 'Waxing Gibbous',\n    'Full Moon', 'Waning Gibbous', 'Last Quarter', 'Waning Crescent'\n]\n\n# Binary flags for key phases\nis_new_moon = ((moon_phase_angle < 15) | (moon_phase_angle > 345)).astype(np.int8)\nis_full_moon = ((moon_phase_angle > 165) & (moon_phase_angle < 195)).astype(np.int8)\nis_waxing = ((moon_phase_angle > 0) & (moon_phase_angle < 180)).astype(np.int8)\nis_waning = ((moon_phase_angle >= 180) & (moon_phase_angle < 360)).astype(np.int8)\n\n# Calculate days to next New Moon and Full Moon (approximate)\n# Average lunar cycle is 29.53 days\nLUNAR_CYCLE_DAYS = 29.53\n\n# Days since New Moon (phase angle / 360 * cycle length)\ndays_since_new_moon = (moon_phase_angle / 360.0) * LUNAR_CYCLE_DAYS\n\n# Days until next New Moon\ndays_to_new_moon = LUNAR_CYCLE_DAYS - days_since_new_moon\n\n# Days to/from Full Moon\ndays_to_full_moon = np.where(\n    moon_phase_angle < 180,\n    (180 - moon_phase_angle) / 360.0 * LUNAR_CYCLE_DAYS,  # Before full\n    (540 - moon_phase_angle) / 360.0 * LUNAR_CYCLE_DAYS   # After full\n)\n\ntemporal_features = {\n    'date': df['date'].values,\n    'moon_phase_angle': moon_phase_angle.astype(np.float32),\n    'moon_phase_category': moon_phase_category,\n    'is_new_moon': is_new_moon,\n    'is_full_moon': is_full_moon,\n    'is_waxing': is_waxing,\n    'is_waning': is_waning,\n    'days_since_new_moon': days_since_new_moon.astype(np.float32),\n    'days_to_new_moon': days_to_new_moon.astype(np.float32),\n    'days_to_full_moon': days_to_full_moon.astype(np.float32),\n}\n\nlogger.info(f\"  \u2713 Created 10 lunar cycle features\")\n\n# ============================================================================\n# STEP 4: Calculate Planetary Cycle Features\n# ============================================================================\nlogger.info(\"\\n[4/6] Calculating planetary cycle features...\")\n\n# Calculate days in current sign for slower planets (important for timing)\nSLOW_PLANETS = ['jupiter', 'saturn']  # These stay in signs for months/years\n\nfor planet in SLOW_PLANETS:\n    lon_col = f'{planet}_longitude'\n    speed_col = f'{planet}_speed'\n\n    if lon_col not in df.columns or speed_col not in df.columns:\n        continue\n\n    longitude = df[lon_col].values\n    speed = df[speed_col].values\n\n    # Calculate degrees within current sign (0-30)\n    degrees_in_sign = longitude % 30\n\n    # Estimate days in current sign (degrees traveled / daily speed)\n    # Protect against division by zero\n    safe_speed = np.where(np.abs(speed) < 0.001, 0.001, speed)\n    days_in_sign = degrees_in_sign / np.abs(safe_speed)\n\n    # Estimate days until sign change (degrees remaining / daily speed)\n    degrees_remaining = 30 - degrees_in_sign\n    days_to_sign_change = degrees_remaining / np.abs(safe_speed)\n\n    # Cap at reasonable values (max 365 days)\n    days_in_sign = np.clip(days_in_sign, 0, 365).astype(np.float32)\n    days_to_sign_change = np.clip(days_to_sign_change, 0, 365).astype(np.float32)\n\n    temporal_features[f'{planet}_days_in_sign'] = days_in_sign\n    temporal_features[f'{planet}_days_to_sign_change'] = days_to_sign_change\n\nlogger.info(f\"  \u2713 Created planetary cycle features for {len(SLOW_PLANETS)} planets\")\n\n# ============================================================================\n# STEP 5: Calculate Calendar-Based Features\n# ============================================================================\nlogger.info(\"\\n[5/6] Calculating calendar-based features...\")\n\ndates = df['date']\n\n# Day of week (0=Monday, 6=Sunday)\nday_of_week = dates.dt.dayofweek.values.astype(np.int8)\n\n# Is weekend?\nis_weekend = (day_of_week >= 5).astype(np.int8)\n\n# Month (1-12)\nmonth = dates.dt.month.values.astype(np.int8)\n\n# Quarter (1-4)\nquarter = dates.dt.quarter.values.astype(np.int8)\n\n# Day of month (1-31)\nday_of_month = dates.dt.day.values.astype(np.int8)\n\n# Week of year (1-53)\nweek_of_year = dates.dt.isocalendar().week.values.astype(np.int8)\n\n# Year (for trend analysis)\nyear = dates.dt.year.values.astype(np.int16)\n\n# Cyclical encodings for periodic features (sin/cos transformation)\n# This helps ML models understand cyclical nature (Dec and Jan are close)\n\n# Month cyclical encoding\nmonth_sin = np.sin(2 * np.pi * month / 12).astype(np.float32)\nmonth_cos = np.cos(2 * np.pi * month / 12).astype(np.float32)\n\n# Day of week cyclical encoding\ndow_sin = np.sin(2 * np.pi * day_of_week / 7).astype(np.float32)\ndow_cos = np.cos(2 * np.pi * day_of_week / 7).astype(np.float32)\n\n# Add to features\ntemporal_features.update({\n    'day_of_week': day_of_week,\n    'is_weekend': is_weekend,\n    'month': month,\n    'quarter': quarter,\n    'day_of_month': day_of_month,\n    'week_of_year': week_of_year,\n    'year': year,\n    'month_sin': month_sin,\n    'month_cos': month_cos,\n    'dow_sin': dow_sin,\n    'dow_cos': dow_cos,\n})\n\nlogger.info(f\"  \u2713 Created 12 calendar-based features\")\n\n# ============================================================================\n# STEP 6: Calculate Mercury Retrograde Periods (Special Feature)\n# ============================================================================\nlogger.info(\"\\n[6/6] Calculating Mercury retrograde periods...\")\n\n# Mercury retrograde is famous in astrology for communication/tech disruptions\nif 'mercury_speed' in df.columns:\n    mercury_speed = df['mercury_speed'].values\n\n    # Is Mercury retrograde?\n    mercury_rx = (mercury_speed < 0).astype(np.int8)\n\n    # Calculate consecutive days of Mercury Rx\n    mercury_rx_days = np.zeros(len(mercury_rx), dtype=np.int16)\n\n    count = 0\n    for i in range(len(mercury_rx)):\n        if mercury_rx[i] == 1:\n            count += 1\n            mercury_rx_days[i] = count\n        else:\n            count = 0\n\n    # Days until next Mercury retrograde (approximate)\n    # Find next Rx period for each day\n    days_to_mercury_rx = np.zeros(len(mercury_rx), dtype=np.int16)\n\n    for i in range(len(mercury_rx)):\n        if mercury_rx[i] == 1:\n            days_to_mercury_rx[i] = 0  # Already in Rx\n        else:\n            # Look ahead to find next Rx\n            found = False\n            for j in range(i + 1, min(i + 120, len(mercury_rx))):  # Look ahead max 120 days\n                if mercury_rx[j] == 1:\n                    days_to_mercury_rx[i] = j - i\n                    found = True\n                    break\n            if not found:\n                days_to_mercury_rx[i] = 120  # Cap at 120 if not found\n\n    temporal_features['mercury_retrograde'] = mercury_rx\n    temporal_features['mercury_rx_day_count'] = mercury_rx_days\n    temporal_features['days_to_mercury_rx'] = days_to_mercury_rx\n\n    logger.info(f\"  \u2713 Created 3 Mercury retrograde features\")\nelse:\n    logger.warning(f\"  \u26a0 Skipping Mercury Rx features - speed column not found\")\n\n# ============================================================================\n# STEP 7: Create DataFrame and Save\n# ============================================================================\nlogger.info(f\"\\n[7/7] Saving temporal features...\")\n\ndf_temporal = pd.DataFrame(temporal_features)\n\ntry:\n    df_temporal.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow')\n    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n    logger.info(f\"  \u2713 Saved: temporal_features.parquet\")\n    logger.info(f\"  \u2713 File size: {file_size_mb:.2f} MB\")\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not save file\")\n    logger.critical(f\"  Error: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# VALIDATION & SAMPLE OUTPUT\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Lunar Cycle Features (First 10 Days)\")\nlogger.info(\"=\" * 70)\n\nsample_cols = ['date', 'moon_phase_angle', 'moon_phase_category',\n               'is_new_moon', 'is_full_moon', 'days_to_new_moon', 'days_to_full_moon']\n\nexisting_cols = [col for col in sample_cols if col in df_temporal.columns]\nsample_data = df_temporal[existing_cols].head(10).copy()\n\n# Add phase name for readability\nif 'moon_phase_category' in sample_data.columns:\n    sample_data['phase_name'] = sample_data['moon_phase_category'].apply(\n        lambda x: PHASE_NAMES[x] if 0 <= x < 8 else 'Unknown'\n    )\n\nprint(\"\\n\" + tabulate(sample_data, headers='keys', tablefmt='grid',\n                     showindex=False, floatfmt=\".1f\"))\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: New Moon and Full Moon Events\")\nlogger.info(\"=\" * 70)\n\n# Find actual New Moon and Full Moon days\nnew_moon_days = df_temporal[df_temporal['is_new_moon'] == 1]\nfull_moon_days = df_temporal[df_temporal['is_full_moon'] == 1]\n\nlogger.info(f\"\\nNew Moons in dataset: {len(new_moon_days)} days\")\nlogger.info(f\"Full Moons in dataset: {len(full_moon_days)} days\")\n\nif len(new_moon_days) > 0:\n    logger.info(\"\\nFirst 5 New Moon dates:\")\n    nm_cols = ['date', 'moon_phase_angle', 'days_since_new_moon']\n    print(\"\\n\" + tabulate(new_moon_days[nm_cols].head(5),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".2f\"))\n\nif len(full_moon_days) > 0:\n    logger.info(\"\\nFirst 5 Full Moon dates:\")\n    fm_cols = ['date', 'moon_phase_angle', 'days_to_full_moon']\n    print(\"\\n\" + tabulate(full_moon_days[fm_cols].head(5),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".2f\"))\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Mercury Retrograde Periods\")\nlogger.info(\"=\" * 70)\n\nif 'mercury_retrograde' in df_temporal.columns:\n    mercury_rx_periods = df_temporal[df_temporal['mercury_retrograde'] == 1]\n\n    logger.info(f\"\\nMercury retrograde days: {len(mercury_rx_periods)} ({len(mercury_rx_periods)/len(df_temporal)*100:.1f}%)\")\n\n    if len(mercury_rx_periods) > 0:\n        # Find start of retrograde periods\n        rx_starts = mercury_rx_periods[mercury_rx_periods['mercury_rx_day_count'] == 1]\n\n        logger.info(f\"Number of Mercury Rx periods: {len(rx_starts)}\")\n        logger.info(\"\\nFirst 5 Mercury Rx period start dates:\")\n\n        rx_cols = ['date', 'mercury_rx_day_count', 'days_to_mercury_rx']\n        print(\"\\n\" + tabulate(rx_starts[rx_cols].head(5),\n                             headers='keys', tablefmt='grid',\n                             showindex=False))\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Calendar Features (First 5 Days)\")\nlogger.info(\"=\" * 70)\n\ncalendar_cols = ['date', 'day_of_week', 'is_weekend', 'month', 'quarter', 'year']\nexisting_cal = [col for col in calendar_cols if col in df_temporal.columns]\n\ncalendar_sample = df_temporal[existing_cal].head(5).copy()\n\n# Add day name\nif 'day_of_week' in calendar_sample.columns:\n    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    calendar_sample['day_name'] = calendar_sample['day_of_week'].apply(\n        lambda x: day_names[x] if 0 <= x < 7 else 'Unknown'\n    )\n\nprint(\"\\n\" + tabulate(calendar_sample, headers='keys', tablefmt='grid', showindex=False))\n\n# ============================================================================\n# DATA QUALITY VALIDATION\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DATA QUALITY VALIDATION\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  Dataset shape: {df_temporal.shape}\")\nlogger.info(f\"  Total features: {len(df_temporal.columns)}\")\nlogger.info(f\"  Date range: {df_temporal['date'].min().date()} to {df_temporal['date'].max().date()}\")\n\nnull_counts = df_temporal.isnull().sum().sum()\nlogger.info(f\"  Null values: {null_counts} ({null_counts / df_temporal.size * 100:.2f}%)\")\n\n# Feature statistics\nlogger.info(\"\\n  Feature breakdown:\")\nlunar_features = len([col for col in df_temporal.columns if 'moon' in col or 'lunar' in col])\ncalendar_features = len([col for col in df_temporal.columns if any(x in col for x in ['day', 'week', 'month', 'quarter', 'year', 'weekend'])])\nmercury_features = len([col for col in df_temporal.columns if 'mercury' in col])\ncycle_features = len([col for col in df_temporal.columns if 'days_in' in col or 'days_to' in col])\n\nlogger.info(f\"    \u2022 Lunar cycle features: {lunar_features}\")\nlogger.info(f\"    \u2022 Calendar features: {calendar_features}\")\nlogger.info(f\"    \u2022 Mercury Rx features: {mercury_features}\")\nlogger.info(f\"    \u2022 Planetary cycle features: {cycle_features}\")\n\n# Value ranges\nlogger.info(\"\\n  Value ranges:\")\nif 'moon_phase_angle' in df_temporal.columns:\n    logger.info(f\"    \u2022 Moon phase angle: {df_temporal['moon_phase_angle'].min():.1f}\u00b0 to {df_temporal['moon_phase_angle'].max():.1f}\u00b0\")\n\nif 'days_to_new_moon' in df_temporal.columns:\n    logger.info(f\"    \u2022 Days to new moon: {df_temporal['days_to_new_moon'].min():.1f} to {df_temporal['days_to_new_moon'].max():.1f}\")\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 2 (TEMPORAL FEATURES) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Integration Points for Downstream Phases:\")\nlogger.info(\"  \u2192 Phase 3 (Model Training):\")\nlogger.info(\"    \u2022 Moon phase as cyclical feature (angle + category)\")\nlogger.info(\"    \u2022 Calendar features for seasonal patterns\")\nlogger.info(\"    \u2022 Mercury Rx as binary classification feature\")\nlogger.info(\"  \u2192 Phase 4 (Backtesting):\")\nlogger.info(\"    \u2022 Filter on 'is_new_moon'/'is_full_moon' for lunar event studies\")\nlogger.info(\"    \u2022 Isolate Mercury Rx periods for sector analysis\")\nlogger.info(\"  \u2192 Phase 5 (Insight Extraction):\")\nlogger.info(\"    \u2022 Quantify Mercury Rx impact per sector\")\nlogger.info(\"    \u2022 Identify most sensitive moon phases\")\n\nlogger.info(\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(\"  1. \u2713 Planetary aspects calculated (Cell 4)\")\nlogger.info(\"  2. \u2713 Transit & positional features (Cell 5)\")\nlogger.info(\"  3. \u2713 Cyclic & temporal features (Cell 6 complete)\")\nlogger.info(\"  4. \u25b6 Run Cell 7: Advanced Astrological Indicators\")\nlogger.info(\"  5. \u25b6 Run Cell 8: Feature Integration & Final Dataset\")\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files:\")\nlogger.info(f\"  {OUTPUT_FILE}\")\nlogger.info(f\"  ({len(df_temporal)} rows \u00d7 {len(df_temporal.columns)} features)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "id": "N_ml-nZ2FEtT"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0008"
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Advanced Astrological Indicators (Phase 2 - Part 4 of 5)\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport itertools\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 2: FEATURE ENGINEERING\")\nlogger.info(\"Phase 2 Progress: Part 4 of 5 (Advanced Indicators)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/6] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\nFEATURE_DATA_PATH = os.path.join(BASE_PATH, 'feature_data')\n\nINPUT_FILE = os.path.join(ALIGNED_DATA_PATH, 'master_aligned_dataset.parquet')\nOUTPUT_FILE = os.path.join(FEATURE_DATA_PATH, 'advanced_features.parquet')\n\nlogger.info(f\"  \u2713 Input: master_aligned_dataset.parquet\")\nlogger.info(f\"  \u2713 Output: advanced_features.parquet\")\n\n# ============================================================================\n# STEP 2: Load Master Aligned Data\n# ============================================================================\nlogger.info(\"\\n[2/6] Loading master aligned dataset...\")\n\nif not os.path.exists(INPUT_FILE):\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Input file not found\")\n    logger.info(\"  Please run Cell 3 first.\")\n    raise SystemExit(1)\n\ndf = pd.read_parquet(INPUT_FILE)\ndf['date'] = pd.to_datetime(df['date'])\n\nlogger.info(f\"  \u2713 Loaded dataset\")\nlogger.info(f\"  \u2713 Shape: {df.shape}\")\nlogger.info(f\"  \u2713 Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n\n# ============================================================================\n# STEP 3: Define Constants and Helper Functions\n# ============================================================================\nlogger.info(\"\\n[3/6] Defining advanced astrological functions...\")\n\nPLANETS = ['sun', 'moon', 'mercury', 'venus', 'mars',\n           'jupiter', 'saturn', 'rahu']\n\n# Harmonic aspects (minor aspects)\nMINOR_ASPECTS = {\n    'semisextile': 30,      # Mild friction\n    'semisquare': 45,       # Mild tension\n    'quintile': 72,         # Creativity\n    'sesquiquadrate': 135,  # Adjustment\n    'quincunx': 150,        # Requires adaptation\n}\n\n# Orb for minor aspects (tighter than major)\nMINOR_ASPECT_ORB = 2.0\n\ndef normalize_angle(angle):\n    \"\"\"Normalize angle to 0-360 range.\"\"\"\n    return angle % 360.0\n\ndef angular_distance(lon1, lon2):\n    \"\"\"Calculate shortest angular distance (0-180\u00b0).\"\"\"\n    diff = np.abs(lon1 - lon2)\n    return np.where(diff > 180, 360.0 - diff, diff)\n\ndef calculate_midpoint(lon1, lon2):\n    \"\"\"\n    Calculate midpoint between two planets.\n    Returns both the direct midpoint and far midpoint (180\u00b0 opposite).\n    Vectorized to handle arrays.\n    \"\"\"\n    lon1 = np.asarray(lon1)\n    lon2 = np.asarray(lon2)\n\n    # Direct midpoint (shorter arc)\n    direct = (lon1 + lon2) / 2.0\n\n    # Handle wrap-around case (when planets are on opposite sides of 0\u00b0)\n    # If distance > 180\u00b0, the midpoint is on the other side\n    needs_adjustment = np.abs(lon1 - lon2) > 180\n    direct = np.where(needs_adjustment, (direct + 180) % 360, direct)\n\n    direct = normalize_angle(direct)\n\n    # Far midpoint (opposite point)\n    far = normalize_angle(direct + 180)\n\n    return direct, far\n\ndef check_midpoint_activation(planet_lon, midpoint_lon, orb=3.0):\n    \"\"\"\n    Check if a planet is conjunct a midpoint.\n    Returns binary flag and exactness score.\n    \"\"\"\n    distance = angular_distance(planet_lon, midpoint_lon)\n    is_active = (distance <= orb).astype(np.int8)\n\n    # Exactness: 1.0 at exact, 0.0 at orb limit\n    exactness = np.maximum(0, 1 - (distance / orb)).astype(np.float32)\n\n    return is_active, exactness\n\nlogger.info(\"  \u2713 Midpoint calculation functions\")\nlogger.info(\"  \u2713 Harmonic aspect definitions\")\n\n# ============================================================================\n# STEP 4: Calculate Planetary Midpoints\n# ============================================================================\nlogger.info(\"\\n[4/6] Calculating planetary midpoints...\")\n\nadvanced_features = {'date': df['date'].values}\nfeature_count = 0\n\n# Focus on important midpoint pairs (not all combinations)\nIMPORTANT_MIDPOINT_PAIRS = [\n    ('sun', 'moon'),      # Personality integration\n    ('sun', 'mercury'),   # Communication of identity\n    ('sun', 'venus'),     # Values and pleasure\n    ('venus', 'mars'),    # Passion and desire\n    ('jupiter', 'saturn'), # Expansion vs contraction\n]\n\nfor p1, p2 in IMPORTANT_MIDPOINT_PAIRS:\n    if f'{p1}_longitude' not in df.columns or f'{p2}_longitude' not in df.columns:\n        continue\n\n    lon1 = df[f'{p1}_longitude'].values\n    lon2 = df[f'{p2}_longitude'].values\n\n    # Calculate midpoint\n    midpoint, far_midpoint = calculate_midpoint(lon1, lon2)\n\n    # Store midpoint longitude\n    advanced_features[f'{p1}_{p2}_midpoint'] = midpoint.astype(np.float32)\n    feature_count += 1\n\n    # Check if other planets activate this midpoint\n    for planet in ['mars', 'jupiter', 'saturn']:\n        if planet in [p1, p2]:\n            continue\n\n        if f'{planet}_longitude' not in df.columns:\n            continue\n\n        planet_lon = df[f'{planet}_longitude'].values\n\n        # Check activation\n        is_active, exactness = check_midpoint_activation(planet_lon, midpoint)\n\n        advanced_features[f'{planet}_on_{p1}_{p2}_midpoint'] = is_active\n        advanced_features[f'{planet}_on_{p1}_{p2}_midpoint_exact'] = exactness\n        feature_count += 2\n\nlogger.info(f\"  \u2713 Created {feature_count} midpoint features\")\n\n# ============================================================================\n# STEP 5: Calculate Minor/Harmonic Aspects\n# ============================================================================\nlogger.info(\"\\n[5/6] Calculating minor harmonic aspects...\")\n\n# Focus on key planet pairs for minor aspects\nKEY_MINOR_PAIRS = [\n    ('mercury', 'venus'),\n    ('mercury', 'mars'),\n    ('venus', 'mars'),\n    ('mars', 'jupiter'),\n    ('mars', 'saturn'),\n    ('jupiter', 'saturn'),\n]\n\nfor p1, p2 in KEY_MINOR_PAIRS:\n    if f'{p1}_longitude' not in df.columns or f'{p2}_longitude' not in df.columns:\n        continue\n\n    lon1 = df[f'{p1}_longitude'].values\n    lon2 = df[f'{p2}_longitude'].values\n\n    # Calculate angular distance\n    ang_dist = angular_distance(lon1, lon2)\n\n    for aspect_name, aspect_angle in MINOR_ASPECTS.items():\n        # Distance from exact minor aspect\n        dist_from_exact = np.abs(ang_dist - aspect_angle)\n        dist_from_exact = np.minimum(dist_from_exact, 360 - dist_from_exact)\n\n        # Is aspect active?\n        is_active = (dist_from_exact <= MINOR_ASPECT_ORB).astype(np.int8)\n\n        # Exactness score\n        exactness = np.where(\n            is_active,\n            (1 - (dist_from_exact / MINOR_ASPECT_ORB)).astype(np.float32),\n            0.0\n        )\n\n        advanced_features[f'{p1}_{p2}_{aspect_name}'] = is_active\n        advanced_features[f'{p1}_{p2}_{aspect_name}_exact'] = exactness\n        feature_count += 2\n\nlogger.info(f\"  \u2713 Created minor aspect features for {len(KEY_MINOR_PAIRS)} pairs\")\n\n# ============================================================================\n# STEP 6: Calculate Composite Daily Indicators\n# ============================================================================\nlogger.info(\"\\n[6/6] Calculating composite daily indicators...\")\n\n# We'll need to load aspect data from Cell 4 to create composites\nASPECTS_FILE = os.path.join(FEATURE_DATA_PATH, 'aspects_features.parquet')\n\nif os.path.exists(ASPECTS_FILE):\n    logger.info(\"  \u2192 Loading aspect features from Cell 4...\")\n    df_aspects = pd.read_parquet(ASPECTS_FILE)\n\n    # Ensure dates match\n    if len(df_aspects) == len(df):\n        # 1. Benefic Aspect Score (positive aspects)\n        benefic_cols = [col for col in df_aspects.columns if any(\n            x in col for x in ['trine_strength', 'sextile_strength']\n        )]\n\n        if benefic_cols:\n            benefic_score = df_aspects[benefic_cols].sum(axis=1).values\n            advanced_features['daily_benefic_score'] = benefic_score.astype(np.float32)\n            feature_count += 1\n\n        # 2. Malefic Aspect Score (challenging aspects)\n        malefic_cols = [col for col in df_aspects.columns if any(\n            x in col for x in ['square_strength', 'opposition_strength']\n        )]\n\n        if malefic_cols:\n            malefic_score = df_aspects[malefic_cols].sum(axis=1).values\n            advanced_features['daily_malefic_score'] = malefic_score.astype(np.float32)\n            feature_count += 1\n\n        # 3. Net Aspect Quality (benefic - malefic)\n        if 'daily_benefic_score' in advanced_features and 'daily_malefic_score' in advanced_features:\n            net_quality = (advanced_features['daily_benefic_score'] -\n                          advanced_features['daily_malefic_score'])\n            advanced_features['daily_aspect_quality'] = net_quality.astype(np.float32)\n            feature_count += 1\n\n        # 4. Aspect Intensity (total aspect strength regardless of type)\n        all_strength_cols = [col for col in df_aspects.columns if col.endswith('_strength')]\n        if all_strength_cols:\n            total_intensity = df_aspects[all_strength_cols].sum(axis=1).values\n            advanced_features['daily_aspect_intensity'] = total_intensity.astype(np.float32)\n            feature_count += 1\n\n        # 5. Harsh Aspect Count (Mars/Saturn involved)\n        harsh_cols = [col for col in df_aspects.columns if\n                     col.endswith('_active') and any(x in col for x in ['mars_saturn', 'saturn_'])]\n\n        if harsh_cols:\n            harsh_count = df_aspects[harsh_cols].sum(axis=1).values\n            advanced_features['daily_harsh_aspect_count'] = harsh_count.astype(np.int8)\n            feature_count += 1\n\n        logger.info(f\"  \u2713 Created {5} composite aspect indicators\")\n    else:\n        logger.warning(f\"  \u26a0 Aspect data length mismatch - skipping composites\")\nelse:\n    logger.warning(f\"  \u26a0 Aspect features file not found - skipping composites\")\n    logger.info(f\"    Run Cell 4 first to generate aspect features\")\n\n# ============================================================================\n# STEP 7: Calculate Planetary Strength Scores\n# ============================================================================\nlogger.info(\"\\n[7/7] Calculating planetary strength scores...\")\n\n# Planetary strength based on multiple factors\nfor planet in ['sun', 'moon', 'jupiter', 'venus', 'mars', 'mercury', 'saturn']:\n    if f'{planet}_longitude' not in df.columns:\n        continue\n\n    # Initialize strength score (0-100 scale)\n    strength = np.zeros(len(df), dtype=np.float32)\n\n    # Factor 1: Speed (faster = stronger, but not for retrograde)\n    if f'{planet}_speed' in df.columns:\n        speed = df[f'{planet}_speed'].values\n\n        # Normalize speed to 0-20 points\n        if planet == 'sun':\n            max_speed = 1.2\n        elif planet == 'moon':\n            max_speed = 15.0\n        elif planet in ['mercury', 'venus']:\n            max_speed = 2.0\n        else:\n            max_speed = 0.3\n\n        speed_score = np.clip((np.abs(speed) / max_speed) * 20, 0, 20)\n\n        # Penalty for retrograde (except Rahu which is always Rx)\n        if planet != 'rahu':\n            speed_score = np.where(speed < 0, speed_score * 0.5, speed_score)\n\n        strength += speed_score\n\n    # Factor 2: Dignity (from Cell 5 if available)\n    # We'll load transit features to check exaltation/debilitation\n    # For now, use a simplified approach based on position\n\n    # Factor 3: Aspect reception (simplified - receives benefic aspects)\n    # This would require aspect data - skip for now or add bonus\n\n    # Factor 4: House position quality (we don't have houses, skip)\n\n    # Normalize to 0-100 scale\n    strength = (strength / 20) * 100\n    strength = np.clip(strength, 0, 100).astype(np.float32)\n\n    advanced_features[f'{planet}_strength_score'] = strength\n    feature_count += 1\n\nlogger.info(f\"  \u2713 Created strength scores for 7 planets\")\n\n# ============================================================================\n# STEP 8: Create DataFrame and Save\n# ============================================================================\nlogger.info(f\"\\n[8/8] Saving advanced features...\")\n\ndf_advanced = pd.DataFrame(advanced_features)\n\ntry:\n    df_advanced.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow')\n    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n    logger.info(f\"  \u2713 Saved: advanced_features.parquet\")\n    logger.info(f\"  \u2713 File size: {file_size_mb:.2f} MB\")\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not save file\")\n    logger.critical(f\"  Error: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# VALIDATION & SAMPLE OUTPUT\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Planetary Midpoints (First 5 Days)\")\nlogger.info(\"=\" * 70)\n\nmidpoint_cols = ['date'] + [col for col in df_advanced.columns if '_midpoint' in col and not 'on_' in col][:5]\nexisting_mp = [col for col in midpoint_cols if col in df_advanced.columns]\n\nif len(existing_mp) > 1:\n    print(\"\\n\" + tabulate(df_advanced[existing_mp].head(5),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".2f\"))\nelse:\n    logger.info(\"\\n  No midpoint features created\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Composite Daily Indicators (First 5 Days)\")\nlogger.info(\"=\" * 70)\n\ncomposite_cols = ['date', 'daily_benefic_score', 'daily_malefic_score',\n                  'daily_aspect_quality', 'daily_aspect_intensity']\nexisting_comp = [col for col in composite_cols if col in df_advanced.columns]\n\nif len(existing_comp) > 1:\n    print(\"\\n\" + tabulate(df_advanced[existing_comp].head(5),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".2f\"))\nelse:\n    logger.info(\"\\n  No composite indicators created (run Cell 4 first)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Planetary Strength Scores (First 5 Days)\")\nlogger.info(\"=\" * 70)\n\nstrength_cols = ['date'] + [col for col in df_advanced.columns if '_strength_score' in col][:5]\nexisting_strength = [col for col in strength_cols if col in df_advanced.columns]\n\nif len(existing_strength) > 1:\n    print(\"\\n\" + tabulate(df_advanced[existing_strength].head(5),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".1f\"))\nelse:\n    logger.info(\"\\n  No strength scores created\")\n\n# ============================================================================\n# DATA QUALITY VALIDATION\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"DATA QUALITY VALIDATION\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n  Dataset shape: {df_advanced.shape}\")\nlogger.info(f\"  Total features: {len(df_advanced.columns)}\")\nlogger.info(f\"  Date range: {df_advanced['date'].min().date()} to {df_advanced['date'].max().date()}\")\n\nnull_counts = df_advanced.isnull().sum().sum()\nlogger.info(f\"  Null values: {null_counts} ({null_counts / df_advanced.size * 100:.2f}%)\")\n\n# Feature breakdown\nlogger.info(\"\\n  Feature breakdown:\")\nmidpoint_features = len([col for col in df_advanced.columns if 'midpoint' in col])\nminor_aspect_features = len([col for col in df_advanced.columns if any(\n    asp in col for asp in MINOR_ASPECTS.keys()\n)])\ncomposite_features = len([col for col in df_advanced.columns if 'daily_' in col])\nstrength_features = len([col for col in df_advanced.columns if '_strength_score' in col])\n\nlogger.info(f\"    \u2022 Midpoint features: {midpoint_features}\")\nlogger.info(f\"    \u2022 Minor aspect features: {minor_aspect_features}\")\nlogger.info(f\"    \u2022 Composite indicators: {composite_features}\")\nlogger.info(f\"    \u2022 Strength scores: {strength_features}\")\n\n# Value ranges for composite indicators\nif 'daily_aspect_quality' in df_advanced.columns:\n    quality = df_advanced['daily_aspect_quality']\n    logger.info(f\"\\n  Daily aspect quality range: {quality.min():.2f} to {quality.max():.2f}\")\n    logger.info(f\"  Average quality: {quality.mean():.2f}\")\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 2 (ADVANCED FEATURES) - STATUS: COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udccb Integration Points for Downstream Phases:\")\nlogger.info(\"  \u2192 Phase 3 (Model Training):\")\nlogger.info(\"    \u2022 Midpoint activations as binary features\")\nlogger.info(\"    \u2022 Minor aspects for nuanced pattern detection\")\nlogger.info(\"    \u2022 Composite scores as continuous features\")\nlogger.info(\"    \u2022 Strength scores for planet weighting\")\nlogger.info(\"  \u2192 Phase 4 (Backtesting):\")\nlogger.info(\"    \u2022 Filter on composite indicators (high benefic days)\")\nlogger.info(\"    \u2022 Isolate specific midpoint activations\")\nlogger.info(\"  \u2192 Phase 5 (Insight Extraction):\")\nlogger.info(\"    \u2022 Identify most influential midpoints per sector\")\nlogger.info(\"    \u2022 Quantify impact of minor aspects\")\nlogger.info(\"    \u2022 Rank planets by strength score correlation\")\n\nlogger.info(\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(\"  1. \u2713 Planetary aspects calculated (Cell 4)\")\nlogger.info(\"  2. \u2713 Transit & positional features (Cell 5)\")\nlogger.info(\"  3. \u2713 Cyclic & temporal features (Cell 6)\")\nlogger.info(\"  4. \u2713 Advanced astrological indicators (Cell 7 complete)\")\nlogger.info(\"  5. \u25b6 Run Cell 8: Feature Integration & Final Dataset\")\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files:\")\nlogger.info(f\"  {OUTPUT_FILE}\")\nlogger.info(f\"  ({len(df_advanced)} rows \u00d7 {len(df_advanced.columns)} features)\")\n\nlogger.info(\"\\n\" + \"=\" * 70)",
   "metadata": {
    "id": "EWoJXUWOGNi8"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0009"
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Feature Integration & Final Dataset (Phase 2 - Part 5 of 5) - FIXED\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport json\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 2: FEATURE ENGINEERING\")\nlogger.info(\"Phase 2 Progress: Part 5 of 5 (Feature Integration)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/8] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\nFEATURE_DATA_PATH = os.path.join(BASE_PATH, 'feature_data')\n\n# Input files\nMASTER_FILE = os.path.join(ALIGNED_DATA_PATH, 'master_aligned_dataset.parquet')\nASPECTS_FILE = os.path.join(FEATURE_DATA_PATH, 'aspects_features.parquet')\nTRANSIT_FILE = os.path.join(FEATURE_DATA_PATH, 'transit_features.parquet')\nTEMPORAL_FILE = os.path.join(FEATURE_DATA_PATH, 'temporal_features.parquet')\nADVANCED_FILE = os.path.join(FEATURE_DATA_PATH, 'advanced_features.parquet')\n\n# Output files\nOUTPUT_FILE = os.path.join(FEATURE_DATA_PATH, 'master_features_dataset.parquet')\nCATALOG_FILE = os.path.join(FEATURE_DATA_PATH, 'feature_catalog.csv')\nMETADATA_FILE = os.path.join(FEATURE_DATA_PATH, 'dataset_metadata.json')\nSPLITS_FILE = os.path.join(FEATURE_DATA_PATH, 'train_val_test_splits.json')\n\nlogger.info(f\"  \u2713 Input files configured (4 feature sets)\")\nlogger.info(f\"  \u2713 Output files configured\")\n\n# ============================================================================\n# STEP 2: Load All Feature Sets\n# ============================================================================\nlogger.info(\"\\n[2/8] Loading all feature datasets...\")\n\nfeature_sets = {}\nload_status = []\n\n# Load master aligned dataset (base)\nif os.path.exists(MASTER_FILE):\n    df_master = pd.read_parquet(MASTER_FILE)\n    df_master['date'] = pd.to_datetime(df_master['date'])\n    feature_sets['master'] = df_master\n    load_status.append(('Master Dataset', len(df_master.columns), '\u2713'))\n    logger.info(f\"  \u2713 Loaded master dataset: {df_master.shape}\")\nelse:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Master dataset not found\")\n    raise SystemExit(1)\n\n# Load aspect features\nif os.path.exists(ASPECTS_FILE):\n    df_aspects = pd.read_parquet(ASPECTS_FILE)\n    df_aspects['date'] = pd.to_datetime(df_aspects['date'])\n    feature_sets['aspects'] = df_aspects\n    load_status.append(('Aspect Features', len(df_aspects.columns), '\u2713'))\n    logger.info(f\"  \u2713 Loaded aspect features: {df_aspects.shape}\")\nelse:\n    logger.warning(f\"  \u26a0 Aspect features not found - skipping\")\n    load_status.append(('Aspect Features', 0, '\u2717'))\n\n# Load transit features\nif os.path.exists(TRANSIT_FILE):\n    df_transit = pd.read_parquet(TRANSIT_FILE)\n    df_transit['date'] = pd.to_datetime(df_transit['date'])\n    feature_sets['transit'] = df_transit\n    load_status.append(('Transit Features', len(df_transit.columns), '\u2713'))\n    logger.info(f\"  \u2713 Loaded transit features: {df_transit.shape}\")\nelse:\n    logger.warning(f\"  \u26a0 Transit features not found - skipping\")\n    load_status.append(('Transit Features', 0, '\u2717'))\n\n# Load temporal features\nif os.path.exists(TEMPORAL_FILE):\n    df_temporal = pd.read_parquet(TEMPORAL_FILE)\n    df_temporal['date'] = pd.to_datetime(df_temporal['date'])\n    feature_sets['temporal'] = df_temporal\n    load_status.append(('Temporal Features', len(df_temporal.columns), '\u2713'))\n    logger.info(f\"  \u2713 Loaded temporal features: {df_temporal.shape}\")\nelse:\n    logger.warning(f\"  \u26a0 Temporal features not found - skipping\")\n    load_status.append(('Temporal Features', 0, '\u2717'))\n\n# Load advanced features\nif os.path.exists(ADVANCED_FILE):\n    df_advanced = pd.read_parquet(ADVANCED_FILE)\n    df_advanced['date'] = pd.to_datetime(df_advanced['date'])\n    feature_sets['advanced'] = df_advanced\n    load_status.append(('Advanced Features', len(df_advanced.columns), '\u2713'))\n    logger.info(f\"  \u2713 Loaded advanced features: {df_advanced.shape}\")\nelse:\n    logger.warning(f\"  \u26a0 Advanced features not found - skipping\")\n    load_status.append(('Advanced Features', 0, '\u2717'))\n\nlogger.info(\"\\n  Load Status Summary:\")\nfor name, count, status in load_status:\n    logger.info(f\"    {status} {name}: {count} features\")\n\n# ============================================================================\n# STEP 3: Merge All Feature Sets (WITH DUPLICATE HANDLING)\n# ============================================================================\nlogger.info(\"\\n[3/8] Merging all feature sets...\")\n\n# Start with master dataset\ndf_merged = df_master.copy()\nlogger.info(f\"  Starting with master: {df_merged.shape}\")\n\n# Track columns to avoid duplicates\nexisting_columns = set(df_merged.columns)\n\n# Merge each feature set on 'date'\nfor name, df_features in feature_sets.items():\n    if name == 'master':\n        continue\n\n    # Get all columns except 'date'\n    feature_cols = [col for col in df_features.columns if col != 'date']\n\n    # CRITICAL FIX: Filter out columns that already exist\n    new_feature_cols = [col for col in feature_cols if col not in existing_columns]\n    duplicate_cols = [col for col in feature_cols if col in existing_columns]\n\n    if duplicate_cols:\n        logger.warning(f\"  \u26a0 Skipping {len(duplicate_cols)} duplicate columns from {name}: {duplicate_cols[:5]}...\")\n\n    if not new_feature_cols:\n        logger.warning(f\"  \u26a0 No new columns to add from {name}\")\n        continue\n\n    # Merge only new columns\n    df_merged = pd.merge(\n        df_merged,\n        df_features[['date'] + new_feature_cols],\n        on='date',\n        how='left',\n        suffixes=('', f'_{name}')  # Add suffix if still somehow duplicates\n    )\n\n    # Update existing columns set\n    existing_columns.update(new_feature_cols)\n\n    logger.info(f\"  + Merged {name}: added {len(new_feature_cols)} columns \u2192 {df_merged.shape}\")\n\n# CRITICAL: Remove any remaining duplicate columns\nduplicate_cols = df_merged.columns[df_merged.columns.duplicated()].tolist()\nif duplicate_cols:\n    logger.warning(f\"\\n  \u26a0 WARNING: Found {len(duplicate_cols)} duplicate columns after merge\")\n    logger.info(f\"    Removing duplicates: {duplicate_cols[:10]}...\")\n    df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n    logger.info(f\"    After cleanup: {df_merged.shape}\")\n\nlogger.info(f\"\\n  \u2713 Final merged dataset: {df_merged.shape}\")\n\n# ============================================================================\n# STEP 4: Quality Checks and Cleanup\n# ============================================================================\nlogger.info(\"\\n[4/8] Performing quality checks...\")\n\n# Check for duplicates\nduplicates = df_merged.duplicated(subset=['date']).sum()\nlogger.info(f\"  \u2022 Duplicate dates: {duplicates}\")\n\nif duplicates > 0:\n    logger.info(f\"    Removing {duplicates} duplicate rows...\")\n    df_merged = df_merged.drop_duplicates(subset=['date'], keep='first')\n\n# Check for null values\nnull_counts = df_merged.isnull().sum()\ntotal_nulls = null_counts.sum()\nnull_pct = (total_nulls / df_merged.size) * 100\n\nlogger.info(f\"  \u2022 Total null values: {total_nulls} ({null_pct:.2f}%)\")\n\nif total_nulls > 0:\n    # Show columns with most nulls\n    top_nulls = null_counts[null_counts > 0].sort_values(ascending=False).head(10)\n    logger.info(f\"\\n  Top columns with nulls:\")\n    for col, count in top_nulls.items():\n        pct = (count / len(df_merged)) * 100\n        logger.info(f\"    \u2022 {col}: {count} ({pct:.1f}%)\")\n\n# Sort by date\ndf_merged = df_merged.sort_values('date').reset_index(drop=True)\nlogger.info(f\"\\n  \u2713 Data cleaned and sorted\")\n\n# ============================================================================\n# STEP 5: Create Train/Validation/Test Splits\n# ============================================================================\nlogger.info(\"\\n[5/8] Creating train/validation/test splits...\")\n\n# Define split dates\ntrain_end = pd.Timestamp('2020-12-31')\nval_end = pd.Timestamp('2023-12-31')\n\ntrain_mask = df_merged['date'] <= train_end\nval_mask = (df_merged['date'] > train_end) & (df_merged['date'] <= val_end)\ntest_mask = df_merged['date'] > val_end\n\ntrain_size = train_mask.sum()\nval_size = val_mask.sum()\ntest_size = test_mask.sum()\n\ntrain_pct = (train_size / len(df_merged)) * 100\nval_pct = (val_size / len(df_merged)) * 100\ntest_pct = (test_size / len(df_merged)) * 100\n\nlogger.info(f\"\\n  Split distribution:\")\nlogger.info(f\"    \u2022 Train: {train_size} rows ({train_pct:.1f}%) | 2000-01-01 to 2020-12-31\")\nlogger.info(f\"    \u2022 Val:   {val_size} rows ({val_pct:.1f}%) | 2021-01-01 to 2023-12-31\")\nlogger.info(f\"    \u2022 Test:  {test_size} rows ({test_pct:.1f}%) | 2024-01-01 to 2025-10-29\")\n\n# Save split metadata\nsplits_metadata = {\n    'train': {\n        'start_date': df_merged[train_mask]['date'].min().strftime('%Y-%m-%d') if train_size > 0 else 'N/A',\n        'end_date': df_merged[train_mask]['date'].max().strftime('%Y-%m-%d') if train_size > 0 else 'N/A',\n        'rows': int(train_size),\n        'percentage': float(train_pct)\n    },\n    'validation': {\n        'start_date': df_merged[val_mask]['date'].min().strftime('%Y-%m-%d') if val_size > 0 else 'N/A',\n        'end_date': df_merged[val_mask]['date'].max().strftime('%Y-%m-%d') if val_size > 0 else 'N/A',\n        'rows': int(val_size),\n        'percentage': float(val_pct)\n    },\n    'test': {\n        'start_date': df_merged[test_mask]['date'].min().strftime('%Y-%m-%d') if test_size > 0 else 'N/A',\n        'end_date': df_merged[test_mask]['date'].max().strftime('%Y-%m-%d') if test_size > 0 else 'N/A',\n        'rows': int(test_size),\n        'percentage': float(test_pct)\n    }\n}\n\nwith open(SPLITS_FILE, 'w') as f:\n    json.dump(splits_metadata, f, indent=2)\n\nlogger.info(f\"\\n  \u2713 Split metadata saved: {SPLITS_FILE}\")\n\n# ============================================================================\n# STEP 6: Create Feature Catalog (FIXED)\n# ============================================================================\nlogger.info(\"\\n[6/8] Creating feature catalog...\")\n\ncatalog_data = []\n\nfor col in df_merged.columns:\n    if col == 'date':\n        continue\n\n    try:\n        # CRITICAL FIX: Use proper Series access\n        col_series = df_merged[col]\n\n        # Skip if this returns a DataFrame (shouldn't happen now but safety check)\n        if isinstance(col_series, pd.DataFrame):\n            logger.warning(f\"  \u26a0 Skipping duplicate column: {col}\")\n            continue\n\n        # Determine category\n        if any(x in col for x in ['_active', '_tight', '_applying', '_strength', '_exact']):\n            category = 'Aspects'\n        elif any(x in col for x in ['_sign', '_nakshatra', '_retrograde', '_exalted', '_dignity']):\n            category = 'Transits'\n        elif any(x in col for x in ['moon_phase', 'mercury_retrograde', 'day_of_week', 'month', 'quarter']):\n            category = 'Temporal'\n        elif any(x in col for x in ['midpoint', 'daily_', '_score']):\n            category = 'Advanced'\n        elif any(x in col for x in ['_longitude', '_speed', 'julian_day']):\n            category = 'Raw Planetary'\n        elif any(x in col for x in ['_open', '_high', '_low', '_close', '_volume', 'currency']):\n            category = 'Financial'\n        else:\n            category = 'Other'\n\n        # Determine data type\n        dtype = col_series.dtype\n        if dtype in ['int8', 'int16', 'int32', 'int64']:\n            data_type = 'Integer'\n        elif dtype in ['float16', 'float32', 'float64']:\n            data_type = 'Float'\n        elif dtype == 'object':\n            data_type = 'Object'\n        else:\n            data_type = str(dtype)\n\n        # Calculate null percentage\n        null_pct = (col_series.isnull().sum() / len(df_merged)) * 100\n\n        # Get value range\n        if data_type in ['Integer', 'Float']:\n            try:\n                val_min = col_series.min()\n                val_max = col_series.max()\n                value_range = f\"{val_min:.2f} to {val_max:.2f}\"\n            except:\n                value_range = \"N/A\"\n        else:\n            value_range = \"N/A\"\n\n        catalog_data.append({\n            'feature_name': col,\n            'category': category,\n            'data_type': data_type,\n            'null_percentage': f\"{null_pct:.2f}%\",\n            'value_range': value_range,\n            'description': f\"{category} feature: {col}\"\n        })\n\n    except Exception as e:\n        logger.warning(f\"  \u26a0 Error processing column {col}: {str(e)[:50]}\")\n        continue\n\ndf_catalog = pd.DataFrame(catalog_data)\ndf_catalog.to_csv(CATALOG_FILE, index=False)\n\nlogger.info(f\"  \u2713 Feature catalog created: {len(catalog_data)} features\")\nlogger.info(f\"  \u2713 Saved to: {CATALOG_FILE}\")\n\n# Print category summary\ncategory_counts = df_catalog['category'].value_counts()\nlogger.info(f\"\\n  Feature breakdown by category:\")\nfor cat, count in category_counts.items():\n    logger.info(f\"    \u2022 {cat}: {count}\")\n\n# ============================================================================\n# STEP 7: Save Master Features Dataset\n# ============================================================================\nlogger.info(\"\\n[7/8] Saving master features dataset...\")\n\ntry:\n    df_merged.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow')\n    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n    logger.info(f\"  \u2713 Saved: master_features_dataset.parquet\")\n    logger.info(f\"  \u2713 File size: {file_size_mb:.2f} MB\")\n    logger.info(f\"  \u2713 Shape: {df_merged.shape}\")\nexcept Exception as e:\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Could not save file\")\n    logger.critical(f\"  Error: {e}\")\n    raise SystemExit(1)\n\n# ============================================================================\n# STEP 8: Create Dataset Metadata\n# ============================================================================\nlogger.info(\"\\n[8/8] Creating dataset metadata...\")\n\nmetadata = {\n    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'dataset_info': {\n        'total_rows': int(len(df_merged)),\n        'total_features': int(len(df_merged.columns)),\n        'date_range_start': df_merged['date'].min().strftime('%Y-%m-%d'),\n        'date_range_end': df_merged['date'].max().strftime('%Y-%m-%d'),\n        'days_covered': int(len(df_merged)),\n    },\n    'feature_categories': {\n        cat: int(count) for cat, count in category_counts.items()\n    },\n    'data_quality': {\n        'null_values': int(total_nulls),\n        'null_percentage': float(null_pct),\n        'duplicate_rows': int(duplicates),\n    },\n    'train_val_test_splits': splits_metadata,\n    'source_files': {\n        'master_aligned': 'aligned_data/master_aligned_dataset.parquet',\n        'aspects': 'feature_data/aspects_features.parquet',\n        'transit': 'feature_data/transit_features.parquet',\n        'temporal': 'feature_data/temporal_features.parquet',\n        'advanced': 'feature_data/advanced_features.parquet',\n    },\n    'phase_2_completion': {\n        'cell_4_aspects': 'Complete',\n        'cell_5_transits': 'Complete',\n        'cell_6_temporal': 'Complete',\n        'cell_7_advanced': 'Complete',\n        'cell_8_integration': 'Complete',\n    }\n}\n\nwith open(METADATA_FILE, 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nlogger.info(f\"  \u2713 Metadata saved: {METADATA_FILE}\")\n\n# ============================================================================\n# FINAL SUMMARY & VALIDATION\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 2 COMPLETION SUMMARY\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udcca Dataset Statistics:\")\nlogger.info(f\"  \u2022 Total rows: {len(df_merged):,}\")\nlogger.info(f\"  \u2022 Total features: {len(df_merged.columns):,}\")\nlogger.info(f\"  \u2022 Date range: {df_merged['date'].min().date()} to {df_merged['date'].max().date()}\")\nlogger.info(f\"  \u2022 File size: {file_size_mb:.2f} MB\")\n\nlogger.info(\"\\n\ud83d\udccb Feature Categories:\")\nfor cat, count in category_counts.items():\n    pct = (count / len(df_merged.columns)) * 100\n    logger.info(f\"  \u2022 {cat}: {count} ({pct:.1f}%)\")\n\nlogger.info(\"\\n\u2705 Data Quality:\")\nlogger.info(f\"  \u2022 Null values: {total_nulls} ({null_pct:.2f}%)\")\nlogger.info(f\"  \u2022 Duplicates: {duplicates}\")\nlogger.error(f\"  \u2022 Date continuity: {'\u2713 Verified' if len(df_merged) > 0 else '\u2717 Failed'}\")\n\nlogger.info(\"\\n\ud83c\udfaf Train/Val/Test Splits:\")\nlogger.info(f\"  \u2022 Train: {train_size:,} rows ({train_pct:.1f}%)\")\nlogger.info(f\"  \u2022 Validation: {val_size:,} rows ({val_pct:.1f}%)\")\nlogger.info(f\"  \u2022 Test: {test_size:,} rows ({test_pct:.1f}%)\")\n\n# Sample preview\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SAMPLE: Master Features Dataset (First 3 Rows)\")\nlogger.info(\"=\" * 70)\n\n# Show a representative sample of columns\nsample_cols = ['date', 'sun_longitude', 'moon_phase_angle',\n               'sun_moon_conjunction_active', 'daily_aspect_quality',\n               'mercury_retrograde', 'jupiter_strength_score']\n\nexisting_sample = [col for col in sample_cols if col in df_merged.columns]\nif existing_sample:\n    print(\"\\n\" + tabulate(df_merged[existing_sample].head(3),\n                         headers='keys', tablefmt='grid',\n                         showindex=False, floatfmt=\".2f\"))\nelse:\n    logger.info(\"\\n  No sample columns available\")\n\n# ============================================================================\n# FINAL STATUS\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 2 (FEATURE ENGINEERING) - STATUS: COMPLETE \u2713\u2713\u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(\"\\n\ud83d\udcc2 Output Files Created:\")\nlogger.info(f\"  1. {OUTPUT_FILE}\")\nlogger.info(f\"     \u2192 Master dataset with all features\")\nlogger.info(f\"  2. {CATALOG_FILE}\")\nlogger.info(f\"     \u2192 Feature catalog with descriptions\")\nlogger.info(f\"  3. {SPLITS_FILE}\")\nlogger.info(f\"     \u2192 Train/validation/test split definitions\")\nlogger.info(f\"  4. {METADATA_FILE}\")\nlogger.info(f\"     \u2192 Complete dataset metadata\")\n\nlogger.info(\"\\n\ud83d\udccb Integration Points for Phase 3:\")\nlogger.info(\"  \u2192 Load master_features_dataset.parquet directly\")\nlogger.info(\"  \u2192 Use feature_catalog.csv for feature selection\")\nlogger.info(\"  \u2192 Use train_val_test_splits.json for consistent splits\")\nlogger.info(\"  \u2192 All features properly aligned and ready for ML\")\n\nlogger.info(\"\\n\ud83c\udfaf Ready for Phase 3: Model Design & Training\")\nlogger.info(\"  The dataset is now ready for:\")\nlogger.info(\"  \u2022 XGBoost, Random Forest, Neural Networks\")\nlogger.info(\"  \u2022 Time-series cross-validation\")\nlogger.info(\"  \u2022 Feature importance analysis\")\nlogger.info(\"  \u2022 SHAP value calculations\")\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"\ud83c\udf89 PHASE 2 COMPLETE - ALL FEATURES ENGINEERED SUCCESSFULLY! \ud83c\udf89\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "id": "U3fTSAhwH152"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cell-0010"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PHASE 3 - \ud83d\udcca Model Design & Training"
   ],
   "metadata": {
    "id": "Uj7IepNCl3rO"
   },
   "id": "cell-0011"
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Multi-Ticker Data Preparation (Phase 3 - Part 1 of 6) - WITH NAN HANDLING\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport json\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n\nnp.random.seed(42)\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 3: PROFESSIONAL ML PIPELINE\")\nlogger.info(\"Phase 3 Progress: Part 1 of 6 (Multi-Ticker Data Preparation)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup Paths\n# ============================================================================\nlogger.info(\"\\n[1/13] Setting up paths...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nFEATURE_DATA_PATH = os.path.join(BASE_PATH, 'feature_data')\nALIGNED_DATA_PATH = os.path.join(BASE_PATH, 'aligned_data')\nPREPARED_DATA_PATH = os.path.join(BASE_PATH, 'prepared_data')\nMULTI_TICKER_PATH = os.path.join(PREPARED_DATA_PATH, 'multi_ticker')\n\n# Create directories\nos.makedirs(MULTI_TICKER_PATH, exist_ok=True)\n\nlogger.info(f\"  \u2713 Input: {FEATURE_DATA_PATH}\")\nlogger.info(f\"  \u2713 Output: {MULTI_TICKER_PATH}\")\n\n# ============================================================================\n# STEP 2: Load Master Features Dataset\n# ============================================================================\nlogger.info(\"\\n[2/13] Loading master features dataset...\")\n\nMASTER_FILE = os.path.join(FEATURE_DATA_PATH, 'master_features_dataset.parquet')\n\nif not os.path.exists(MASTER_FILE):\n    logger.critical(f\"\\n\u2717 FATAL ERROR: Master features dataset not found\")\n    logger.info(\"  Please run Cell 8 first to generate the integrated dataset.\")\n    raise SystemExit(1)\n\ndf_features = pd.read_parquet(MASTER_FILE)\ndf_features['date'] = pd.to_datetime(df_features['date'])\n\nlogger.info(f\"  \u2713 Loaded master dataset\")\nlogger.info(f\"  \u2713 Shape: {df_features.shape}\")\nlogger.info(f\"  \u2713 Date range: {df_features['date'].min().date()} to {df_features['date'].max().date()}\")\n\n# Get list of astrological feature columns (exclude financial/date columns)\nastro_features = [col for col in df_features.columns if col not in ['date'] and not any(\n    x in col for x in ['_open', '_high', '_low', '_close', '_volume', 'currency', 'volume_unit', '_adj_close']\n)]\n\nlogger.info(f\"  \u2713 Identified {len(astro_features)} astrological features\")\n\n# Check for NaN in astrological features\nastro_nulls = df_features[astro_features].isnull().sum().sum()\nlogger.info(f\"  \u2713 Astrological features NaN count: {astro_nulls} ({astro_nulls / df_features[astro_features].size * 100:.2f}%)\")\n\n# ============================================================================\n# STEP 3: Handle Missing Values in Astrological Features\n# ============================================================================\nlogger.info(\"\\n[3/13] Handling missing values in astrological features...\")\n\nif astro_nulls > 0:\n    logger.info(f\"  Found {astro_nulls} missing values in astrological features\")\n\n    # Show columns with most nulls\n    null_counts = df_features[astro_features].isnull().sum()\n    top_nulls = null_counts[null_counts > 0].sort_values(ascending=False).head(10)\n\n    logger.info(f\"\\n  Top columns with missing values:\")\n    for col, count in top_nulls.items():\n        pct = (count / len(df_features)) * 100\n        logger.info(f\"    \u2022 {col}: {count} ({pct:.1f}%)\")\n\n    # Strategy 1: Drop columns with >50% missing values (unreliable)\n    high_null_threshold = 0.5\n    high_null_cols = null_counts[null_counts / len(df_features) > high_null_threshold].index.tolist()\n\n    if high_null_cols:\n        logger.info(f\"\\n  Dropping {len(high_null_cols)} columns with >{high_null_threshold*100:.0f}% missing values:\")\n        for col in high_null_cols[:5]:\n            logger.info(f\"    \u2022 {col}\")\n        if len(high_null_cols) > 5:\n            logger.info(f\"    \u2022 ... and {len(high_null_cols) - 5} more\")\n\n        astro_features = [col for col in astro_features if col not in high_null_cols]\n\n    # Strategy 2: Impute remaining missing values with median\n    remaining_nulls = df_features[astro_features].isnull().sum().sum()\n\n    if remaining_nulls > 0:\n        logger.info(f\"\\n  Imputing {remaining_nulls} remaining missing values with median...\")\n\n        astro_imputer = SimpleImputer(strategy='median', add_indicator=False)\n        df_features[astro_features] = astro_imputer.fit_transform(df_features[astro_features])\n\n        # Save imputer for documentation\n        with open(os.path.join(MULTI_TICKER_PATH, 'astro_imputer.pkl'), 'wb') as f:\n            pickle.dump(astro_imputer, f)\n\n        logger.info(f\"  \u2713 Imputation complete\")\n\n    # Verify no NaN values remain\n    final_nulls = df_features[astro_features].isnull().sum().sum()\n    logger.info(f\"  \u2713 Remaining NaN in astro features: {final_nulls}\")\nelse:\n    logger.info(f\"  \u2713 No missing values in astrological features\")\n\nlogger.info(f\"  \u2713 Final astrological features: {len(astro_features)}\")\n\n# ============================================================================\n# STEP 4: Define Ticker Universe and Metadata\n# ============================================================================\nlogger.info(\"\\n[4/13] Defining ticker universe and metadata...\")\n\n# Define comprehensive ticker metadata\nTICKER_METADATA = {\n    # US Tech\n    'AAPL': {'sector': 'Technology', 'region': 'US', 'market_cap': 'Large'},\n    'MSFT': {'sector': 'Technology', 'region': 'US', 'market_cap': 'Large'},\n    'NVDA': {'sector': 'Technology', 'region': 'US', 'market_cap': 'Large'},\n\n    # US Indices\n    'GSPC': {'sector': 'Indices', 'region': 'US', 'market_cap': 'Index'},\n    'DJI': {'sector': 'Indices', 'region': 'US', 'market_cap': 'Index'},\n    'NDX': {'sector': 'Indices', 'region': 'US', 'market_cap': 'Index'},\n    'RUT': {'sector': 'Indices', 'region': 'US', 'market_cap': 'Index'},\n    'VIX': {'sector': 'Indices', 'region': 'US', 'market_cap': 'Index'},\n    'TNX': {'sector': 'Indices', 'region': 'US', 'market_cap': 'Index'},\n\n    # India Indices\n    'NSEI': {'sector': 'Indices', 'region': 'India', 'market_cap': 'Index'},\n    'NSEBANK': {'sector': 'Finance', 'region': 'India', 'market_cap': 'Index'},\n    'NIFTY_FIN_SERVICE_NS': {'sector': 'Finance', 'region': 'India', 'market_cap': 'Index'},\n    'CNXIT': {'sector': 'Technology', 'region': 'India', 'market_cap': 'Index'},\n    'CNXPHARMA': {'sector': 'Pharma', 'region': 'India', 'market_cap': 'Index'},\n    'CNXAUTO': {'sector': 'Indices', 'region': 'India', 'market_cap': 'Index'},\n    'CNXMETAL': {'sector': 'Commodities', 'region': 'India', 'market_cap': 'Index'},\n    'CNXFMCG': {'sector': 'Indices', 'region': 'India', 'market_cap': 'Index'},\n    'INDIAVIX': {'sector': 'Indices', 'region': 'India', 'market_cap': 'Index'},\n\n    # India Stocks\n    'RELIANCE_NS': {'sector': 'Commodities', 'region': 'India', 'market_cap': 'Large'},\n    'TCS_NS': {'sector': 'Technology', 'region': 'India', 'market_cap': 'Large'},\n    'HDFCBANK_NS': {'sector': 'Finance', 'region': 'India', 'market_cap': 'Large'},\n\n    # Global Indices\n    'N225': {'sector': 'Indices', 'region': 'Asia', 'market_cap': 'Index'},\n    'FTSE': {'sector': 'Indices', 'region': 'Europe', 'market_cap': 'Index'},\n    'GDAXI': {'sector': 'Indices', 'region': 'Europe', 'market_cap': 'Index'},\n    '000001_SS': {'sector': 'Indices', 'region': 'Asia', 'market_cap': 'Index'},\n    'HSI': {'sector': 'Indices', 'region': 'Asia', 'market_cap': 'Index'},\n\n    # Commodities\n    'GC': {'sector': 'Commodities', 'region': 'Global', 'market_cap': 'Commodity'},\n    'CL': {'sector': 'Commodities', 'region': 'Global', 'market_cap': 'Commodity'},\n    'SI': {'sector': 'Commodities', 'region': 'Global', 'market_cap': 'Commodity'},\n\n    # Currencies\n    'DX_Y_NYB': {'sector': 'Currencies', 'region': 'Global', 'market_cap': 'Currency'},\n    'USDINR_X': {'sector': 'Currencies', 'region': 'Global', 'market_cap': 'Currency'},\n    'EURUSD_X': {'sector': 'Currencies', 'region': 'Global', 'market_cap': 'Currency'},\n}\n\n# Find which tickers have data in our dataset\navailable_tickers = []\nfor ticker, metadata in TICKER_METADATA.items():\n    close_col = f'{ticker}_close'\n    if close_col in df_features.columns:\n        # Check if ticker has sufficient non-null data\n        non_null_pct = (1 - df_features[close_col].isnull().mean()) * 100\n        if non_null_pct >= 50:  # At least 50% data availability\n            available_tickers.append(ticker)\n        else:\n            logger.warning(f\"  \u26a0 Skipping {ticker}: only {non_null_pct:.1f}% data available\")\n\nlogger.info(f\"\\n  \u2713 Found {len(available_tickers)} tickers with sufficient data\")\nlogger.info(f\"  Available tickers: {', '.join(available_tickers[:10])}...\")\n\n# ============================================================================\n# STEP 5: Stack All Ticker Data (OPTIMIZED)\n# ============================================================================\nlogger.info(\"\\n[5/13] Stacking all ticker data...\")\nlogger.info(\"  (This creates ~190,000 samples from multi-ticker expansion)\")\n\n# Use list to collect DataFrames, then concat once at the end\nticker_dfs = []\nprocessing_summary = []\n\nfor i, ticker in enumerate(available_tickers):\n    if (i + 1) % 5 == 0:\n        logger.info(f\"  Processing ticker {i+1}/{len(available_tickers)}...\")\n\n    # Get financial columns for this ticker\n    close_col = f'{ticker}_close'\n    volume_col = f'{ticker}_volume'\n\n    if close_col not in df_features.columns:\n        continue\n\n    # Create ticker-specific DataFrame\n    ticker_data = {\n        'date': df_features['date'],\n        'ticker': ticker,\n        'close': df_features[close_col],\n    }\n\n    # Add volume if available\n    if volume_col in df_features.columns:\n        ticker_data['volume'] = df_features[volume_col]\n    else:\n        ticker_data['volume'] = np.nan\n\n    # Create DataFrame\n    ticker_df = pd.DataFrame(ticker_data)\n\n    # Remove rows with null close prices\n    initial_rows = len(ticker_df)\n    ticker_df = ticker_df.dropna(subset=['close'])\n    final_rows = len(ticker_df)\n\n    if final_rows < 100:  # Need at least 100 days of data\n        logger.warning(f\"  \u26a0 Skipping {ticker}: only {final_rows} valid days\")\n        continue\n\n    # Add metadata\n    metadata = TICKER_METADATA[ticker]\n    ticker_df['sector'] = metadata['sector']\n    ticker_df['region'] = metadata['region']\n    ticker_df['market_cap'] = metadata['market_cap']\n\n    # Calculate technical indicators\n    close_prices = ticker_df['close'].values\n    volume = ticker_df['volume'].values\n\n    # 1. Returns (1-day, 5-day, 20-day)\n    returns_1d = np.zeros(len(close_prices), dtype=np.float32)\n    returns_1d[1:] = (close_prices[1:] / close_prices[:-1] - 1) * 100\n\n    returns_5d = np.zeros(len(close_prices), dtype=np.float32)\n    returns_5d[5:] = (close_prices[5:] / close_prices[:-5] - 1) * 100\n\n    returns_20d = np.zeros(len(close_prices), dtype=np.float32)\n    returns_20d[20:] = (close_prices[20:] / close_prices[:-20] - 1) * 100\n\n    # 2. Moving averages\n    sma_20 = pd.Series(close_prices).rolling(window=20, min_periods=1).mean().values\n    sma_50 = pd.Series(close_prices).rolling(window=50, min_periods=1).mean().values\n\n    # 3. RSI (14-day)\n    delta = np.diff(close_prices)\n    gain = np.where(delta > 0, delta, 0)\n    loss = np.where(delta < 0, -delta, 0)\n\n    avg_gain = pd.Series(gain).rolling(window=14, min_periods=1).mean().values\n    avg_loss = pd.Series(loss).rolling(window=14, min_periods=1).mean().values\n\n    rs = avg_gain / (avg_loss + 1e-10)\n    rsi = 100 - (100 / (1 + rs))\n    rsi = np.concatenate([[50], rsi])  # Add initial value\n\n    # 4. Bollinger Bands\n    bb_middle = sma_20\n    bb_std = pd.Series(close_prices).rolling(window=20, min_periods=1).std().values\n    bb_upper = bb_middle + (2 * bb_std)\n    bb_lower = bb_middle - (2 * bb_std)\n    bb_position = (close_prices - bb_lower) / (bb_upper - bb_lower + 1e-10)\n\n    # 5. ATR (Average True Range - volatility)\n    high_low = pd.Series(close_prices).rolling(window=2).apply(\n        lambda x: abs(x.iloc[-1] - x.iloc[0]) if len(x) == 2 else 0\n    ).values\n    atr = pd.Series(high_low).rolling(window=14, min_periods=1).mean().values\n\n    # 6. Volume ratio (handle NaN volumes)\n    volume_clean = np.nan_to_num(volume, nan=0.0)\n    volume_ma = pd.Series(volume_clean).rolling(window=20, min_periods=1).mean().values\n    volume_ratio = volume_clean / (volume_ma + 1e-10)\n\n    # 7. Volatility (20-day rolling std of returns)\n    volatility_20d = pd.Series(returns_1d).rolling(window=20, min_periods=1).std().values\n\n    # Create all technical features at once using a dictionary\n    tech_features = pd.DataFrame({\n        'returns_1d': returns_1d,\n        'returns_5d': returns_5d,\n        'returns_20d': returns_20d,\n        'sma_20': sma_20,\n        'sma_50': sma_50,\n        'rsi_14': rsi,\n        'bb_position': bb_position,\n        'atr_14': atr,\n        'volume_ratio': volume_ratio,\n        'volatility_20d': volatility_20d,\n    }, index=ticker_df.index)\n\n    # OPTIMIZED: Concatenate all new columns at once\n    ticker_df = pd.concat([ticker_df, tech_features], axis=1)\n\n    # Calculate targets (forward returns)\n    # Target 1: 5-day forward return > 1%\n    target_5day = np.zeros(len(close_prices), dtype=np.int8)\n    forward_returns_5d = np.zeros(len(close_prices), dtype=np.float32)\n    if len(close_prices) > 5:\n        forward_returns_5d[:-5] = (close_prices[5:] / close_prices[:-5] - 1) * 100\n        target_5day[:-5] = (forward_returns_5d[:-5] > 1.0).astype(np.int8)\n\n    # Target 2: 3-day forward return > 0.5%\n    target_3day = np.zeros(len(close_prices), dtype=np.int8)\n    forward_returns_3d = np.zeros(len(close_prices), dtype=np.float32)\n    if len(close_prices) > 3:\n        forward_returns_3d[:-3] = (close_prices[3:] / close_prices[:-3] - 1) * 100\n        target_3day[:-3] = (forward_returns_3d[:-3] > 0.5).astype(np.int8)\n\n    # Target 3: Volatility regime (next 5 days)\n    target_volatility = np.zeros(len(close_prices), dtype=np.int8)\n    if len(close_prices) > 5:\n        future_vol = pd.Series(close_prices).rolling(window=5).std().shift(-5).values\n        median_vol = np.nanmedian(future_vol)\n        target_volatility = (future_vol > median_vol).astype(np.int8)\n\n    # Target 4: Continuous 5-day return\n    target_magnitude = forward_returns_5d\n\n    # Add targets using DataFrame (optimized)\n    target_features = pd.DataFrame({\n        'target_5day': target_5day,\n        'target_3day': target_3day,\n        'target_volatility': target_volatility,\n        'target_magnitude': target_magnitude,\n    }, index=ticker_df.index)\n\n    # OPTIMIZED: Concatenate targets\n    ticker_df = pd.concat([ticker_df, target_features], axis=1)\n\n    # Remove last 5 rows (no valid targets)\n    ticker_df = ticker_df.iloc[:-5]\n\n    # Add to list\n    ticker_dfs.append(ticker_df)\n\n    processing_summary.append({\n        'ticker': ticker,\n        'sector': metadata['sector'],\n        'rows': len(ticker_df),\n        'null_pct': f\"{ticker_df.isnull().mean().mean()*100:.1f}%\"\n    })\n\n# CRITICAL OPTIMIZATION: Concatenate all ticker DataFrames at once\nlogger.info(f\"\\n  Concatenating {len(ticker_dfs)} ticker datasets...\")\ndf_stacked = pd.concat(ticker_dfs, axis=0, ignore_index=True)\n\nlogger.info(f\"  \u2713 Stacked dataset created: {df_stacked.shape}\")\nlogger.info(f\"  \u2713 Total samples: {len(df_stacked):,}\")\n\n# Display processing summary\nlogger.info(\"\\n  Processing summary:\")\nsummary_df = pd.DataFrame(processing_summary)\nprint(\"\\n\" + tabulate(summary_df.head(10), headers='keys', tablefmt='grid', showindex=False))\n\n# ============================================================================\n# STEP 6: Add Astrological Features to Each Row\n# ============================================================================\nlogger.info(\"\\n[6/13] Adding astrological features...\")\n\n# Merge astrological features based on date\nastro_cols = ['date'] + astro_features\ndf_astro = df_features[astro_cols].copy()\n\n# OPTIMIZED: Single merge operation\ndf_stacked = pd.merge(\n    df_stacked,\n    df_astro,\n    on='date',\n    how='left'\n)\n\nlogger.info(f\"  \u2713 Added {len(astro_features)} astrological features\")\nlogger.info(f\"  \u2713 Shape after merge: {df_stacked.shape}\")\n\n# Check for NaN after merge\nmerge_nulls = df_stacked.isnull().sum().sum()\nif merge_nulls > 0:\n    logger.warning(f\"  \u26a0 Found {merge_nulls} NaN values after merge\")\n\n    # This shouldn't happen since we imputed earlier, but handle it\n    null_cols = df_stacked.isnull().sum()\n    null_cols = null_cols[null_cols > 0].sort_values(ascending=False).head(5)\n\n    logger.info(f\"  Top columns with NaN:\")\n    for col, count in null_cols.items():\n        logger.info(f\"    \u2022 {col}: {count}\")\n\n    # Forward fill then backward fill (for any edge cases)\n    df_stacked = df_stacked.fillna(method='ffill').fillna(method='bfill').fillna(0)\n\n    final_nulls = df_stacked.isnull().sum().sum()\n    logger.info(f\"  \u2713 After filling: {final_nulls} NaN remaining\")\n\nlogger.info(f\"  \u2713 Final shape: {df_stacked.shape}\")\n\n# ============================================================================\n# STEP 7: Handle Infinite and Invalid Values\n# ============================================================================\nlogger.info(\"\\n[7/13] Checking for infinite and invalid values...\")\n\n# Check for infinite values\ninf_mask = np.isinf(df_stacked.select_dtypes(include=[np.number]).values)\ninf_count = inf_mask.sum()\n\nif inf_count > 0:\n    logger.warning(f\"  \u26a0 Found {inf_count} infinite values\")\n    numeric_cols = df_stacked.select_dtypes(include=[np.number]).columns\n    df_stacked[numeric_cols] = df_stacked[numeric_cols].replace([np.inf, -np.inf], 0)\n    logger.info(f\"  \u2713 Replaced infinite values with 0\")\nelse:\n    logger.info(f\"  \u2713 No infinite values found\")\n\n# Final NaN check\nfinal_nan_count = df_stacked.isnull().sum().sum()\nlogger.info(f\"  \u2713 Final NaN count: {final_nan_count}\")\n\nif final_nan_count > 0:\n    logger.warning(f\"  \u26a0 WARNING: {final_nan_count} NaN values remain. Filling with 0...\")\n    df_stacked = df_stacked.fillna(0)\n    logger.info(f\"  \u2713 All NaN values resolved\")\n\n# ============================================================================\n# STEP 8: Encode Categorical Variables\n# ============================================================================\nlogger.info(\"\\n[8/13] Encoding categorical variables...\")\n\nlabel_encoders = {}\n\n# Encode ticker\nle_ticker = LabelEncoder()\ndf_stacked['ticker_id'] = le_ticker.fit_transform(df_stacked['ticker'])\nlabel_encoders['ticker'] = le_ticker\n\n# Encode sector\nle_sector = LabelEncoder()\ndf_stacked['sector_id'] = le_sector.fit_transform(df_stacked['sector'])\nlabel_encoders['sector'] = le_sector\n\n# Encode region\nle_region = LabelEncoder()\ndf_stacked['region_id'] = le_region.fit_transform(df_stacked['region'])\nlabel_encoders['region'] = le_region\n\nlogger.info(f\"  \u2713 Encoded ticker: {len(le_ticker.classes_)} unique values\")\nlogger.info(f\"  \u2713 Encoded sector: {len(le_sector.classes_)} unique values\")\nlogger.info(f\"  \u2713 Encoded region: {len(le_region.classes_)} unique values\")\n\n# Save encoders\nwith open(os.path.join(MULTI_TICKER_PATH, 'label_encoders.pkl'), 'wb') as f:\n    pickle.dump(label_encoders, f)\n\nlogger.info(f\"  \u2713 Saved label encoders\")\n\n# ============================================================================\n# STEP 9: Create Train/Validation/Test Splits (Time-Based)\n# ============================================================================\nlogger.info(\"\\n[9/13] Creating time-based splits...\")\n\n# Define split dates\ntrain_end = pd.Timestamp('2020-12-31')\nval_end = pd.Timestamp('2023-12-31')\n\ntrain_mask = df_stacked['date'] <= train_end\nval_mask = (df_stacked['date'] > train_end) & (df_stacked['date'] <= val_end)\ntest_mask = df_stacked['date'] > val_end\n\ndf_train = df_stacked[train_mask].copy()\ndf_val = df_stacked[val_mask].copy()\ndf_test = df_stacked[test_mask].copy()\n\nlogger.info(f\"\\n  Split distribution:\")\nlogger.info(f\"    \u2022 Train: {len(df_train):,} rows ({len(df_train)/len(df_stacked)*100:.1f}%)\")\nlogger.info(f\"    \u2022 Val:   {len(df_val):,} rows ({len(df_val)/len(df_stacked)*100:.1f}%)\")\nlogger.info(f\"    \u2022 Test:  {len(df_test):,} rows ({len(df_test)/len(df_stacked)*100:.1f}%)\")\n\n# ============================================================================\n# STEP 10: Prepare Features and Targets\n# ============================================================================\nlogger.info(\"\\n[10/13] Preparing features and targets...\")\n\n# Define feature columns (exclude metadata and targets)\nexclude_cols = ['date', 'ticker', 'sector', 'region', 'market_cap', 'close', 'volume',\n                'target_5day', 'target_3day', 'target_volatility', 'target_magnitude']\n\nfeature_cols = [col for col in df_stacked.columns if col not in exclude_cols]\n\nlogger.info(f\"  \u2713 Selected {len(feature_cols)} features for modeling\")\n\n# Prepare X (features) and y (target)\nX_train = df_train[['date'] + feature_cols].copy()\nX_val = df_val[['date'] + feature_cols].copy()\nX_test = df_test[['date'] + feature_cols].copy()\n\n# Primary target: 5-day direction\ny_train = df_train[['target_5day']].copy()\ny_train.columns = ['target']\n\ny_val = df_val[['target_5day']].copy()\ny_val.columns = ['target']\n\ny_test = df_test[['target_5day']].copy()\ny_test.columns = ['target']\n\nlogger.info(f\"  \u2713 X_train: {X_train.shape}\")\nlogger.info(f\"  \u2713 X_val: {X_val.shape}\")\nlogger.info(f\"  \u2713 X_test: {X_test.shape}\")\n\n# Check class balance\ntrain_balance = y_train['target'].value_counts()\nlogger.info(f\"\\n  Target distribution (train):\")\nlogger.info(f\"    \u2022 Class 0 (down): {train_balance.get(0, 0):,} ({train_balance.get(0, 0)/len(y_train)*100:.1f}%)\")\nlogger.info(f\"    \u2022 Class 1 (up):   {train_balance.get(1, 0):,} ({train_balance.get(1, 0)/len(y_train)*100:.1f}%)\")\n\n# ============================================================================\n# STEP 11: Feature Scaling\n# ============================================================================\nlogger.info(\"\\n[11/13] Applying feature scaling...\")\n\n# Identify numeric columns (exclude categorical IDs and date)\nnumeric_cols = [col for col in feature_cols if col not in ['ticker_id', 'sector_id', 'region_id', 'date']]\n\nlogger.info(f\"  \u2713 Scaling {len(numeric_cols)} numeric features\")\n\n# Fit scaler on training data only\nscaler = StandardScaler()\nX_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\nX_val[numeric_cols] = scaler.transform(X_val[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n\n# Save scaler\nwith open(os.path.join(MULTI_TICKER_PATH, 'feature_scaler.pkl'), 'wb') as f:\n    pickle.dump(scaler, f)\n\nlogger.info(f\"  \u2713 Feature scaler saved\")\n\n# ============================================================================\n# STEP 12: Final Data Quality Verification\n# ============================================================================\nlogger.info(\"\\n[12/13] Final data quality verification...\")\n\n# Check for any remaining issues\ntrain_issues = {\n    'NaN': X_train.isnull().sum().sum(),\n    'Inf': np.isinf(X_train.select_dtypes(include=[np.number]).values).sum(),\n}\n\nval_issues = {\n    'NaN': X_val.isnull().sum().sum(),\n    'Inf': np.isinf(X_val.select_dtypes(include=[np.number]).values).sum(),\n}\n\ntest_issues = {\n    'NaN': X_test.isnull().sum().sum(),\n    'Inf': np.isinf(X_test.select_dtypes(include=[np.number]).values).sum(),\n}\n\nlogger.info(f\"\\n  Data quality report:\")\nlogger.info(f\"    Train - NaN: {train_issues['NaN']}, Inf: {train_issues['Inf']}\")\nlogger.info(f\"    Val   - NaN: {val_issues['NaN']}, Inf: {val_issues['Inf']}\")\nlogger.info(f\"    Test  - NaN: {test_issues['NaN']}, Inf: {test_issues['Inf']}\")\n\nif any(v > 0 for issues in [train_issues, val_issues, test_issues] for v in issues.values()):\n    logger.warning(f\"\\n  \u26a0 WARNING: Data quality issues detected!\")\n    raise ValueError(\"Data contains NaN or Inf values after all processing steps\")\nelse:\n    logger.info(f\"\\n  \u2713 All data quality checks passed!\")\n\n# ============================================================================\n# STEP 13: Save Prepared Data\n# ============================================================================\nlogger.info(\"\\n[13/13] Saving prepared datasets...\")\n\n# Save datasets\nX_train.to_parquet(os.path.join(MULTI_TICKER_PATH, 'X_train.parquet'), index=False)\nX_val.to_parquet(os.path.join(MULTI_TICKER_PATH, 'X_val.parquet'), index=False)\nX_test.to_parquet(os.path.join(MULTI_TICKER_PATH, 'X_test.parquet'), index=False)\n\ny_train.to_parquet(os.path.join(MULTI_TICKER_PATH, 'y_train.parquet'), index=False)\ny_val.to_parquet(os.path.join(MULTI_TICKER_PATH, 'y_val.parquet'), index=False)\ny_test.to_parquet(os.path.join(MULTI_TICKER_PATH, 'y_test.parquet'), index=False)\n\nlogger.info(f\"  \u2713 Saved X_train.parquet\")\nlogger.info(f\"  \u2713 Saved X_val.parquet\")\nlogger.info(f\"  \u2713 Saved X_test.parquet\")\nlogger.info(f\"  \u2713 Saved y_train.parquet\")\nlogger.info(f\"  \u2713 Saved y_val.parquet\")\nlogger.info(f\"  \u2713 Saved y_test.parquet\")\n\n# Save metadata\nmetadata = {\n    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'num_tickers': len(available_tickers),\n    'num_sectors': len(le_sector.classes_),\n    'num_regions': len(le_region.classes_),\n    'total_samples': len(df_stacked),\n    'train_samples': len(df_train),\n    'val_samples': len(df_val),\n    'test_samples': len(df_test),\n    'num_features': len(feature_cols),\n    'tickers': available_tickers,\n    'sectors': le_sector.classes_.tolist(),\n    'regions': le_region.classes_.tolist(),\n    'feature_columns': feature_cols,\n    'data_quality': {\n        'nan_values': 0,\n        'inf_values': 0,\n        'imputation_applied': bool(astro_nulls > 0),\n    }\n}\n\nwith open(os.path.join(MULTI_TICKER_PATH, 'dataset_metadata.json'), 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nlogger.info(f\"  \u2713 Saved dataset_metadata.json\")\n\n# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 3 PART 1 (MULTI-TICKER PREPARATION) - COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n\ud83d\udcca Dataset Summary:\")\nlogger.info(f\"  \u2022 Total samples: {len(df_stacked):,}\")\nlogger.info(f\"  \u2022 Train: {len(df_train):,} | Val: {len(df_val):,} | Test: {len(df_test):,}\")\nlogger.info(f\"  \u2022 Features: {len(feature_cols)}\")\nlogger.info(f\"  \u2022 Tickers: {len(available_tickers)}\")\nlogger.info(f\"  \u2022 Sectors: {len(le_sector.classes_)}\")\n\nlogger.info(f\"\\n\u2705 Data Quality:\")\nlogger.info(f\"  \u2022 NaN values: 0 (all handled)\")\nlogger.info(f\"  \u2022 Infinite values: 0 (all handled)\")\nlogger.info(f\"  \u2022 Ready for ML training\")\n\nlogger.info(f\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(f\"  1. \u2713 Multi-ticker data prepared ({len(df_stacked):,} samples)\")\nlogger.info(f\"  2. \u25b6 Run Cell 10: LightGBM Training\")\nlogger.info(f\"  3. \u25b6 Run Cell 11: Sector-Specific Models\")\nlogger.info(f\"  4. \u25b6 Run Cell 12: Ensemble Methods\")\nlogger.info(f\"  5. \u25b6 Run Cell 13: Walk-Forward Validation\")\nlogger.info(f\"  6. \u25b6 Run Cell 14: SHAP Analysis\")\n\nlogger.info(f\"\\n\ud83d\udcc2 Output: {MULTI_TICKER_PATH}/\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqK9VEuKmAXV",
    "outputId": "1968d9b6-0cae-41a5-9fd2-0959d4b12057"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "ASTRO-FINANCE PROJECT - PHASE 3: PROFESSIONAL ML PIPELINE\n",
      "Phase 3 Progress: Part 1 of 6 (Multi-Ticker Data Preparation)\n",
      "======================================================================\n",
      "\n",
      "[1/13] Setting up paths...\n",
      "  \u2713 Input: /content/drive/MyDrive/AstroFinanceProject/feature_data\n",
      "  \u2713 Output: /content/drive/MyDrive/AstroFinanceProject/prepared_data/multi_ticker\n",
      "\n",
      "[2/13] Loading master features dataset...\n",
      "  \u2713 Loaded master dataset\n",
      "  \u2713 Shape: (9434, 1127)\n",
      "  \u2713 Date range: 2000-01-01 to 2025-10-29\n",
      "  \u2713 Identified 934 astrological features\n",
      "  \u2713 Astrological features NaN count: 0 (0.00%)\n",
      "\n",
      "[3/13] Handling missing values in astrological features...\n",
      "  \u2713 No missing values in astrological features\n",
      "  \u2713 Final astrological features: 934\n",
      "\n",
      "[4/13] Defining ticker universe and metadata...\n",
      "  \u26a0 Skipping NSEI: only 47.1% data available\n",
      "  \u26a0 Skipping NSEBANK: only 44.2% data available\n",
      "  \u26a0 Skipping NIFTY_FIN_SERVICE_NS: only 36.8% data available\n",
      "  \u26a0 Skipping CNXIT: only 44.0% data available\n",
      "  \u26a0 Skipping CNXPHARMA: only 38.5% data available\n",
      "  \u26a0 Skipping CNXAUTO: only 37.2% data available\n",
      "  \u26a0 Skipping CNXMETAL: only 37.2% data available\n",
      "  \u26a0 Skipping CNXFMCG: only 38.3% data available\n",
      "  \u26a0 Skipping INDIAVIX: only 45.9% data available\n",
      "\n",
      "  \u2713 Found 21 tickers with sufficient data\n",
      "  Available tickers: AAPL, MSFT, NVDA, GSPC, DJI, NDX, RUT, VIX, TNX, RELIANCE_NS...\n",
      "\n",
      "[5/13] Stacking all ticker data...\n",
      "  (This creates ~190,000 samples from multi-ticker expansion)\n",
      "  Processing ticker 5/21...\n",
      "  Processing ticker 10/21...\n",
      "  Processing ticker 15/21...\n",
      "  Processing ticker 20/21...\n",
      "\n",
      "  Concatenating 21 ticker datasets...\n",
      "  \u2713 Stacked dataset created: (134508, 21)\n",
      "  \u2713 Total samples: 134,508\n",
      "\n",
      "  Processing summary:\n",
      "\n",
      "+-------------+-------------+--------+------------+\n",
      "| ticker      | sector      |   rows | null_pct   |\n",
      "+=============+=============+========+============+\n",
      "| AAPL        | Technology  |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| MSFT        | Technology  |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| NVDA        | Technology  |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| GSPC        | Indices     |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| DJI         | Indices     |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| NDX         | Indices     |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| RUT         | Indices     |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| VIX         | Indices     |   6491 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| TNX         | Indices     |   6485 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "| RELIANCE_NS | Commodities |   6437 | 0.0%       |\n",
      "+-------------+-------------+--------+------------+\n",
      "\n",
      "[6/13] Adding astrological features...\n",
      "  \u2713 Added 934 astrological features\n",
      "  \u2713 Shape after merge: (134508, 955)\n",
      "  \u26a0 Found 63 NaN values after merge\n",
      "  Top columns with NaN:\n",
      "    \u2022 bb_position: 21\n",
      "    \u2022 atr_14: 21\n",
      "    \u2022 volatility_20d: 21\n",
      "  \u2713 After filling: 0 NaN remaining\n",
      "  \u2713 Final shape: (134508, 955)\n",
      "\n",
      "[7/13] Checking for infinite and invalid values...\n",
      "  \u2713 No infinite values found\n",
      "  \u2713 Final NaN count: 0\n",
      "\n",
      "[8/13] Encoding categorical variables...\n",
      "  \u2713 Encoded ticker: 21 unique values\n",
      "  \u2713 Encoded sector: 5 unique values\n",
      "  \u2713 Encoded region: 5 unique values\n",
      "  \u2713 Saved label encoders\n",
      "\n",
      "[9/13] Creating time-based splits...\n",
      "\n",
      "  Split distribution:\n",
      "    \u2022 Train: 109,286 rows (81.2%)\n",
      "    \u2022 Val:   15,731 rows (11.7%)\n",
      "    \u2022 Test:  9,491 rows (7.1%)\n",
      "\n",
      "[10/13] Preparing features and targets...\n",
      "  \u2713 Selected 947 features for modeling\n",
      "  \u2713 X_train: (109286, 948)\n",
      "  \u2713 X_val: (15731, 948)\n",
      "  \u2713 X_test: (9491, 948)\n",
      "\n",
      "  Target distribution (train):\n",
      "    \u2022 Class 0 (down): 67,207 (61.5%)\n",
      "    \u2022 Class 1 (up):   42,079 (38.5%)\n",
      "\n",
      "[11/13] Applying feature scaling...\n",
      "  \u2713 Scaling 944 numeric features\n",
      "  \u2713 Feature scaler saved\n",
      "\n",
      "[12/13] Final data quality verification...\n",
      "\n",
      "  Data quality report:\n",
      "    Train - NaN: 0, Inf: 0\n",
      "    Val   - NaN: 0, Inf: 0\n",
      "    Test  - NaN: 0, Inf: 0\n",
      "\n",
      "  \u2713 All data quality checks passed!\n",
      "\n",
      "[13/13] Saving prepared datasets...\n",
      "  \u2713 Saved X_train.parquet\n",
      "  \u2713 Saved X_val.parquet\n",
      "  \u2713 Saved X_test.parquet\n",
      "  \u2713 Saved y_train.parquet\n",
      "  \u2713 Saved y_val.parquet\n",
      "  \u2713 Saved y_test.parquet\n",
      "  \u2713 Saved dataset_metadata.json\n",
      "\n",
      "======================================================================\n",
      "PHASE 3 PART 1 (MULTI-TICKER PREPARATION) - COMPLETE \u2713\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udcca Dataset Summary:\n",
      "  \u2022 Total samples: 134,508\n",
      "  \u2022 Train: 109,286 | Val: 15,731 | Test: 9,491\n",
      "  \u2022 Features: 947\n",
      "  \u2022 Tickers: 21\n",
      "  \u2022 Sectors: 5\n",
      "\n",
      "\u2705 Data Quality:\n",
      "  \u2022 NaN values: 0 (all handled)\n",
      "  \u2022 Infinite values: 0 (all handled)\n",
      "  \u2022 Ready for ML training\n",
      "\n",
      "\ud83d\udccb Next Steps:\n",
      "  1. \u2713 Multi-ticker data prepared (134,508 samples)\n",
      "  2. \u25b6 Run Cell 10: LightGBM Training\n",
      "  3. \u25b6 Run Cell 11: Sector-Specific Models\n",
      "  4. \u25b6 Run Cell 12: Ensemble Methods\n",
      "  5. \u25b6 Run Cell 13: Walk-Forward Validation\n",
      "  6. \u25b6 Run Cell 14: SHAP Analysis\n",
      "\n",
      "\ud83d\udcc2 Output: /content/drive/MyDrive/AstroFinanceProject/prepared_data/multi_ticker/\n",
      "======================================================================\n"
     ]
    }
   ],
   "id": "cell-0012"
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report, log_loss\n)\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n# Cell 10: LightGBM Training - v5 (Phase 3 - Part 2 of 6)\n# ================================================================\n#\n# v5 FIX: AGGRESSIVE REGULARIZATION\n# - Even more conservative than v4\n# - Addresses the moderate performance by reducing overfitting further\n# - Lower learning rates, more dropout, higher regularization\n# - Target: Get Test AUC closer to Val AUC\n#\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport lightgbm as lgb\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 3: LIGHTGBM v5\")\nlogger.info(\"Phase 3 Progress: Part 2 of 6 (AGGRESSIVE Anti-Overfitting)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup and Load Data\n# ============================================================================\nlogger.info(\"\\n[1/10] Loading prepared data...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nMULTI_TICKER_PATH = os.path.join(BASE_PATH, 'prepared_data', 'multi_ticker')\nMODEL_PATH = os.path.join(BASE_PATH, 'models')\nLGBM_PATH = os.path.join(MODEL_PATH, 'lightgbm_improved')\n\nos.makedirs(LGBM_PATH, exist_ok=True)\n\n# Load data\nX_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_train.parquet'))\nX_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_val.parquet'))\nX_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_test.parquet'))\n\ny_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_train.parquet'))['target'].values\ny_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_val.parquet'))['target'].values\ny_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_test.parquet'))['target'].values\n\n# Extract and remove dates\ndates_train = X_train['date']\ndates_val = X_val['date']\ndates_test = X_test['date']\n\nX_train = X_train.drop('date', axis=1)\nX_val = X_val.drop('date', axis=1)\nX_test = X_test.drop('date', axis=1)\n\nlogger.info(f\"  \u2713 Train: {X_train.shape}\")\nlogger.info(f\"  \u2713 Val: {X_val.shape}\")\nlogger.info(f\"  \u2713 Test: {X_test.shape}\")\n\n# Load metadata\nwith open(os.path.join(MULTI_TICKER_PATH, 'dataset_metadata.json'), 'r') as f:\n    metadata = json.load(f)\n\nlogger.info(f\"\\n  Dataset: {metadata['num_tickers']} tickers, {metadata['total_samples']:,} samples\")\n\n# ============================================================================\n# STEP 2: Analyze Dataset Characteristics\n# ============================================================================\nlogger.info(\"\\n[2/10] Analyzing dataset characteristics...\")\n\nclass_counts = np.bincount(y_train)\nimbalance_ratio = class_counts[0] / class_counts[1] if class_counts[1] > 0 else 1\nminority_class_pct = min(class_counts) / sum(class_counts) * 100\n\nlogger.info(f\"  Class 0: {class_counts[0]:,} ({class_counts[0]/len(y_train)*100:.1f}%)\")\nlogger.info(f\"  Class 1: {class_counts[1]:,} ({class_counts[1]/len(y_train)*100:.1f}%)\")\nlogger.info(f\"  Imbalance: {imbalance_ratio:.2f}\")\n\ncategorical_features = [col for col in X_train.columns if col in ['ticker_id', 'sector_id', 'region_id']]\n\n# ============================================================================\n# STEP 3: Train Baseline Models\n# ============================================================================\nlogger.info(\"\\n[3/10] Training baseline models...\")\n\nbaseline_results = []\n\n# Random\nrandom_clf = DummyClassifier(strategy='stratified', random_state=42)\nrandom_clf.fit(X_train, y_train)\ny_pred_random = random_clf.predict(X_test)\nacc_random = accuracy_score(y_test, y_pred_random)\n\nbaseline_results.append({'Model': 'Random', 'Accuracy': f\"{acc_random:.4f}\", 'AUC': '0.5000'})\n\n# Logistic Regression\nlr_clf = LogisticRegression(max_iter=200, random_state=42, n_jobs=-1, verbose=0)\nlr_clf.fit(X_train, y_train)\ny_pred_lr = lr_clf.predict(X_test)\ny_proba_lr = lr_clf.predict_proba(X_test)[:, 1]\nacc_lr = accuracy_score(y_test, y_pred_lr)\nauc_lr = roc_auc_score(y_test, y_proba_lr)\n\nbaseline_results.append({'Model': 'Logistic Regression', 'Accuracy': f\"{acc_lr:.4f}\", 'AUC': f\"{auc_lr:.4f}\"})\n\nlogger.info(f\"  \u2713 Baselines: Random={acc_random:.4f}, LR AUC={auc_lr:.4f}\")\n\n# ============================================================================\n# STEP 4: AGGRESSIVE REGULARIZATION PARAMETERS\n# ============================================================================\nlogger.info(\"\\n[4/10] Setting AGGRESSIVE regularization parameters...\")\n\nbase_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'verbose': -1,\n    'seed': 42,\n    'n_jobs': -1,\n    'force_col_wise': True,\n\n    # AGGRESSIVE REGULARIZATION\n    'num_leaves': 15,                # Reduced from 25\n    'max_depth': 4,                  # Reduced from 6\n    'learning_rate': 0.02,           # Reduced from 0.04\n    'min_child_samples': 100,        # Increased significantly\n\n    # Feature sampling (dropout)\n    'feature_fraction': 0.5,         # Use only 50% of features per tree\n    'feature_fraction_bynode': 0.5,  # Additional node-level dropout\n    'bagging_fraction': 0.6,         # Use only 60% of data per tree\n    'bagging_freq': 5,\n\n    # Regularization\n    'lambda_l1': 1.0,                # Strong L1\n    'lambda_l2': 1.0,                # Strong L2\n    'min_gain_to_split': 0.02,      # Higher split threshold\n    'max_bin': 200,                  # Reduced from default 255\n\n    # Additional anti-overfitting\n    'path_smooth': 1.0,              # Smooth leaf values\n    'min_data_per_group': 50,        # For categorical features\n\n    # Class imbalance\n    'scale_pos_weight': float(imbalance_ratio * 0.7) if imbalance_ratio > 1.5 else 1.0\n}\n\nlogger.info(f\"  \u2713 Ultra-conservative parameters set:\")\nlogger.info(f\"    \u2022 Learning rate: {base_params['learning_rate']} (VERY LOW)\")\nlogger.info(f\"    \u2022 Max depth: {base_params['max_depth']} (SHALLOW)\")\nlogger.info(f\"    \u2022 Feature fraction: {base_params['feature_fraction']} (HIGH DROPOUT)\")\nlogger.info(f\"    \u2022 Regularization: L1={base_params['lambda_l1']}, L2={base_params['lambda_l2']}\")\n\n# ============================================================================\n# STEP 5: Validation-Based Early Stopping\n# ============================================================================\nlogger.info(\"\\n[5/10] Training with validation-based early stopping...\")\n\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\nval_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_data)\n\nlogger.info(f\"  Training with very conservative early stopping (rounds=150)...\")\nevals_result = {}\n\nmodel_val = lgb.train(\n    base_params,\n    train_data,\n    num_boost_round=3000,\n    valid_sets=[train_data, val_data],\n    valid_names=['train', 'valid'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=150, verbose=False),  # Very conservative\n        lgb.record_evaluation(evals_result)\n    ]\n)\n\noptimal_rounds = model_val.best_iteration\ntrain_auc = evals_result['train']['auc'][optimal_rounds - 1]\nval_auc = evals_result['valid']['auc'][optimal_rounds - 1]\nauc_gap = train_auc - val_auc\n\nlogger.info(f\"\\n  \u2713 Stopping results:\")\nlogger.info(f\"    \u2022 Rounds: {optimal_rounds}\")\nlogger.info(f\"    \u2022 Train AUC: {train_auc:.4f}\")\nlogger.info(f\"    \u2022 Val AUC: {val_auc:.4f}\")\nlogger.warning(f\"    \u2022 Gap: {auc_gap:.4f} {'\u2713 EXCELLENT' if auc_gap < 0.03 else '\u2713 OK' if auc_gap < 0.05 else '\u26a0 HIGH'}\")\n\n# ============================================================================\n# STEP 6: Train Final Model\n# ============================================================================\nlogger.info(f\"\\n[6/10] Training final model with {optimal_rounds} rounds...\")\n\nstart_time = datetime.now()\n\nlgb_model = lgb.train(\n    base_params,\n    train_data,\n    num_boost_round=optimal_rounds,\n    valid_sets=[train_data, val_data],\n    valid_names=['train', 'valid'],\n    callbacks=[lgb.log_evaluation(period=200)]\n)\n\ntraining_time = (datetime.now() - start_time).total_seconds()\nlogger.info(f\"  \u2713 Complete in {training_time:.1f}s\")\n\n# ============================================================================\n# STEP 7: Evaluate on Test Set\n# ============================================================================\nlogger.info(\"\\n[7/10] Evaluating on test set...\")\n\ny_pred_proba = lgb_model.predict(X_test)\ny_pred = (y_pred_proba >= 0.5).astype(int)\n\nacc = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nauc = roc_auc_score(y_test, y_pred_proba)\nlogloss = log_loss(y_test, y_pred_proba)\n\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\nlogger.info(f\"\\n  Test Performance:\")\nlogger.info(f\"    \u2022 Accuracy:  {acc:.4f}\")\nlogger.info(f\"    \u2022 Precision: {precision:.4f}\")\nlogger.info(f\"    \u2022 Recall:    {recall:.4f}\")\nlogger.info(f\"    \u2022 F1-Score:  {f1:.4f}\")\nlogger.info(f\"    \u2022 ROC-AUC:   {auc:.4f}\")\n\nlogger.info(f\"\\n  Generalization Check:\")\nlogger.info(f\"    \u2022 Train AUC: {train_auc:.4f}\")\nlogger.info(f\"    \u2022 Val AUC:   {val_auc:.4f}\")\nlogger.info(f\"    \u2022 Test AUC:  {auc:.4f}\")\nlogger.warning(f\"    \u2022 Train\u2192Val: {abs(train_auc - val_auc):.4f} {'\u2713' if abs(train_auc - val_auc) < 0.05 else '\u26a0'}\")\nlogger.warning(f\"    \u2022 Val\u2192Test:  {abs(val_auc - auc):.4f} {'\u2713' if abs(val_auc - auc) < 0.03 else '\u26a0'}\")\n\n# ============================================================================\n# STEP 8: Multi-Threshold Analysis\n# ============================================================================\nlogger.info(\"\\n[8/10] Multi-threshold analysis...\")\n\nthresholds = [0.45, 0.50, 0.55, 0.60, 0.65]\nthreshold_results = []\n\nfor threshold in thresholds:\n    y_pred_t = (y_pred_proba >= threshold).astype(int)\n    if y_pred_t.sum() == 0:\n        continue\n\n    acc_t = accuracy_score(y_test, y_pred_t)\n    prec_t = precision_score(y_test, y_pred_t, zero_division=0)\n    predicted_ups = y_pred_t.sum()\n    actual_wins = (y_pred_t & y_test).sum()\n    win_rate = actual_wins / predicted_ups if predicted_ups > 0 else 0\n\n    threshold_results.append({\n        'Threshold': threshold,\n        'Predictions': predicted_ups,\n        'Win_Rate': f\"{win_rate:.2%}\",\n        'Precision': f\"{prec_t:.4f}\"\n    })\n\nthreshold_df = pd.DataFrame(threshold_results)\nprint(\"\\n\" + tabulate(threshold_df, headers='keys', tablefmt='grid', showindex=False))\n\n# ============================================================================\n# STEP 9: Feature Importance\n# ============================================================================\nlogger.info(\"\\n[9/10] Feature importance...\")\n\nimportance = lgb_model.feature_importance(importance_type='gain')\nfeature_names = lgb_model.feature_name()\n\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\nimportance_df['importance_pct'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n\ndef categorize_feature(feat_name):\n    if feat_name in ['ticker_id', 'sector_id', 'region_id']:\n        return 'Categorical'\n    elif any(x in feat_name for x in ['sun_', 'moon_', 'mercury_', 'venus_', 'mars_', 'jupiter_', 'saturn_']):\n        return 'Planetary'\n    elif any(x in feat_name for x in ['aspect_', 'conjunction', 'opposition', 'trine', 'square']):\n        return 'Aspects'\n    elif any(x in feat_name for x in ['rsi', 'sma', 'bb_', 'atr', 'volume_ratio', 'returns_']):\n        return 'Technical'\n    else:\n        return 'Other'\n\nimportance_df['category'] = importance_df['feature'].apply(categorize_feature)\n\nimportance_df.to_csv(os.path.join(LGBM_PATH, 'feature_importance_detailed.csv'), index=False)\n\nlogger.info(f\"\\n  Top 15 Features:\")\nfor idx, row in importance_df.head(15).iterrows():\n    logger.info(f\"    {row['feature']:35s}: {row['importance_pct']:5.2f}% [{row['category']}]\")\n\ncategory_importance = importance_df.groupby('category')['importance_pct'].sum().sort_values(ascending=False)\nlogger.info(f\"\\n  By Category:\")\nfor cat, pct in category_importance.items():\n    logger.info(f\"    \u2022 {cat:15s}: {pct:5.1f}%\")\n\n# ============================================================================\n# STEP 10: Save Results\n# ============================================================================\nlogger.info(\"\\n[10/10] Saving model and results...\")\n\nlgb_model.save_model(os.path.join(LGBM_PATH, 'lightgbm_model.txt'))\n\nwith open(os.path.join(LGBM_PATH, 'baseline_models.pkl'), 'wb') as f:\n    pickle.dump({'random': random_clf, 'logistic_regression': lr_clf}, f)\n\nperformance = {\n    'model_info': {\n        'version': 'v5_aggressive_regularization',\n        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'optimal_rounds': int(optimal_rounds),\n        'training_time_seconds': float(training_time),\n        'parameters': base_params\n    },\n    'stopping_analysis': {\n        'train_auc': float(train_auc),\n        'val_auc': float(val_auc),\n        'train_val_gap': float(auc_gap)\n    },\n    'test_performance': {\n        'accuracy': float(acc),\n        'precision': float(precision),\n        'recall': float(recall),\n        'f1_score': float(f1),\n        'roc_auc': float(auc),\n        'log_loss': float(logloss),\n        'val_to_test_gap': float(abs(val_auc - auc)),\n        'confusion_matrix': cm.tolist()\n    },\n    'baselines': {\n        'random': {'accuracy': float(acc_random), 'auc': 0.5},\n        'logistic_regression': {'accuracy': float(acc_lr), 'auc': float(auc_lr)}\n    },\n    'feature_importance_top20': importance_df.head(20).to_dict('records'),\n    'category_importance': category_importance.to_dict()\n}\n\nwith open(os.path.join(LGBM_PATH, 'performance_metrics_comprehensive.json'), 'w') as f:\n    json.dump(performance, f, indent=2)\n\nlogger.info(f\"  \u2713 Saved to {LGBM_PATH}/\")\n\n# ============================================================================\n# FINAL ASSESSMENT\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"FINAL ASSESSMENT\")\nlogger.info(\"=\" * 70)\n\nval_test_gap = abs(val_auc - auc)\ntrain_val_gap = abs(train_auc - val_auc)\n\nif train_val_gap < 0.03 and val_test_gap < 0.03 and auc > 0.58:\n    status = \"EXCELLENT\"\n    assessment = f\"\u2705 Strong generalization! Val\u2192Test gap only {val_test_gap:.4f}\"\nelif train_val_gap < 0.05 and val_test_gap < 0.05 and auc > 0.55:\n    status = \"GOOD\"\n    assessment = f\"\u2705 Good generalization. Gaps well controlled.\"\nelif val_test_gap < 0.06:\n    status = \"MODERATE\"\n    assessment = f\"\u26a0 Acceptable but room for improvement\"\nelse:\n    status = \"NEEDS WORK\"\n    assessment = f\"\u26a0 Still overfitting. Consider even simpler model.\"\n\nlogger.info(f\"\\n  Status: {status}\")\nlogger.info(f\"  {assessment}\")\nlogger.info(f\"\\n  Metrics:\")\nlogger.info(f\"    \u2022 Test AUC: {auc:.4f}\")\nlogger.info(f\"    \u2022 Train\u2192Val gap: {train_val_gap:.4f}\")\nlogger.info(f\"    \u2022 Val\u2192Test gap: {val_test_gap:.4f}\")\n\nperformance['status'] = status\nperformance['assessment'] = assessment\n\nwith open(os.path.join(LGBM_PATH, 'performance_metrics_comprehensive.json'), 'w') as f:\n    json.dump(performance, f, indent=2)\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"CELL 10 v5 (AGGRESSIVE REGULARIZATION) - COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\nlogger.info(f\"\\n\ud83d\udccb Next: Run Cell 11 with matching aggressive parameters\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvVB-JzPEk2P",
    "outputId": "ab50afd2-fd3e-4c72-96a3-ee82b51bc601"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "ASTRO-FINANCE PROJECT - PHASE 3: LIGHTGBM v5\n",
      "Phase 3 Progress: Part 2 of 6 (AGGRESSIVE Anti-Overfitting)\n",
      "======================================================================\n",
      "\n",
      "[1/10] Loading prepared data...\n",
      "  \u2713 Train: (109286, 947)\n",
      "  \u2713 Val: (15731, 947)\n",
      "  \u2713 Test: (9491, 947)\n",
      "\n",
      "  Dataset: 21 tickers, 134,508 samples\n",
      "\n",
      "[2/10] Analyzing dataset characteristics...\n",
      "  Class 0: 67,207 (61.5%)\n",
      "  Class 1: 42,079 (38.5%)\n",
      "  Imbalance: 1.60\n",
      "\n",
      "[3/10] Training baseline models...\n",
      "  \u2713 Baselines: Random=0.5299, LR AUC=0.5097\n",
      "\n",
      "[4/10] Setting AGGRESSIVE regularization parameters...\n",
      "  \u2713 Ultra-conservative parameters set:\n",
      "    \u2022 Learning rate: 0.02 (VERY LOW)\n",
      "    \u2022 Max depth: 4 (SHALLOW)\n",
      "    \u2022 Feature fraction: 0.5 (HIGH DROPOUT)\n",
      "    \u2022 Regularization: L1=1.0, L2=1.0\n",
      "\n",
      "[5/10] Training with validation-based early stopping...\n",
      "  Training with very conservative early stopping (rounds=150)...\n",
      "\n",
      "  \u2713 Stopping results:\n",
      "    \u2022 Rounds: 163\n",
      "    \u2022 Train AUC: 0.6880\n",
      "    \u2022 Val AUC: 0.5845\n",
      "    \u2022 Gap: 0.1035 \u26a0 HIGH\n",
      "\n",
      "[6/10] Training final model with 163 rounds...\n",
      "  \u2713 Complete in 33.6s\n",
      "\n",
      "[7/10] Evaluating on test set...\n",
      "\n",
      "  Test Performance:\n",
      "    \u2022 Accuracy:  0.6072\n",
      "    \u2022 Precision: 0.5590\n",
      "    \u2022 Recall:    0.0428\n",
      "    \u2022 F1-Score:  0.0795\n",
      "    \u2022 ROC-AUC:   0.5735\n",
      "\n",
      "  Generalization Check:\n",
      "    \u2022 Train AUC: 0.6880\n",
      "    \u2022 Val AUC:   0.5845\n",
      "    \u2022 Test AUC:  0.5735\n",
      "    \u2022 Train\u2192Val: 0.1035 \u26a0\n",
      "    \u2022 Val\u2192Test:  0.0110 \u2713\n",
      "\n",
      "[8/10] Multi-threshold analysis...\n",
      "\n",
      "+-------------+---------------+------------+-------------+\n",
      "|   Threshold |   Predictions | Win_Rate   |   Precision |\n",
      "+=============+===============+============+=============+\n",
      "|        0.45 |          1912 | 45.35%     |      0.4535 |\n",
      "+-------------+---------------+------------+-------------+\n",
      "|        0.5  |           288 | 55.90%     |      0.559  |\n",
      "+-------------+---------------+------------+-------------+\n",
      "|        0.55 |             6 | 50.00%     |      0.5    |\n",
      "+-------------+---------------+------------+-------------+\n",
      "\n",
      "[9/10] Feature importance...\n",
      "\n",
      "  Top 15 Features:\n",
      "    volatility_20d                     : 14.46% [Other]\n",
      "    ticker_id                          :  9.78% [Categorical]\n",
      "    sector_id                          :  4.11% [Categorical]\n",
      "    returns_5d                         :  3.20% [Technical]\n",
      "    sma_50                             :  2.06% [Technical]\n",
      "    mars_saturn_square_exact_dist      :  1.94% [Planetary]\n",
      "    sma_20                             :  1.86% [Technical]\n",
      "    jupiter_longitude                  :  1.84% [Planetary]\n",
      "    volume_ratio                       :  1.77% [Technical]\n",
      "    returns_20d                        :  1.72% [Technical]\n",
      "    bb_position                        :  1.16% [Technical]\n",
      "    mars_dignity_score                 :  1.07% [Planetary]\n",
      "    day_of_month                       :  1.06% [Other]\n",
      "    venus_rahu_sextile_exact_dist      :  1.01% [Planetary]\n",
      "    atr_14                             :  0.88% [Technical]\n",
      "\n",
      "  By Category:\n",
      "    \u2022 Planetary      :  54.0%\n",
      "    \u2022 Other          :  17.7%\n",
      "    \u2022 Categorical    :  14.7%\n",
      "    \u2022 Technical      :  13.5%\n",
      "    \u2022 Aspects        :   0.1%\n",
      "\n",
      "[10/10] Saving model and results...\n",
      "  \u2713 Saved to /content/drive/MyDrive/AstroFinanceProject/models/lightgbm_improved/\n",
      "\n",
      "======================================================================\n",
      "FINAL ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "  Status: MODERATE\n",
      "  \u26a0 Acceptable but room for improvement\n",
      "\n",
      "  Metrics:\n",
      "    \u2022 Test AUC: 0.5735\n",
      "    \u2022 Train\u2192Val gap: 0.1035\n",
      "    \u2022 Val\u2192Test gap: 0.0110\n",
      "\n",
      "======================================================================\n",
      "CELL 10 v5 (AGGRESSIVE REGULARIZATION) - COMPLETE \u2713\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udccb Next: Run Cell 11 with matching aggressive parameters\n",
      "======================================================================\n"
     ]
    }
   ],
   "id": "cell-0013"
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix\n)\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n# Cell 11: Sector-Specific Models - v3 (Phase 3 - Part 3 of 6)\n# ================================================================\n#\n# v3 FIX: AGGRESSIVE REGULARIZATION (matching Cell 10 v5)\n# - Even more conservative parameters per sector\n# - Target: Reduce the 4/4 overfitting sectors to 0\n#\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 3: SECTOR MODELS v3\")\nlogger.info(\"Phase 3 Progress: Part 3 of 6 (AGGRESSIVE Anti-Overfitting)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup and Load Data\n# ============================================================================\nlogger.info(\"\\n[1/8] Loading data...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nMULTI_TICKER_PATH = os.path.join(BASE_PATH, 'prepared_data', 'multi_ticker')\nMODEL_PATH = os.path.join(BASE_PATH, 'models')\nSECTOR_MODEL_PATH = os.path.join(MODEL_PATH, 'sector_models_improved')\n\nos.makedirs(SECTOR_MODEL_PATH, exist_ok=True)\n\nX_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_train.parquet'))\nX_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_val.parquet'))\nX_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_test.parquet'))\n\ny_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_train.parquet'))['target'].values\ny_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_val.parquet'))['target'].values\ny_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_test.parquet'))['target'].values\n\nX_train = X_train.drop('date', axis=1)\nX_val = X_val.drop('date', axis=1)\nX_test = X_test.drop('date', axis=1)\n\nwith open(os.path.join(MULTI_TICKER_PATH, 'label_encoders.pkl'), 'rb') as f:\n    encoders = pickle.load(f)\n\nlogger.info(f\"  \u2713 Data loaded: {X_train.shape}\")\n\n# ============================================================================\n# STEP 2: Get Sector Assignments\n# ============================================================================\nlogger.info(\"\\n[2/8] Analyzing sectors...\")\n\nsector_ids_train = X_train['sector_id'].values\nsector_ids_val = X_val['sector_id'].values\nsector_ids_test = X_test['sector_id'].values\n\nsectors_train = encoders['sector'].inverse_transform(sector_ids_train.astype(int))\nsectors_val = encoders['sector'].inverse_transform(sector_ids_val.astype(int))\nsectors_test = encoders['sector'].inverse_transform(sector_ids_test.astype(int))\n\nsector_analysis = []\nfor sector_name in encoders['sector'].classes_:\n    train_mask = sectors_train == sector_name\n    if train_mask.sum() > 0:\n        sector_y = y_train[train_mask]\n        class_counts = np.bincount(sector_y)\n        imbalance = class_counts[0] / class_counts[1] if len(class_counts) > 1 and class_counts[1] > 0 else 999\n\n        sector_analysis.append({\n            'Sector': sector_name,\n            'Samples': int(train_mask.sum()),\n            'Imbalance': float(imbalance)\n        })\n\nsector_analysis_df = pd.DataFrame(sector_analysis)\nprint(\"\\n\" + tabulate(sector_analysis_df, headers='keys', tablefmt='grid', showindex=False))\n\n# ============================================================================\n# STEP 3: AGGRESSIVE Sector Parameters\n# ============================================================================\nlogger.info(\"\\n[3/8] Setting AGGRESSIVE sector parameters...\")\n\ndef get_aggressive_params(train_size, imbalance):\n    \"\"\"Ultra-conservative parameters to eliminate overfitting.\"\"\"\n\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'verbose': -1,\n        'seed': 42,\n        'n_jobs': -1,\n        'force_col_wise': True,\n\n        # VERY AGGRESSIVE REGULARIZATION\n        'num_leaves': 10 if train_size < 1000 else 12 if train_size < 3000 else 15,\n        'max_depth': 3 if train_size < 1000 else 4,\n        'learning_rate': 0.015 if train_size < 1000 else 0.02,\n        'min_child_samples': max(80, train_size // 20),  # Very high\n\n        # Extreme dropout\n        'feature_fraction': 0.4,\n        'feature_fraction_bynode': 0.4,\n        'bagging_fraction': 0.5,\n        'bagging_freq': 5,\n\n        # Strong regularization\n        'lambda_l1': 1.5,\n        'lambda_l2': 1.5,\n        'min_gain_to_split': 0.03,\n        'max_bin': 150,\n        'path_smooth': 1.0,\n\n        # Training config\n        'max_rounds': 1000 if train_size < 1000 else 1500,\n        'early_stopping': 200,  # Very patient\n\n        # Imbalance\n        'scale_pos_weight': float(imbalance * 0.5) if imbalance > 2 else 1.0\n    }\n\n    return params\n\ncategorical_features = ['ticker_id', 'sector_id', 'region_id']\n\n# ============================================================================\n# STEP 4: Train Sector Models\n# ============================================================================\nlogger.info(\"\\n[4/8] Training sector models with AGGRESSIVE regularization...\")\n\nsector_models = {}\nsector_results = []\nMIN_SAMPLES = 150\n\nfor sector_name in encoders['sector'].classes_:\n    train_mask = sectors_train == sector_name\n    val_mask = sectors_val == sector_name\n    test_mask = sectors_test == sector_name\n\n    if train_mask.sum() < MIN_SAMPLES or val_mask.sum() < 30:\n        logger.info(f\"  \u2298 {sector_name}: Skipped (insufficient samples)\")\n        continue\n\n    logger.info(f\"\\n  \u2192 {sector_name}\")\n    logger.info(f\"     Train={train_mask.sum()}, Val={val_mask.sum()}, Test={test_mask.sum()}\")\n\n    X_sec_train = X_train[train_mask]\n    y_sec_train = y_train[train_mask]\n    X_sec_val = X_val[val_mask]\n    y_sec_val = y_val[val_mask]\n    X_sec_test = X_test[test_mask]\n    y_sec_test = y_test[test_mask]\n\n    # Get imbalance for this sector\n    class_counts = np.bincount(y_sec_train)\n    imbalance = class_counts[0] / class_counts[1] if len(class_counts) > 1 and class_counts[1] > 0 else 1.0\n\n    params = get_aggressive_params(train_mask.sum(), imbalance)\n    max_rounds = params.pop('max_rounds')\n    early_stop = params.pop('early_stopping')\n\n    train_data = lgb.Dataset(X_sec_train, label=y_sec_train, categorical_feature=categorical_features)\n    val_data = lgb.Dataset(X_sec_val, label=y_sec_val, categorical_feature=categorical_features, reference=train_data)\n\n    try:\n        evals_result = {}\n        model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=max_rounds,\n            valid_sets=[train_data, val_data],\n            valid_names=['train', 'valid'],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=early_stop, verbose=False),\n                lgb.record_evaluation(evals_result)\n            ]\n        )\n\n        rounds = model.best_iteration\n        train_auc = evals_result['train']['auc'][rounds - 1]\n        val_auc = evals_result['valid']['auc'][rounds - 1]\n\n        # Test\n        y_pred_proba = model.predict(X_sec_test)\n        test_auc = roc_auc_score(y_sec_test, y_pred_proba) if len(np.unique(y_sec_test)) > 1 else 0.5\n\n        train_val_gap = abs(train_auc - val_auc)\n        val_test_gap = abs(val_auc - test_auc)\n\n        logger.info(f\"     Rounds: {rounds}\")\n        logger.info(f\"     Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}, Test AUC: {test_auc:.4f}\")\n        logger.info(f\"     Train\u2192Val: {train_val_gap:.4f}, Val\u2192Test: {val_test_gap:.4f}\")\n\n        # Strict overfitting criteria\n        is_overfit = (train_val_gap > 0.04) or (val_test_gap > 0.04)\n        status = 'OVERFIT' if is_overfit else 'OK'\n\n        logger.warning(f\"     Status: {status} {'\u26a0' if is_overfit else '\u2713'}\")\n\n        sector_models[sector_name] = model\n        sector_results.append({\n            'Sector': sector_name,\n            'Samples': int(train_mask.sum()),\n            'Rounds': int(rounds),\n            'Train_AUC': float(train_auc),\n            'Val_AUC': float(val_auc),\n            'Test_AUC': float(test_auc),\n            'Train_Val_Gap': float(train_val_gap),\n            'Val_Test_Gap': float(val_test_gap),\n            'Status': status\n        })\n\n        model.save_model(os.path.join(SECTOR_MODEL_PATH, f\"{sector_name.replace(' ', '_')}_model.txt\"))\n\n    except Exception as e:\n        logger.error(f\"     \u2717 Failed: {e}\")\n\nlogger.info(f\"\\n  \u2713 Trained {len(sector_models)} models\")\n\n# ============================================================================\n# STEP 5: Results Summary\n# ============================================================================\nlogger.info(\"\\n[5/8] Sector performance summary...\")\n\nif sector_results:\n    results_df = pd.DataFrame(sector_results)\n\n    print(\"\\n\" + tabulate(results_df, headers='keys', tablefmt='grid', showindex=False,\n                          floatfmt=('.0f', '.0f', '.0f', '.4f', '.4f', '.4f', '.4f', '.4f', 's')))\n\n    overfit_count = (results_df['Status'] == 'OVERFIT').sum()\n    avg_val_test_gap = results_df['Val_Test_Gap'].mean()\n\n    logger.info(f\"\\n  Statistics:\")\n    logger.info(f\"    \u2022 Avg Test AUC: {results_df['Test_AUC'].mean():.4f}\")\n    logger.info(f\"    \u2022 Avg Val\u2192Test gap: {avg_val_test_gap:.4f}\")\n    logger.info(f\"    \u2022 Overfitting sectors: {overfit_count}/{len(results_df)}\")\n\n    results_df.to_csv(os.path.join(SECTOR_MODEL_PATH, 'sector_performance.csv'), index=False)\n\n# ============================================================================\n# STEP 6-8: Save and Assess\n# ============================================================================\nlogger.info(\"\\n[6/8] Saving results...\")\n\ncomprehensive = {\n    'version': 'v3_aggressive_regularization',\n    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'num_models': len(sector_models),\n    'sector_results': results_df.to_dict('records') if sector_results else []\n}\n\nwith open(os.path.join(SECTOR_MODEL_PATH, 'comprehensive_results.json'), 'w') as f:\n    json.dump(comprehensive, f, indent=2)\n\nlogger.info(\"\\n[7/8] Final assessment...\")\n\nif sector_results:\n    if overfit_count == 0 and avg_val_test_gap < 0.03:\n        status = \"EXCELLENT\"\n        assessment = \"\u2705 Zero overfitting! All sectors generalize well.\"\n    elif overfit_count <= len(results_df) * 0.25 and avg_val_test_gap < 0.04:\n        status = \"GOOD\"\n        assessment = f\"\u2705 Minimal overfitting ({overfit_count} sectors)\"\n    else:\n        status = \"IMPROVED\"\n        assessment = f\"\u26a0 Still {overfit_count} overfitting sectors\"\n\n    logger.info(f\"\\n  Status: {status}\")\n    logger.info(f\"  {assessment}\")\nelse:\n    status = \"NO MODELS\"\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"CELL 11 v3 (AGGRESSIVE) - COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\nlogger.info(f\"\\n\ud83d\udccb Status: {status}\")\nlogger.info(f\"\ud83d\udccb Next: Run Cell 12 (should perform even better now)\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNn8r_tMohFY",
    "outputId": "20620582-fe45-4814-d9f8-38135c77c9d5"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "ASTRO-FINANCE PROJECT - PHASE 3: SECTOR MODELS v3\n",
      "Phase 3 Progress: Part 3 of 6 (AGGRESSIVE Anti-Overfitting)\n",
      "======================================================================\n",
      "\n",
      "[1/8] Loading data...\n",
      "  \u2713 Data loaded: (109286, 947)\n",
      "\n",
      "[2/8] Analyzing sectors...\n",
      "\n",
      "+-------------+-----------+-------------+\n",
      "| Sector      |   Samples |   Imbalance |\n",
      "+=============+===========+=============+\n",
      "| Commodities |     20563 |     1.42603 |\n",
      "+-------------+-----------+-------------+\n",
      "| Currencies  |      5311 |     5.1046  |\n",
      "+-------------+-----------+-------------+\n",
      "| Finance     |      5252 |     1.42363 |\n",
      "+-------------+-----------+-------------+\n",
      "| Indices     |     57738 |     1.67777 |\n",
      "+-------------+-----------+-------------+\n",
      "| Technology  |     20422 |     1.2681  |\n",
      "+-------------+-----------+-------------+\n",
      "\n",
      "[3/8] Setting AGGRESSIVE sector parameters...\n",
      "\n",
      "[4/8] Training sector models with AGGRESSIVE regularization...\n",
      "\n",
      "  \u2192 Commodities\n",
      "     Train=20563, Val=3000, Test=1815\n",
      "     Rounds: 2\n",
      "     Train AUC: 0.6000, Val AUC: 0.5550, Test AUC: 0.4847\n",
      "     Train\u2192Val: 0.0450, Val\u2192Test: 0.0704\n",
      "     Status: OVERFIT \u26a0\n",
      "\n",
      "  \u2192 Currencies\n",
      "     Train=5311, Val=753, Test=456\n",
      "     Rounds: 13\n",
      "     Train AUC: 0.7181, Val AUC: 0.6043, Test AUC: 0.3400\n",
      "     Train\u2192Val: 0.1138, Val\u2192Test: 0.2643\n",
      "     Status: OVERFIT \u26a0\n",
      "\n",
      "  \u2192 Finance\n",
      "     Train=5252, Val=741, Test=447\n",
      "     Rounds: 11\n",
      "     Train AUC: 0.6685, Val AUC: 0.6010, Test AUC: 0.5763\n",
      "     Train\u2192Val: 0.0675, Val\u2192Test: 0.0247\n",
      "     Status: OVERFIT \u26a0\n",
      "\n",
      "  \u2192 Indices\n",
      "     Train=57738, Val=8237, Test=4964\n",
      "     Rounds: 7\n",
      "     Train AUC: 0.6087, Val AUC: 0.5711, Test AUC: 0.5886\n",
      "     Train\u2192Val: 0.0376, Val\u2192Test: 0.0175\n",
      "     Status: OK \u2713\n",
      "\n",
      "  \u2192 Technology\n",
      "     Train=20422, Val=3000, Test=1809\n",
      "     Rounds: 2\n",
      "     Train AUC: 0.5848, Val AUC: 0.5508, Test AUC: 0.5168\n",
      "     Train\u2192Val: 0.0340, Val\u2192Test: 0.0340\n",
      "     Status: OK \u2713\n",
      "\n",
      "  \u2713 Trained 5 models\n",
      "\n",
      "[5/8] Sector performance summary...\n",
      "\n",
      "+-------------+-----------+----------+-------------+-----------+------------+-----------------+----------------+----------+\n",
      "| Sector      |   Samples |   Rounds |   Train_AUC |   Val_AUC |   Test_AUC |   Train_Val_Gap |   Val_Test_Gap | Status   |\n",
      "+=============+===========+==========+=============+===========+============+=================+================+==========+\n",
      "| Commodities |     20563 |        2 |      0.6000 |    0.5550 |     0.4847 |          0.0450 |         0.0704 | OVERFIT  |\n",
      "+-------------+-----------+----------+-------------+-----------+------------+-----------------+----------------+----------+\n",
      "| Currencies  |      5311 |       13 |      0.7181 |    0.6043 |     0.3400 |          0.1138 |         0.2643 | OVERFIT  |\n",
      "+-------------+-----------+----------+-------------+-----------+------------+-----------------+----------------+----------+\n",
      "| Finance     |      5252 |       11 |      0.6685 |    0.6010 |     0.5763 |          0.0675 |         0.0247 | OVERFIT  |\n",
      "+-------------+-----------+----------+-------------+-----------+------------+-----------------+----------------+----------+\n",
      "| Indices     |     57738 |        7 |      0.6087 |    0.5711 |     0.5886 |          0.0376 |         0.0175 | OK       |\n",
      "+-------------+-----------+----------+-------------+-----------+------------+-----------------+----------------+----------+\n",
      "| Technology  |     20422 |        2 |      0.5848 |    0.5508 |     0.5168 |          0.0340 |         0.0340 | OK       |\n",
      "+-------------+-----------+----------+-------------+-----------+------------+-----------------+----------------+----------+\n",
      "\n",
      "  Statistics:\n",
      "    \u2022 Avg Test AUC: 0.5013\n",
      "    \u2022 Avg Val\u2192Test gap: 0.0822\n",
      "    \u2022 Overfitting sectors: 3/5\n",
      "\n",
      "[6/8] Saving results...\n",
      "\n",
      "[7/8] Final assessment...\n",
      "\n",
      "  Status: IMPROVED\n",
      "  \u26a0 Still 3 overfitting sectors\n",
      "\n",
      "======================================================================\n",
      "CELL 11 v3 (AGGRESSIVE) - COMPLETE \u2713\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udccb Status: IMPROVED\n",
      "\ud83d\udccb Next: Run Cell 12 (should perform even better now)\n",
      "======================================================================\n"
     ]
    }
   ],
   "id": "cell-0014"
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, log_loss, brier_score_loss\n)\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n# Cell 12: Ensemble Methods - IMPROVED v2 (Phase 3 - Part 4 of 6)\n# ================================================================\n#\n# IMPROVEMENTS v2:\n# 1. Validation-based meta-model training (NOT CV-based)\n# 2. Conservative stacking to prevent overfitting\n# 3. Monitors ensemble train/val/test gaps\n# 4. Adaptive weighting based on validation performance\n# 5. Overfitting detection in ensemble\n#\n# Key Fix: Uses validation set for meta-model training (same approach as Cells 10 & 11 v4/v2)\n#\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 3: ENSEMBLE METHODS v2\")\nlogger.info(\"Phase 3 Progress: Part 4 of 6 (Validation-Based Anti-Overfitting)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup and Load Data\n# ============================================================================\nlogger.info(\"\\n[1/11] Loading prepared data and existing models...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nMULTI_TICKER_PATH = os.path.join(BASE_PATH, 'prepared_data', 'multi_ticker')\nMODEL_PATH = os.path.join(BASE_PATH, 'models')\nLGBM_PATH = os.path.join(MODEL_PATH, 'lightgbm_improved')\nSECTOR_MODEL_PATH = os.path.join(MODEL_PATH, 'sector_models_improved')\nENSEMBLE_PATH = os.path.join(MODEL_PATH, 'ensemble_improved')\n\nos.makedirs(ENSEMBLE_PATH, exist_ok=True)\n\n# Load data\nX_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_train.parquet'))\nX_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_val.parquet'))\nX_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_test.parquet'))\n\ny_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_train.parquet'))['target'].values\ny_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_val.parquet'))['target'].values\ny_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_test.parquet'))['target'].values\n\n# Remove dates but keep for reference\ndates_train = X_train['date']\ndates_val = X_val['date']\ndates_test = X_test['date']\n\nX_train = X_train.drop('date', axis=1)\nX_val = X_val.drop('date', axis=1)\nX_test = X_test.drop('date', axis=1)\n\n# Load encoders\nwith open(os.path.join(MULTI_TICKER_PATH, 'label_encoders.pkl'), 'rb') as f:\n    encoders = pickle.load(f)\n\nlogger.info(f\"  \u2713 Data loaded: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}\")\n\n# ============================================================================\n# STEP 2: Load Base Models\n# ============================================================================\nlogger.info(\"\\n[2/11] Loading base models...\")\n\n# Load global model\ntry:\n    global_lgbm = lgb.Booster(model_file=os.path.join(LGBM_PATH, 'lightgbm_model.txt'))\n    logger.info(f\"  \u2713 Loaded global LightGBM model\")\n    has_global = True\nexcept:\n    logger.info(f\"  \u2298 Global LightGBM model not found\")\n    has_global = False\n\n# Load sector models\nsector_models = {}\nif os.path.exists(SECTOR_MODEL_PATH):\n    sector_files = [f for f in os.listdir(SECTOR_MODEL_PATH) if f.endswith('_model.txt')]\n    for sector_file in sector_files:\n        sector_name = sector_file.replace('_model.txt', '').replace('_', ' ')\n        model_path = os.path.join(SECTOR_MODEL_PATH, sector_file)\n        try:\n            sector_models[sector_name] = lgb.Booster(model_file=model_path)\n        except:\n            logger.info(f\"  \u2298 Failed to load {sector_name}\")\n    logger.info(f\"  \u2713 Loaded {len(sector_models)} sector-specific models\")\nelse:\n    logger.info(f\"  \u2298 Sector models directory not found\")\n\nif not has_global and len(sector_models) == 0:\n    logger.error(\"\\n  \u2717 ERROR: No base models found. Please run Cells 10 and 11 first.\")\n    raise FileNotFoundError(\"No base models available for ensemble\")\n\n# ============================================================================\n# STEP 3: Generate Base Model Predictions\n# ============================================================================\nlogger.info(\"\\n[3/11] Generating base model predictions...\")\n\n# Get sector assignments\nsector_ids_train = X_train['sector_id'].values\nsector_ids_val = X_val['sector_id'].values\nsector_ids_test = X_test['sector_id'].values\n\nsectors_train = encoders['sector'].inverse_transform(sector_ids_train.astype(int))\nsectors_val = encoders['sector'].inverse_transform(sector_ids_val.astype(int))\nsectors_test = encoders['sector'].inverse_transform(sector_ids_test.astype(int))\n\n# Initialize prediction dataframes\nbase_preds_train = pd.DataFrame()\nbase_preds_val = pd.DataFrame()\nbase_preds_test = pd.DataFrame()\n\n# Global model predictions\nif has_global:\n    logger.info(\"  \u2192 Global LightGBM predictions...\")\n    base_preds_train['global_lgbm'] = global_lgbm.predict(X_train)\n    base_preds_val['global_lgbm'] = global_lgbm.predict(X_val)\n    base_preds_test['global_lgbm'] = global_lgbm.predict(X_test)\n\n# Sector-specific predictions\nif len(sector_models) > 0:\n    logger.info(\"  \u2192 Sector-specific predictions...\")\n\n    def get_sector_predictions(X, sectors, models, fallback_preds=None):\n        preds = np.zeros(len(X))\n        for i, sector in enumerate(sectors):\n            if sector in models:\n                preds[i] = models[sector].predict(X.iloc[[i]])[0]\n            elif fallback_preds is not None:\n                preds[i] = fallback_preds[i]\n            else:\n                preds[i] = 0.5  # neutral if no model available\n        return preds\n\n    fallback_train = base_preds_train['global_lgbm'].values if has_global else None\n    fallback_val = base_preds_val['global_lgbm'].values if has_global else None\n    fallback_test = base_preds_test['global_lgbm'].values if has_global else None\n\n    base_preds_train['sector_specific'] = get_sector_predictions(X_train, sectors_train, sector_models, fallback_train)\n    base_preds_val['sector_specific'] = get_sector_predictions(X_val, sectors_val, sector_models, fallback_val)\n    base_preds_test['sector_specific'] = get_sector_predictions(X_test, sectors_test, sector_models, fallback_test)\n\nlogger.info(f\"  \u2713 Generated {len(base_preds_train.columns)} base prediction sets\")\n\n# Evaluate base models\nlogger.info(f\"\\n  Base Model Performance (Validation Set):\")\nfor col in base_preds_val.columns:\n    val_auc = roc_auc_score(y_val, base_preds_val[col])\n    test_auc = roc_auc_score(y_test, base_preds_test[col])\n    logger.info(f\"    \u2022 {col:20s}: Val AUC={val_auc:.4f}, Test AUC={test_auc:.4f}, Gap={abs(val_auc-test_auc):.4f}\")\n\n# ============================================================================\n# STEP 4: Simple Averaging Ensemble\n# ============================================================================\nlogger.info(\"\\n[4/11] Creating simple averaging ensemble...\")\n\navg_preds_train = base_preds_train.mean(axis=1).values\navg_preds_val = base_preds_val.mean(axis=1).values\navg_preds_test = base_preds_test.mean(axis=1).values\n\navg_train_auc = roc_auc_score(y_train, avg_preds_train)\navg_val_auc = roc_auc_score(y_val, avg_preds_val)\navg_test_auc = roc_auc_score(y_test, avg_preds_test)\n\nlogger.info(f\"  Simple Average Ensemble:\")\nlogger.info(f\"    \u2022 Train AUC: {avg_train_auc:.4f}\")\nlogger.info(f\"    \u2022 Val AUC:   {avg_val_auc:.4f}\")\nlogger.info(f\"    \u2022 Test AUC:  {avg_test_auc:.4f}\")\nlogger.info(f\"    \u2022 Train\u2192Val gap: {abs(avg_train_auc - avg_val_auc):.4f}\")\nlogger.info(f\"    \u2022 Val\u2192Test gap:  {abs(avg_val_auc - avg_test_auc):.4f}\")\n\n# ============================================================================\n# STEP 5: Validation-Based Weighted Ensemble\n# ============================================================================\nlogger.info(\"\\n[5/11] Creating validation-based weighted ensemble...\")\n\n# Weight each model by its validation AUC performance above random (0.5)\nval_weights = []\nfor col in base_preds_val.columns:\n    val_auc = roc_auc_score(y_val, base_preds_val[col])\n    weight = max(0, val_auc - 0.5)  # Performance above random\n    val_weights.append(weight)\n\n# Normalize weights\nval_weights = np.array(val_weights)\nif val_weights.sum() > 0:\n    val_weights = val_weights / val_weights.sum()\nelse:\n    val_weights = np.ones(len(val_weights)) / len(val_weights)\n\nlogger.info(f\"  Validation-Based Weights:\")\nfor col, weight in zip(base_preds_val.columns, val_weights):\n    logger.info(f\"    \u2022 {col:20s}: {weight:.3f}\")\n\n# Create weighted predictions\nweighted_preds_train = (base_preds_train.values * val_weights).sum(axis=1)\nweighted_preds_val = (base_preds_val.values * val_weights).sum(axis=1)\nweighted_preds_test = (base_preds_test.values * val_weights).sum(axis=1)\n\nweighted_train_auc = roc_auc_score(y_train, weighted_preds_train)\nweighted_val_auc = roc_auc_score(y_val, weighted_preds_val)\nweighted_test_auc = roc_auc_score(y_test, weighted_preds_test)\n\nlogger.info(f\"\\n  Weighted Ensemble:\")\nlogger.info(f\"    \u2022 Train AUC: {weighted_train_auc:.4f}\")\nlogger.info(f\"    \u2022 Val AUC:   {weighted_val_auc:.4f}\")\nlogger.info(f\"    \u2022 Test AUC:  {weighted_test_auc:.4f}\")\nlogger.info(f\"    \u2022 Train\u2192Val gap: {abs(weighted_train_auc - weighted_val_auc):.4f}\")\nlogger.info(f\"    \u2022 Val\u2192Test gap:  {abs(weighted_val_auc - weighted_test_auc):.4f}\")\n\n# ============================================================================\n# STEP 6: Conservative Stacking Meta-Model\n# ============================================================================\nlogger.info(\"\\n[6/11] Training conservative stacking meta-model...\")\n\n# Use simple logistic regression with strong regularization as meta-model\nmeta_model = LogisticRegression(\n    C=0.1,  # Strong regularization\n    penalty='l2',\n    max_iter=200,\n    random_state=42,\n    solver='lbfgs'\n)\n\n# Train on validation set (more realistic than CV)\nlogger.info(f\"  Training meta-model on validation set...\")\nmeta_model.fit(base_preds_val, y_val)\n\n# Generate predictions\nstack_preds_train = meta_model.predict_proba(base_preds_train)[:, 1]\nstack_preds_val = meta_model.predict_proba(base_preds_val)[:, 1]\nstack_preds_test = meta_model.predict_proba(base_preds_test)[:, 1]\n\nstack_train_auc = roc_auc_score(y_train, stack_preds_train)\nstack_val_auc = roc_auc_score(y_val, stack_preds_val)\nstack_test_auc = roc_auc_score(y_test, stack_preds_test)\n\nlogger.info(f\"\\n  Stacked Meta-Model:\")\nlogger.info(f\"    \u2022 Train AUC: {stack_train_auc:.4f}\")\nlogger.info(f\"    \u2022 Val AUC:   {stack_val_auc:.4f}\")\nlogger.info(f\"    \u2022 Test AUC:  {stack_test_auc:.4f}\")\nlogger.info(f\"    \u2022 Train\u2192Val gap: {abs(stack_train_auc - stack_val_auc):.4f}\")\nlogger.info(f\"    \u2022 Val\u2192Test gap:  {abs(stack_val_auc - stack_test_auc):.4f}\")\n\n# Meta-model coefficients\nlogger.info(f\"\\n  Meta-Model Coefficients:\")\nfor col, coef in zip(base_preds_val.columns, meta_model.coef_[0]):\n    logger.info(f\"    \u2022 {col:20s}: {coef:+.3f}\")\n\n# ============================================================================\n# STEP 7: Ensemble Comparison\n# ============================================================================\nlogger.info(\"\\n[7/11] Comparing ensemble methods...\")\n\nensemble_comparison = pd.DataFrame([\n    {\n        'Method': 'Simple Average',\n        'Train_AUC': f\"{avg_train_auc:.4f}\",\n        'Val_AUC': f\"{avg_val_auc:.4f}\",\n        'Test_AUC': f\"{avg_test_auc:.4f}\",\n        'Val_Test_Gap': f\"{abs(avg_val_auc - avg_test_auc):.4f}\"\n    },\n    {\n        'Method': 'Weighted (Val-Based)',\n        'Train_AUC': f\"{weighted_train_auc:.4f}\",\n        'Val_AUC': f\"{weighted_val_auc:.4f}\",\n        'Test_AUC': f\"{weighted_test_auc:.4f}\",\n        'Val_Test_Gap': f\"{abs(weighted_val_auc - weighted_test_auc):.4f}\"\n    },\n    {\n        'Method': 'Stacked Meta-Model',\n        'Train_AUC': f\"{stack_train_auc:.4f}\",\n        'Val_AUC': f\"{stack_val_auc:.4f}\",\n        'Test_AUC': f\"{stack_test_auc:.4f}\",\n        'Val_Test_Gap': f\"{abs(stack_val_auc - stack_test_auc):.4f}\"\n    }\n])\n\nprint(\"\\n\" + tabulate(ensemble_comparison, headers='keys', tablefmt='grid', showindex=False))\n\n# Select best ensemble based on validation performance and minimal overfitting\nbest_idx = ensemble_comparison.apply(\n    lambda row: float(row['Val_AUC']) - 2*float(row['Val_Test_Gap']),  # Penalize overfitting\n    axis=1\n).idxmax()\n\nbest_method = ensemble_comparison.iloc[best_idx]['Method']\nlogger.info(f\"\\n  \u2713 Best ensemble method: {best_method}\")\n\n# Use best method predictions\nif best_idx == 0:\n    final_preds_test = avg_preds_test\n    final_preds_val = avg_preds_val\nelif best_idx == 1:\n    final_preds_test = weighted_preds_test\n    final_preds_val = weighted_preds_val\nelse:\n    final_preds_test = stack_preds_test\n    final_preds_val = stack_preds_val\n\n# ============================================================================\n# STEP 8: Threshold Optimization\n# ============================================================================\nlogger.info(\"\\n[8/11] Optimizing prediction threshold on validation set...\")\n\nthresholds = np.arange(0.4, 0.7, 0.05)\nthreshold_results = []\n\nfor threshold in thresholds:\n    y_pred_val = (final_preds_val >= threshold).astype(int)\n\n    if y_pred_val.sum() == 0:\n        continue\n\n    acc = accuracy_score(y_val, y_pred_val)\n    prec = precision_score(y_val, y_pred_val, zero_division=0)\n    rec = recall_score(y_val, y_pred_val, zero_division=0)\n    f1 = f1_score(y_val, y_pred_val, zero_division=0)\n\n    threshold_results.append({\n        'Threshold': threshold,\n        'Accuracy': f\"{acc:.4f}\",\n        'Precision': f\"{prec:.4f}\",\n        'Recall': f\"{rec:.4f}\",\n        'F1': f\"{f1:.4f}\"\n    })\n\nthreshold_df = pd.DataFrame(threshold_results)\nprint(\"\\n\" + tabulate(threshold_df, headers='keys', tablefmt='grid', showindex=False))\n\n# Select threshold with best F1 score\nbest_threshold = float(threshold_df.iloc[threshold_df['F1'].astype(float).idxmax()]['Threshold'])\nlogger.info(f\"\\n  \u2713 Optimal threshold: {best_threshold}\")\n\n# ============================================================================\n# STEP 9: Final Test Set Evaluation\n# ============================================================================\nlogger.info(\"\\n[9/11] Final evaluation on test set...\")\n\ny_pred_test = (final_preds_test >= best_threshold).astype(int)\n\ntest_acc = accuracy_score(y_test, y_pred_test)\ntest_prec = precision_score(y_test, y_pred_test, zero_division=0)\ntest_rec = recall_score(y_test, y_pred_test, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred_test, zero_division=0)\ntest_auc = roc_auc_score(y_test, final_preds_test)\n\ncm = confusion_matrix(y_test, y_pred_test)\ntn, fp, fn, tp = cm.ravel()\n\nlogger.info(f\"\\n  Final Ensemble Performance:\")\nlogger.info(f\"    \u2022 Accuracy:  {test_acc:.4f}\")\nlogger.info(f\"    \u2022 Precision: {test_prec:.4f}\")\nlogger.info(f\"    \u2022 Recall:    {test_rec:.4f}\")\nlogger.info(f\"    \u2022 F1-Score:  {test_f1:.4f}\")\nlogger.info(f\"    \u2022 ROC-AUC:   {test_auc:.4f}\")\n\nlogger.info(f\"\\n  Confusion Matrix:\")\nlogger.info(f\"    TN={tn:5d}  FP={fp:5d}\")\nlogger.info(f\"    FN={fn:5d}  TP={tp:5d}\")\n\n# ============================================================================\n# STEP 10: Save Ensemble Models and Results\n# ============================================================================\nlogger.info(\"\\n[10/11] Saving ensemble models and results...\")\n\n# Save meta-model\nwith open(os.path.join(ENSEMBLE_PATH, 'meta_model.pkl'), 'wb') as f:\n    pickle.dump(meta_model, f)\n\n# Save weights\nensemble_config = {\n    'best_method': best_method,\n    'validation_weights': {col: float(w) for col, w in zip(base_preds_val.columns, val_weights)},\n    'optimal_threshold': float(best_threshold),\n    'base_models': list(base_preds_val.columns)\n}\n\nwith open(os.path.join(ENSEMBLE_PATH, 'ensemble_config.json'), 'w') as f:\n    json.dump(ensemble_config, f, indent=2)\n\n# Comprehensive results\nensemble_results = {\n    'ensemble_info': {\n        'version': 'v2_validation_based_anti_overfitting',\n        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'best_method': best_method,\n        'num_base_models': len(base_preds_val.columns),\n        'optimal_threshold': float(best_threshold)\n    },\n    'ensemble_comparison': ensemble_comparison.to_dict('records'),\n    'test_performance': {\n        'accuracy': float(test_acc),\n        'precision': float(test_prec),\n        'recall': float(test_rec),\n        'f1_score': float(test_f1),\n        'roc_auc': float(test_auc),\n        'confusion_matrix': cm.tolist()\n    },\n    'validation_weights': ensemble_config['validation_weights']\n}\n\nwith open(os.path.join(ENSEMBLE_PATH, 'comprehensive_results.json'), 'w') as f:\n    json.dump(ensemble_results, f, indent=2)\n\nlogger.info(f\"  \u2713 Saved to {ENSEMBLE_PATH}/\")\n\n# ============================================================================\n# STEP 11: Final Assessment\n# ============================================================================\nlogger.info(\"\\n[11/11] Final ensemble assessment...\")\n\n# Check for overfitting\nval_test_gap = abs(float(ensemble_comparison.iloc[best_idx]['Val_AUC']) -\n                   float(ensemble_comparison.iloc[best_idx]['Test_AUC']))\n\nlogger.info(f\"\\n  Overfitting Check:\")\nlogger.info(f\"    \u2022 Val\u2192Test AUC gap: {val_test_gap:.4f}\")\n\nif val_test_gap < 0.02:\n    status = \"EXCELLENT\"\n    assessment = \"Ensemble generalizes extremely well!\"\nelif val_test_gap < 0.04:\n    status = \"GOOD\"\n    assessment = \"Ensemble shows good generalization\"\nelif val_test_gap < 0.06:\n    status = \"MODERATE\"\n    assessment = \"Some overfitting detected, but acceptable\"\nelse:\n    status = \"OVERFIT\"\n    assessment = \"Significant overfitting - use simpler ensemble\"\n\nlogger.info(f\"    \u2022 Status: {status}\")\nlogger.info(f\"    \u2022 Assessment: {assessment}\")\n\nensemble_results['status'] = status\nensemble_results['assessment'] = assessment\n\nwith open(os.path.join(ENSEMBLE_PATH, 'comprehensive_results.json'), 'w') as f:\n    json.dump(ensemble_results, f, indent=2)\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 3 PART 4 (ENSEMBLE METHODS v2) - COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n\ud83d\udcca Key Takeaways:\")\nlogger.info(f\"  \u2022 Best method: {best_method}\")\nlogger.info(f\"  \u2022 Test AUC: {test_auc:.4f}\")\nlogger.info(f\"  \u2022 Val\u2192Test gap: {val_test_gap:.4f}\")\nlogger.info(f\"  \u2022 Status: {status}\")\n\nlogger.info(f\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(f\"  1. \u2713 Global model trained\")\nlogger.info(f\"  2. \u2713 Sector models trained\")\nlogger.info(f\"  3. \u2713 Ensemble created (status: {status})\")\nlogger.info(f\"  4. \u25b6 Run Cell 13: Walk-Forward Validation\")\nlogger.info(f\"  5. \u25b6 Run Cell 14: SHAP Analysis\")\n\nlogger.info(f\"\\n\ud83d\udcc2 Output: {ENSEMBLE_PATH}/\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFvgf2zRoiGS",
    "outputId": "eb6fcc6d-904c-4d8b-f6d2-a2f620d5ce9f"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "ASTRO-FINANCE PROJECT - PHASE 3: ENSEMBLE METHODS v2\n",
      "Phase 3 Progress: Part 4 of 6 (Validation-Based Anti-Overfitting)\n",
      "======================================================================\n",
      "\n",
      "[1/11] Loading prepared data and existing models...\n",
      "  \u2713 Data loaded: Train=(109286, 947), Val=(15731, 947), Test=(9491, 947)\n",
      "\n",
      "[2/11] Loading base models...\n",
      "  \u2713 Loaded global LightGBM model\n",
      "  \u2713 Loaded 5 sector-specific models\n",
      "\n",
      "[3/11] Generating base model predictions...\n",
      "  \u2192 Global LightGBM predictions...\n",
      "  \u2192 Sector-specific predictions...\n",
      "  \u2713 Generated 2 base prediction sets\n",
      "\n",
      "  Base Model Performance (Validation Set):\n",
      "    \u2022 global_lgbm         : Val AUC=0.5845, Test AUC=0.5735, Gap=0.0110\n",
      "    \u2022 sector_specific     : Val AUC=0.5786, Test AUC=0.5756, Gap=0.0031\n",
      "\n",
      "[4/11] Creating simple averaging ensemble...\n",
      "  Simple Average Ensemble:\n",
      "    \u2022 Train AUC: 0.6678\n",
      "    \u2022 Val AUC:   0.5864\n",
      "    \u2022 Test AUC:  0.5755\n",
      "    \u2022 Train\u2192Val gap: 0.0814\n",
      "    \u2022 Val\u2192Test gap:  0.0109\n",
      "\n",
      "[5/11] Creating validation-based weighted ensemble...\n",
      "  Validation-Based Weights:\n",
      "    \u2022 global_lgbm         : 0.518\n",
      "    \u2022 sector_specific     : 0.482\n",
      "\n",
      "  Weighted Ensemble:\n",
      "    \u2022 Train AUC: 0.6693\n",
      "    \u2022 Val AUC:   0.5865\n",
      "    \u2022 Test AUC:  0.5755\n",
      "    \u2022 Train\u2192Val gap: 0.0828\n",
      "    \u2022 Val\u2192Test gap:  0.0110\n",
      "\n",
      "[6/11] Training conservative stacking meta-model...\n",
      "  Training meta-model on validation set...\n",
      "\n",
      "  Stacked Meta-Model:\n",
      "    \u2022 Train AUC: 0.6787\n",
      "    \u2022 Val AUC:   0.5864\n",
      "    \u2022 Test AUC:  0.5750\n",
      "    \u2022 Train\u2192Val gap: 0.0924\n",
      "    \u2022 Val\u2192Test gap:  0.0114\n",
      "\n",
      "  Meta-Model Coefficients:\n",
      "    \u2022 global_lgbm         : +2.653\n",
      "    \u2022 sector_specific     : +1.326\n",
      "\n",
      "[7/11] Comparing ensemble methods...\n",
      "\n",
      "+----------------------+-------------+-----------+------------+----------------+\n",
      "| Method               |   Train_AUC |   Val_AUC |   Test_AUC |   Val_Test_Gap |\n",
      "+======================+=============+===========+============+================+\n",
      "| Simple Average       |      0.6678 |    0.5864 |     0.5755 |         0.0109 |\n",
      "+----------------------+-------------+-----------+------------+----------------+\n",
      "| Weighted (Val-Based) |      0.6693 |    0.5865 |     0.5755 |         0.011  |\n",
      "+----------------------+-------------+-----------+------------+----------------+\n",
      "| Stacked Meta-Model   |      0.6787 |    0.5864 |     0.575  |         0.0114 |\n",
      "+----------------------+-------------+-----------+------------+----------------+\n",
      "\n",
      "  \u2713 Best ensemble method: Simple Average\n",
      "\n",
      "[8/11] Optimizing prediction threshold on validation set...\n",
      "\n",
      "+-------------+------------+-------------+----------+--------+\n",
      "|   Threshold |   Accuracy |   Precision |   Recall |     F1 |\n",
      "+=============+============+=============+==========+========+\n",
      "|        0.4  |     0.5186 |      0.4221 |   0.7381 | 0.5371 |\n",
      "+-------------+------------+-------------+----------+--------+\n",
      "|        0.45 |     0.6084 |      0.4626 |   0.2174 | 0.2958 |\n",
      "+-------------+------------+-------------+----------+--------+\n",
      "|        0.5  |     0.6222 |      0.6216 |   0.0039 | 0.0077 |\n",
      "+-------------+------------+-------------+----------+--------+\n",
      "\n",
      "  \u2713 Optimal threshold: 0.4\n",
      "\n",
      "[9/11] Final evaluation on test set...\n",
      "\n",
      "  Final Ensemble Performance:\n",
      "    \u2022 Accuracy:  0.5512\n",
      "    \u2022 Precision: 0.4459\n",
      "    \u2022 Recall:    0.5460\n",
      "    \u2022 F1-Score:  0.4909\n",
      "    \u2022 ROC-AUC:   0.5755\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN= 3177  FP= 2552\n",
      "    FN= 1708  TP= 2054\n",
      "\n",
      "[10/11] Saving ensemble models and results...\n",
      "  \u2713 Saved to /content/drive/MyDrive/AstroFinanceProject/models/ensemble_improved/\n",
      "\n",
      "[11/11] Final ensemble assessment...\n",
      "\n",
      "  Overfitting Check:\n",
      "    \u2022 Val\u2192Test AUC gap: 0.0109\n",
      "    \u2022 Status: EXCELLENT\n",
      "    \u2022 Assessment: Ensemble generalizes extremely well!\n",
      "\n",
      "======================================================================\n",
      "PHASE 3 PART 4 (ENSEMBLE METHODS v2) - COMPLETE \u2713\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udcca Key Takeaways:\n",
      "  \u2022 Best method: Simple Average\n",
      "  \u2022 Test AUC: 0.5755\n",
      "  \u2022 Val\u2192Test gap: 0.0109\n",
      "  \u2022 Status: EXCELLENT\n",
      "\n",
      "\ud83d\udccb Next Steps:\n",
      "  1. \u2713 Global model trained\n",
      "  2. \u2713 Sector models trained\n",
      "  3. \u2713 Ensemble created (status: EXCELLENT)\n",
      "  4. \u25b6 Run Cell 13: Walk-Forward Validation\n",
      "  5. \u25b6 Run Cell 14: SHAP Analysis\n",
      "\n",
      "\ud83d\udcc2 Output: /content/drive/MyDrive/AstroFinanceProject/models/ensemble_improved/\n",
      "======================================================================\n"
     ]
    }
   ],
   "id": "cell-0015"
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix\n)\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n# Cell 13: Walk-Forward Validation (Phase 3 - Part 5 of 6)\n# ================================================================\n#\n# PURPOSE: Test model robustness across time periods\n# - Simulates real trading: train on past, predict future\n# - Multiple time windows to check stability\n# - Detects if model degrades over time\n# - Validates that performance isn't just luck on one test set\n#\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\nfrom datetime import datetime, timedelta\nfrom tabulate import tabulate\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 3: WALK-FORWARD VALIDATION\")\nlogger.info(\"Phase 3 Progress: Part 5 of 6\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup and Load Data\n# ============================================================================\nlogger.info(\"\\n[1/8] Loading data and models...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nMULTI_TICKER_PATH = os.path.join(BASE_PATH, 'prepared_data', 'multi_ticker')\nMODEL_PATH = os.path.join(BASE_PATH, 'models')\nLGBM_PATH = os.path.join(MODEL_PATH, 'lightgbm_improved')\nENSEMBLE_PATH = os.path.join(MODEL_PATH, 'ensemble_improved')\nWF_PATH = os.path.join(MODEL_PATH, 'walk_forward')\n\nos.makedirs(WF_PATH, exist_ok=True)\n\n# Load full dataset (we'll split it ourselves for walk-forward)\nX_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_train.parquet'))\nX_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_val.parquet'))\nX_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_test.parquet'))\n\ny_train = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_train.parquet'))['target'].values\ny_val = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_val.parquet'))['target'].values\ny_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_test.parquet'))['target'].values\n\n# Keep dates for walk-forward windows\ndates_train = X_train['date']\ndates_val = X_val['date']\ndates_test = X_test['date']\n\n# Combine all data for walk-forward splitting\nX_full = pd.concat([X_train, X_val, X_test], ignore_index=True)\ny_full = np.concatenate([y_train, y_val, y_test])\ndates_full = pd.concat([dates_train, dates_val, dates_test], ignore_index=True)\n\n# Remove date column for modeling\nX_full_no_date = X_full.drop('date', axis=1)\n\nlogger.info(f\"  \u2713 Full dataset: {X_full.shape}\")\nlogger.info(f\"  \u2713 Date range: {dates_full.min()} to {dates_full.max()}\")\n\n# Load trained model\ntry:\n    best_model = lgb.Booster(model_file=os.path.join(LGBM_PATH, 'lightgbm_model.txt'))\n    logger.info(f\"  \u2713 Loaded LightGBM model\")\n    has_model = True\nexcept:\n    logger.info(f\"  \u2298 LightGBM model not found - will skip model-based validation\")\n    has_model = False\n\ncategorical_features = ['ticker_id', 'sector_id', 'region_id']\n\n# ============================================================================\n# STEP 2: Define Walk-Forward Windows\n# ============================================================================\nlogger.info(\"\\n[2/8] Defining walk-forward time windows...\")\n\n# Convert dates to datetime\ndates_dt = pd.to_datetime(dates_full)\n\n# Strategy: Rolling windows with expanding training set\n# - Start with first 60% as initial training\n# - Each window: train on all past data, predict next period\n# - Move forward by 20% each time\n\ndate_min = dates_dt.min()\ndate_max = dates_dt.max()\ntotal_days = (date_max - date_min).days\n\nlogger.info(f\"  Total time span: {total_days} days\")\nlogger.info(f\"  Strategy: Expanding window (train on all past data)\")\n\n# Create 5 validation windows\nnum_windows = 5\nwindow_size_days = total_days // (num_windows + 1)\n\nwindows = []\nfor i in range(num_windows):\n    # Training: from start to cutoff\n    train_cutoff = date_min + timedelta(days=window_size_days * (i + 1))\n\n    # Test: next window\n    test_start = train_cutoff\n    test_end = train_cutoff + timedelta(days=window_size_days)\n\n    # Get indices\n    train_mask = dates_dt < train_cutoff\n    test_mask = (dates_dt >= test_start) & (dates_dt < test_end)\n\n    if train_mask.sum() < 1000 or test_mask.sum() < 100:\n        continue\n\n    windows.append({\n        'window_id': i + 1,\n        'train_cutoff': train_cutoff,\n        'test_start': test_start,\n        'test_end': test_end,\n        'train_samples': int(train_mask.sum()),\n        'test_samples': int(test_mask.sum()),\n        'train_mask': train_mask,\n        'test_mask': test_mask\n    })\n\nlogger.info(f\"\\n  Created {len(windows)} walk-forward windows:\")\nfor w in windows:\n    logger.info(f\"    Window {w['window_id']}: Train={w['train_samples']:,} \u2192 Test={w['test_samples']:,}\")\n    logger.info(f\"      Train: up to {w['train_cutoff'].date()}\")\n    logger.info(f\"      Test: {w['test_start'].date()} to {w['test_end'].date()}\")\n\n# ============================================================================\n# STEP 3: Walk-Forward Validation with Retrained Models\n# ============================================================================\nlogger.info(\"\\n[3/8] Running walk-forward validation (retraining each window)...\")\n\n# Load parameters from best model\nwith open(os.path.join(LGBM_PATH, 'performance_metrics_comprehensive.json'), 'r') as f:\n    best_params = json.load(f)['model_info']['parameters']\n\nwf_results = []\n\nfor window in windows:\n    logger.info(f\"\\n  \u2192 Window {window['window_id']}...\")\n\n    # Extract data\n    X_wf_train = X_full_no_date[window['train_mask']]\n    y_wf_train = y_full[window['train_mask']]\n    X_wf_test = X_full_no_date[window['test_mask']]\n    y_wf_test = y_full[window['test_mask']]\n\n    # Split training into train/val for early stopping\n    train_size = int(len(X_wf_train) * 0.85)\n    X_wf_tr = X_wf_train.iloc[:train_size]\n    y_wf_tr = y_wf_train[:train_size]\n    X_wf_val = X_wf_train.iloc[train_size:]\n    y_wf_val = y_wf_train[train_size:]\n\n    logger.info(f\"     Train: {len(X_wf_tr):,}, Val: {len(X_wf_val):,}, Test: {len(X_wf_test):,}\")\n\n    # Create datasets\n    train_data = lgb.Dataset(X_wf_tr, label=y_wf_tr, categorical_feature=categorical_features)\n    val_data = lgb.Dataset(X_wf_val, label=y_wf_val, categorical_feature=categorical_features, reference=train_data)\n\n    # Train model with early stopping\n    try:\n        evals_result = {}\n        wf_model = lgb.train(\n            best_params,\n            train_data,\n            num_boost_round=2000,\n            valid_sets=[val_data],\n            valid_names=['valid'],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=100, verbose=False),\n                lgb.record_evaluation(evals_result)\n            ]\n        )\n\n        rounds = wf_model.best_iteration\n        val_auc = wf_model.best_score['valid']['auc']\n\n        # Predict on test window\n        y_pred_proba = wf_model.predict(X_wf_test)\n        y_pred = (y_pred_proba >= 0.5).astype(int)\n\n        # Metrics\n        test_auc = roc_auc_score(y_wf_test, y_pred_proba) if len(np.unique(y_wf_test)) > 1 else 0.5\n        test_acc = accuracy_score(y_wf_test, y_pred)\n        test_prec = precision_score(y_wf_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_wf_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_wf_test, y_pred, zero_division=0)\n\n        val_test_gap = abs(val_auc - test_auc)\n\n        logger.info(f\"     Rounds: {rounds}\")\n        logger.info(f\"     Val AUC: {val_auc:.4f}, Test AUC: {test_auc:.4f}\")\n        logger.warning(f\"     Gap: {val_test_gap:.4f} {'\u2713' if val_test_gap < 0.05 else '\u26a0'}\")\n        logger.info(f\"     Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n\n        wf_results.append({\n            'Window': window['window_id'],\n            'Train_Samples': window['train_samples'],\n            'Test_Samples': window['test_samples'],\n            'Test_Period': f\"{window['test_start'].date()} to {window['test_end'].date()}\",\n            'Rounds': rounds,\n            'Val_AUC': float(val_auc),\n            'Test_AUC': float(test_auc),\n            'Test_Acc': float(test_acc),\n            'Test_Prec': float(test_prec),\n            'Test_Rec': float(test_rec),\n            'Test_F1': float(test_f1),\n            'Val_Test_Gap': float(val_test_gap),\n            'Status': 'OK' if val_test_gap < 0.05 else 'OVERFIT'\n        })\n\n    except Exception as e:\n        logger.error(f\"     \u2717 Training failed: {e}\")\n        continue\n\nlogger.info(f\"\\n  \u2713 Completed {len(wf_results)} walk-forward windows\")\n\n# ============================================================================\n# STEP 4: Walk-Forward Results Summary\n# ============================================================================\nlogger.info(\"\\n[4/8] Walk-forward validation results...\")\n\nif wf_results:\n    wf_df = pd.DataFrame(wf_results)\n\n    print(\"\\n\" + tabulate(\n        wf_df[['Window', 'Test_Period', 'Val_AUC', 'Test_AUC', 'Test_Acc', 'Val_Test_Gap', 'Status']],\n        headers='keys',\n        tablefmt='grid',\n        showindex=False,\n        floatfmt=('.0f', 's', '.4f', '.4f', '.4f', '.4f', 's')\n    ))\n\n    # Statistics\n    avg_test_auc = wf_df['Test_AUC'].mean()\n    std_test_auc = wf_df['Test_AUC'].std()\n    avg_gap = wf_df['Val_Test_Gap'].mean()\n    overfit_count = (wf_df['Status'] == 'OVERFIT').sum()\n\n    logger.info(f\"\\n  Overall Statistics:\")\n    logger.info(f\"    \u2022 Average Test AUC: {avg_test_auc:.4f} \u00b1 {std_test_auc:.4f}\")\n    logger.info(f\"    \u2022 Average Val\u2192Test gap: {avg_gap:.4f}\")\n    logger.info(f\"    \u2022 Windows with overfitting: {overfit_count}/{len(wf_df)}\")\n    logger.info(f\"    \u2022 Performance stability: {std_test_auc:.4f} std deviation\")\n\n    # Save results\n    wf_df.to_csv(os.path.join(WF_PATH, 'walk_forward_results.csv'), index=False)\n\nelse:\n    wf_df = pd.DataFrame()\n    logger.info(\"  \u2298 No walk-forward results\")\n\n# ============================================================================\n# STEP 5: Temporal Performance Analysis\n# ============================================================================\nlogger.info(\"\\n[5/8] Analyzing performance over time...\")\n\nif len(wf_df) > 0:\n    # Check for performance degradation\n    first_half_auc = wf_df.iloc[:len(wf_df)//2]['Test_AUC'].mean()\n    second_half_auc = wf_df.iloc[len(wf_df)//2:]['Test_AUC'].mean()\n    degradation = first_half_auc - second_half_auc\n\n    logger.info(f\"\\n  Temporal Stability Check:\")\n    logger.info(f\"    \u2022 First half windows: AUC = {first_half_auc:.4f}\")\n    logger.info(f\"    \u2022 Second half windows: AUC = {second_half_auc:.4f}\")\n    logger.info(f\"    \u2022 Degradation: {degradation:+.4f} ({degradation/first_half_auc*100:+.1f}%)\")\n\n    if abs(degradation) < 0.02:\n        temporal_status = \"STABLE\"\n        logger.info(f\"    \u2713 {temporal_status} - Performance consistent over time\")\n    elif degradation > 0.02:\n        temporal_status = \"DEGRADING\"\n        logger.warning(f\"    \u26a0 {temporal_status} - Performance declining in recent periods\")\n    else:\n        temporal_status = \"IMPROVING\"\n        logger.info(f\"    \u2713 {temporal_status} - Performance improving over time\")\n\n# ============================================================================\n# STEP 6: Comparison with Original Test Set\n# ============================================================================\nlogger.info(\"\\n[6/8] Comparing walk-forward with original test set...\")\n\nif has_model and len(wf_df) > 0:\n    # Load original test performance\n    with open(os.path.join(LGBM_PATH, 'performance_metrics_comprehensive.json'), 'r') as f:\n        original_perf = json.load(f)\n\n    original_test_auc = original_perf['test_performance']['roc_auc']\n    wf_avg_auc = wf_df['Test_AUC'].mean()\n\n    logger.info(f\"\\n  Performance Comparison:\")\n    logger.info(f\"    \u2022 Original test set AUC: {original_test_auc:.4f}\")\n    logger.info(f\"    \u2022 Walk-forward avg AUC: {wf_avg_auc:.4f}\")\n    logger.info(f\"    \u2022 Difference: {wf_avg_auc - original_test_auc:+.4f}\")\n\n    if abs(wf_avg_auc - original_test_auc) < 0.03:\n        consistency = \"CONSISTENT\"\n        logger.info(f\"    \u2713 {consistency} - Results align with original validation\")\n    else:\n        consistency = \"INCONSISTENT\"\n        logger.warning(f\"    \u26a0 {consistency} - Walk-forward differs from original test\")\n\n# ============================================================================\n# STEP 7: Visualizations\n# ============================================================================\nlogger.info(\"\\n[7/8] Creating visualizations...\")\n\nif len(wf_df) > 0:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle('Walk-Forward Validation Analysis', fontsize=16, fontweight='bold')\n\n    # Plot 1: AUC over windows\n    ax1 = axes[0, 0]\n    ax1.plot(wf_df['Window'], wf_df['Val_AUC'], marker='o', label='Val AUC', linewidth=2)\n    ax1.plot(wf_df['Window'], wf_df['Test_AUC'], marker='s', label='Test AUC', linewidth=2)\n    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n    ax1.set_xlabel('Window')\n    ax1.set_ylabel('AUC')\n    ax1.set_title('Performance Across Time Windows')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Plot 2: Val-Test gap\n    ax2 = axes[0, 1]\n    colors = ['green' if s == 'OK' else 'red' for s in wf_df['Status']]\n    ax2.bar(wf_df['Window'], wf_df['Val_Test_Gap'], color=colors, alpha=0.7)\n    ax2.axhline(y=0.05, color='orange', linestyle='--', label='Threshold (0.05)')\n    ax2.set_xlabel('Window')\n    ax2.set_ylabel('Val\u2192Test Gap')\n    ax2.set_title('Overfitting Check by Window')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # Plot 3: Metrics comparison\n    ax3 = axes[1, 0]\n    metrics = ['Test_Acc', 'Test_Prec', 'Test_Rec', 'Test_F1']\n    metric_avgs = [wf_df[m].mean() for m in metrics]\n    ax3.bar(['Accuracy', 'Precision', 'Recall', 'F1'], metric_avgs, color='steelblue', alpha=0.7)\n    ax3.set_ylabel('Score')\n    ax3.set_title('Average Performance Metrics')\n    ax3.set_ylim(0, 1)\n    ax3.grid(True, alpha=0.3, axis='y')\n\n    # Plot 4: Performance stability\n    ax4 = axes[1, 1]\n    ax4.boxplot([wf_df['Val_AUC'], wf_df['Test_AUC']], labels=['Val AUC', 'Test AUC'])\n    ax4.set_ylabel('AUC')\n    ax4.set_title('Performance Distribution')\n    ax4.grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(WF_PATH, 'walk_forward_analysis.png'), dpi=300, bbox_inches='tight')\n    plt.close()\n\n    logger.info(f\"  \u2713 Saved visualization: walk_forward_analysis.png\")\n\n# ============================================================================\n# STEP 8: Save Comprehensive Results\n# ============================================================================\nlogger.info(\"\\n[8/8] Saving comprehensive walk-forward results...\")\n\nwf_comprehensive = {\n    'validation_info': {\n        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'num_windows': len(wf_df) if len(wf_df) > 0 else 0,\n        'strategy': 'expanding_window',\n        'retrain_each_window': True\n    },\n    'window_results': wf_df.to_dict('records') if len(wf_df) > 0 else [],\n    'overall_statistics': {\n        'avg_test_auc': float(wf_df['Test_AUC'].mean()) if len(wf_df) > 0 else 0,\n        'std_test_auc': float(wf_df['Test_AUC'].std()) if len(wf_df) > 0 else 0,\n        'avg_val_test_gap': float(wf_df['Val_Test_Gap'].mean()) if len(wf_df) > 0 else 0,\n        'overfit_windows': int((wf_df['Status'] == 'OVERFIT').sum()) if len(wf_df) > 0 else 0,\n        'temporal_status': temporal_status if len(wf_df) > 0 else 'N/A'\n    }\n}\n\nwith open(os.path.join(WF_PATH, 'walk_forward_comprehensive.json'), 'w') as f:\n    json.dump(wf_comprehensive, f, indent=2)\n\nlogger.info(f\"  \u2713 Saved to {WF_PATH}/\")\n\n# ============================================================================\n# FINAL ASSESSMENT\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"WALK-FORWARD VALIDATION ASSESSMENT\")\nlogger.info(\"=\" * 70)\n\nif len(wf_df) > 0:\n    avg_auc = wf_df['Test_AUC'].mean()\n    std_auc = wf_df['Test_AUC'].std()\n    avg_gap = wf_df['Val_Test_Gap'].mean()\n\n    if avg_auc > 0.60 and std_auc < 0.05 and avg_gap < 0.04:\n        status = \"EXCELLENT\"\n        assessment = \"\u2705 Model is robust across time periods\"\n    elif avg_auc > 0.55 and std_auc < 0.08 and avg_gap < 0.06:\n        status = \"GOOD\"\n        assessment = \"\u2705 Model shows reasonable stability\"\n    elif avg_auc > 0.52:\n        status = \"MODERATE\"\n        assessment = \"\u26a0 Some instability detected\"\n    else:\n        status = \"WEAK\"\n        assessment = \"\u26a0 Model not robust across time\"\n\n    logger.info(f\"\\n  Status: {status}\")\n    logger.info(f\"  {assessment}\")\n    logger.info(f\"\\n  Key Metrics:\")\n    logger.info(f\"    \u2022 Avg AUC: {avg_auc:.4f} \u00b1 {std_auc:.4f}\")\n    logger.info(f\"    \u2022 Avg Val\u2192Test gap: {avg_gap:.4f}\")\n    logger.info(f\"    \u2022 Temporal: {temporal_status}\")\nelse:\n    status = \"INCOMPLETE\"\n    logger.info(f\"\\n  Status: {status}\")\n    logger.info(f\"  No walk-forward results available\")\n\nwf_comprehensive['status'] = status\nwf_comprehensive['assessment'] = assessment if len(wf_df) > 0 else \"N/A\"\n\nwith open(os.path.join(WF_PATH, 'walk_forward_comprehensive.json'), 'w') as f:\n    json.dump(wf_comprehensive, f, indent=2)\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 3 PART 5 (WALK-FORWARD VALIDATION) - COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n\ud83d\udccb Next Steps:\")\nlogger.info(f\"  1. \u2713 Data prepared\")\nlogger.info(f\"  2. \u2713 Global model trained\")\nlogger.info(f\"  3. \u2713 Sector models trained\")\nlogger.info(f\"  4. \u2713 Ensemble created\")\nlogger.info(f\"  5. \u2713 Walk-forward validation (status: {status})\")\nlogger.info(f\"  6. \u25b6 Run Cell 14: SHAP Analysis\")\n\nlogger.info(f\"\\n\ud83d\udcc2 Output: {WF_PATH}/\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPjfeo5ELZ2u",
    "outputId": "3c0bb9f8-06b5-4b37-ed63-091ae3d1915d"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "ASTRO-FINANCE PROJECT - PHASE 3: WALK-FORWARD VALIDATION\n",
      "Phase 3 Progress: Part 5 of 6\n",
      "======================================================================\n",
      "\n",
      "[1/8] Loading data and models...\n",
      "  \u2713 Full dataset: (134508, 948)\n",
      "  \u2713 Date range: 2000-01-03 00:00:00 to 2025-10-22 00:00:00\n",
      "  \u2713 Loaded LightGBM model\n",
      "\n",
      "[2/8] Defining walk-forward time windows...\n",
      "  Total time span: 9424 days\n",
      "  Strategy: Expanding window (train on all past data)\n",
      "\n",
      "  Created 5 walk-forward windows:\n",
      "    Window 1: Train=21,530 \u2192 Test=22,659\n",
      "      Train: up to 2004-04-21\n",
      "      Test: 2004-04-21 to 2008-08-08\n",
      "    Window 2: Train=44,189 \u2192 Test=22,571\n",
      "      Train: up to 2008-08-08\n",
      "      Test: 2008-08-08 to 2012-11-25\n",
      "    Window 3: Train=66,760 \u2192 Test=22,530\n",
      "      Train: up to 2012-11-25\n",
      "      Test: 2012-11-25 to 2017-03-14\n",
      "    Window 4: Train=89,290 \u2192 Test=22,583\n",
      "      Train: up to 2017-03-14\n",
      "      Test: 2017-03-14 to 2021-07-01\n",
      "    Window 5: Train=111,873 \u2192 Test=22,576\n",
      "      Train: up to 2021-07-01\n",
      "      Test: 2021-07-01 to 2025-10-18\n",
      "\n",
      "[3/8] Running walk-forward validation (retraining each window)...\n",
      "\n",
      "  \u2192 Window 1...\n",
      "     Train: 18,300, Val: 3,230, Test: 22,659\n",
      "     Rounds: 1\n",
      "     Val AUC: 0.6045, Test AUC: 0.5050\n",
      "     Gap: 0.0995 \u26a0\n",
      "     Accuracy: 0.5989, F1: 0.0000\n",
      "\n",
      "  \u2192 Window 2...\n",
      "     Train: 37,560, Val: 6,629, Test: 22,571\n",
      "     Rounds: 16\n",
      "     Val AUC: 0.6457, Test AUC: 0.5391\n",
      "     Gap: 0.1066 \u26a0\n",
      "     Accuracy: 0.5951, F1: 0.0000\n",
      "\n",
      "  \u2192 Window 3...\n",
      "     Train: 56,746, Val: 10,014, Test: 22,530\n",
      "     Rounds: 13\n",
      "     Val AUC: 0.6485, Test AUC: 0.5676\n",
      "     Gap: 0.0809 \u26a0\n",
      "     Accuracy: 0.6297, F1: 0.0000\n",
      "\n",
      "  \u2192 Window 4...\n",
      "     Train: 75,896, Val: 13,394, Test: 22,583\n",
      "     Rounds: 39\n",
      "     Val AUC: 0.6561, Test AUC: 0.5959\n",
      "     Gap: 0.0602 \u26a0\n",
      "     Accuracy: 0.6185, F1: 0.0000\n",
      "\n",
      "  \u2192 Window 5...\n",
      "     Train: 95,092, Val: 16,781, Test: 22,576\n",
      "     Rounds: 117\n",
      "     Val AUC: 0.6688, Test AUC: 0.5665\n",
      "     Gap: 0.1023 \u26a0\n",
      "     Accuracy: 0.6155, F1: 0.0886\n",
      "\n",
      "  \u2713 Completed 5 walk-forward windows\n",
      "\n",
      "[4/8] Walk-forward validation results...\n",
      "\n",
      "+----------+--------------------------+-----------+------------+------------+----------------+----------+\n",
      "|   Window | Test_Period              |   Val_AUC |   Test_AUC |   Test_Acc |   Val_Test_Gap | Status   |\n",
      "+==========+==========================+===========+============+============+================+==========+\n",
      "|        1 | 2004-04-21 to 2008-08-08 |    0.6045 |     0.5050 |     0.5989 |         0.0995 | OVERFIT  |\n",
      "+----------+--------------------------+-----------+------------+------------+----------------+----------+\n",
      "|        2 | 2008-08-08 to 2012-11-25 |    0.6457 |     0.5391 |     0.5951 |         0.1066 | OVERFIT  |\n",
      "+----------+--------------------------+-----------+------------+------------+----------------+----------+\n",
      "|        3 | 2012-11-25 to 2017-03-14 |    0.6485 |     0.5676 |     0.6297 |         0.0809 | OVERFIT  |\n",
      "+----------+--------------------------+-----------+------------+------------+----------------+----------+\n",
      "|        4 | 2017-03-14 to 2021-07-01 |    0.6561 |     0.5959 |     0.6185 |         0.0602 | OVERFIT  |\n",
      "+----------+--------------------------+-----------+------------+------------+----------------+----------+\n",
      "|        5 | 2021-07-01 to 2025-10-18 |    0.6688 |     0.5665 |     0.6155 |         0.1023 | OVERFIT  |\n",
      "+----------+--------------------------+-----------+------------+------------+----------------+----------+\n",
      "\n",
      "  Overall Statistics:\n",
      "    \u2022 Average Test AUC: 0.5548 \u00b1 0.0344\n",
      "    \u2022 Average Val\u2192Test gap: 0.0899\n",
      "    \u2022 Windows with overfitting: 5/5\n",
      "    \u2022 Performance stability: 0.0344 std deviation\n",
      "\n",
      "[5/8] Analyzing performance over time...\n",
      "\n",
      "  Temporal Stability Check:\n",
      "    \u2022 First half windows: AUC = 0.5220\n",
      "    \u2022 Second half windows: AUC = 0.5767\n",
      "    \u2022 Degradation: -0.0547 (-10.5%)\n",
      "    \u2713 IMPROVING - Performance improving over time\n",
      "\n",
      "[6/8] Comparing walk-forward with original test set...\n",
      "\n",
      "  Performance Comparison:\n",
      "    \u2022 Original test set AUC: 0.5735\n",
      "    \u2022 Walk-forward avg AUC: 0.5548\n",
      "    \u2022 Difference: -0.0187\n",
      "    \u2713 CONSISTENT - Results align with original validation\n",
      "\n",
      "[7/8] Creating visualizations...\n",
      "  \u2713 Saved visualization: walk_forward_analysis.png\n",
      "\n",
      "[8/8] Saving comprehensive walk-forward results...\n",
      "  \u2713 Saved to /content/drive/MyDrive/AstroFinanceProject/models/walk_forward/\n",
      "\n",
      "======================================================================\n",
      "WALK-FORWARD VALIDATION ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "  Status: MODERATE\n",
      "  \u26a0 Some instability detected\n",
      "\n",
      "  Key Metrics:\n",
      "    \u2022 Avg AUC: 0.5548 \u00b1 0.0344\n",
      "    \u2022 Avg Val\u2192Test gap: 0.0899\n",
      "    \u2022 Temporal: IMPROVING\n",
      "\n",
      "======================================================================\n",
      "PHASE 3 PART 5 (WALK-FORWARD VALIDATION) - COMPLETE \u2713\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udccb Next Steps:\n",
      "  1. \u2713 Data prepared\n",
      "  2. \u2713 Global model trained\n",
      "  3. \u2713 Sector models trained\n",
      "  4. \u2713 Ensemble created\n",
      "  5. \u2713 Walk-forward validation (status: MODERATE)\n",
      "  6. \u25b6 Run Cell 14: SHAP Analysis\n",
      "\n",
      "\ud83d\udcc2 Output: /content/drive/MyDrive/AstroFinanceProject/models/walk_forward/\n",
      "======================================================================\n"
     ]
    }
   ],
   "id": "cell-0016"
  },
  {
   "cell_type": "code",
   "source": "# Cell 14: SHAP Analysis (Phase 3 - Part 6 of 6)\n# ================================================================\n#\n# PURPOSE: Model interpretability and feature importance\n# - Understand what drives predictions\n# - Identify most influential astrological features\n# - Validate model is using sensible patterns\n# - Detect potential data leakage or spurious correlations\n#\n# ================================================================\n\nimport logging\nimport sys\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\nfrom datetime import datetime\nfrom tabulate import tabulate\nimport lightgbm as lgb\nimport shap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Configure logging with timestamps and levels\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(levelname)-8s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nlogger.info(\"=\" * 70)\nlogger.info(\"ASTRO-FINANCE PROJECT - PHASE 3: SHAP ANALYSIS\")\nlogger.info(\"Phase 3 Progress: Part 6 of 6 (FINAL)\")\nlogger.info(\"=\" * 70)\n\n# ============================================================================\n# STEP 1: Setup and Load Data\n# ============================================================================\nlogger.info(\"\\n[1/7] Loading data and models...\")\n\nBASE_PATH = '/content/drive/MyDrive/AstroFinanceProject'\nMULTI_TICKER_PATH = os.path.join(BASE_PATH, 'prepared_data', 'multi_ticker')\nMODEL_PATH = os.path.join(BASE_PATH, 'models')\nLGBM_PATH = os.path.join(MODEL_PATH, 'lightgbm_improved')\nSHAP_PATH = os.path.join(MODEL_PATH, 'shap_analysis')\n\nos.makedirs(SHAP_PATH, exist_ok=True)\n\n# Load test data (for SHAP analysis)\nX_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'X_test.parquet'))\ny_test = pd.read_parquet(os.path.join(MULTI_TICKER_PATH, 'y_test.parquet'))['target'].values\n\ndates_test = X_test['date']\nX_test = X_test.drop('date', axis=1)\n\nlogger.info(f\"  \u2713 Test data: {X_test.shape}\")\n\n# Load model\ntry:\n    model = lgb.Booster(model_file=os.path.join(LGBM_PATH, 'lightgbm_model.txt'))\n    logger.info(f\"  \u2713 Loaded LightGBM model\")\nexcept:\n    logger.error(f\"  \u2717 Model not found. Please run Cell 10 first.\")\n    raise FileNotFoundError(\"LightGBM model required for SHAP analysis\")\n\n# Load feature importance\nimportance_df = pd.read_csv(os.path.join(LGBM_PATH, 'feature_importance_detailed.csv'))\n\n# ============================================================================\n# STEP 2: Sample Selection for SHAP (Computational Efficiency)\n# ============================================================================\nlogger.info(\"\\n[2/7] Selecting representative samples for SHAP analysis...\")\n\n# SHAP is computationally expensive - use subset\n# Strategy: Sample from different prediction ranges\ny_pred_proba = model.predict(X_test)\n\n# Create bins: low, medium, high confidence predictions\nbins = [0, 0.4, 0.6, 1.0]\nbin_labels = ['low_conf', 'medium_conf', 'high_conf']\nprediction_bins = pd.cut(y_pred_proba, bins=bins, labels=bin_labels)\n\n# Sample from each bin\nsamples_per_bin = 100\nsample_indices = []\n\nfor bin_label in bin_labels:\n    bin_mask = prediction_bins == bin_label\n    bin_indices = np.where(bin_mask)[0]\n\n    if len(bin_indices) > 0:\n        n_samples = min(samples_per_bin, len(bin_indices))\n        sampled = np.random.choice(bin_indices, size=n_samples, replace=False)\n        sample_indices.extend(sampled)\n\nsample_indices = np.array(sample_indices)  # Convert to numpy array\nX_shap = X_test.iloc[sample_indices]\ny_shap = y_test[sample_indices]\n\n# FIX: Use the array directly instead of iloc on Categorical\nprediction_bins_sampled = prediction_bins[sample_indices]\n\nlogger.info(f\"  \u2713 Selected {len(X_shap)} samples for SHAP\")\nlogger.info(f\"    \u2022 Low confidence: {(prediction_bins_sampled == 'low_conf').sum()}\")\nlogger.info(f\"    \u2022 Medium confidence: {(prediction_bins_sampled == 'medium_conf').sum()}\")\nlogger.info(f\"    \u2022 High confidence: {(prediction_bins_sampled == 'high_conf').sum()}\")\n\n# ============================================================================\n# STEP 3: Compute SHAP Values\n# ============================================================================\nlogger.info(\"\\n[3/7] Computing SHAP values...\")\nlogger.info(\"  (This may take a few minutes...)\")\n\n# Create SHAP explainer\nexplainer = shap.TreeExplainer(model)\n\n# Compute SHAP values\nshap_values = explainer.shap_values(X_shap)\n\n# For binary classification, SHAP returns values for positive class\nif isinstance(shap_values, list):\n    shap_values = shap_values[1]  # Get positive class SHAP values\n\nlogger.info(f\"  \u2713 SHAP values computed: {shap_values.shape}\")\n\n# ============================================================================\n# STEP 4: Global Feature Importance (SHAP-based)\n# ============================================================================\nlogger.info(\"\\n[4/7] Analyzing global feature importance...\")\n\n# Calculate mean absolute SHAP value for each feature\nshap_importance = np.abs(shap_values).mean(axis=0)\n\nshap_importance_df = pd.DataFrame({\n    'feature': X_shap.columns,\n    'shap_importance': shap_importance\n}).sort_values('shap_importance', ascending=False)\n\nshap_importance_df['shap_importance_pct'] = shap_importance_df['shap_importance'] / shap_importance_df['shap_importance'].sum() * 100\n\n# Categorize features\ndef categorize_feature(feat_name):\n    if feat_name in ['ticker_id', 'sector_id', 'region_id']:\n        return 'Categorical'\n    elif any(x in feat_name for x in ['sun_', 'moon_', 'mercury_', 'venus_', 'mars_', 'jupiter_', 'saturn_']):\n        return 'Planetary'\n    elif any(x in feat_name for x in ['aspect_', 'conjunction', 'opposition', 'trine', 'square']):\n        return 'Aspects'\n    elif any(x in feat_name for x in ['moon_phase', 'mercury_retrograde', 'day_of_week', 'month']):\n        return 'Temporal'\n    elif any(x in feat_name for x in ['rsi', 'sma', 'bb_', 'atr', 'volume_ratio', 'returns_']):\n        return 'Technical'\n    else:\n        return 'Other'\n\nshap_importance_df['category'] = shap_importance_df['feature'].apply(categorize_feature)\n\nlogger.info(f\"\\n  Top 20 Features by SHAP Importance:\")\nfor idx, row in shap_importance_df.head(20).iterrows():\n    logger.info(f\"    {row['feature']:35s}: {row['shap_importance_pct']:5.2f}% [{row['category']}]\")\n\n# Category-wise SHAP importance\ncategory_shap = shap_importance_df.groupby('category')['shap_importance_pct'].sum().sort_values(ascending=False)\nlogger.info(f\"\\n  SHAP Importance by Category:\")\nfor cat, pct in category_shap.items():\n    logger.info(f\"    \u2022 {cat:15s}: {pct:5.1f}%\")\n\n# Save\nshap_importance_df.to_csv(os.path.join(SHAP_PATH, 'shap_feature_importance.csv'), index=False)\n\n# ============================================================================\n# STEP 5: Compare SHAP vs Gain Importance\n# ============================================================================\nlogger.info(\"\\n[5/7] Comparing SHAP importance with Gain importance...\")\n\n# Merge SHAP and Gain importance\ncomparison_df = shap_importance_df[['feature', 'shap_importance_pct', 'category']].merge(\n    importance_df[['feature', 'importance_pct']],\n    on='feature',\n    how='left'\n)\n\ncomparison_df.columns = ['feature', 'SHAP_%', 'category', 'Gain_%']\ncomparison_df['Gain_%'] = comparison_df['Gain_%'].fillna(0)\n\n# Calculate agreement (correlation)\ncorrelation = comparison_df[['SHAP_%', 'Gain_%']].corr().iloc[0, 1]\nlogger.info(f\"\\n  Correlation between SHAP and Gain: {correlation:.3f}\")\n\nif correlation > 0.7:\n    agreement = \"HIGH\"\n    logger.info(f\"  \u2713 {agreement} agreement - Both methods identify similar features\")\nelif correlation > 0.5:\n    agreement = \"MODERATE\"\n    logger.warning(f\"  \u26a0 {agreement} agreement - Some differences in feature rankings\")\nelse:\n    agreement = \"LOW\"\n    logger.warning(f\"  \u26a0 {agreement} agreement - Methods disagree on important features\")\n\n# Top features that differ\ncomparison_df['rank_diff'] = abs(\n    comparison_df['SHAP_%'].rank(ascending=False) -\n    comparison_df['Gain_%'].rank(ascending=False)\n)\ndisagreement_features = comparison_df.nlargest(10, 'rank_diff')\n\nlogger.info(f\"\\n  Top 10 Features with Largest Ranking Disagreement:\")\nfor idx, row in disagreement_features.iterrows():\n    logger.info(f\"    {row['feature']:30s}: SHAP={row['SHAP_%']:5.2f}%, Gain={row['Gain_%']:5.2f}%\")\n\ncomparison_df.to_csv(os.path.join(SHAP_PATH, 'importance_comparison.csv'), index=False)\n\n# ============================================================================\n# STEP 6: Visualizations\n# ============================================================================\nlogger.info(\"\\n[6/7] Creating SHAP visualizations...\")\n\n# === Plot 1: Summary Plot ===\nlogger.info(\"  Creating summary plot...\")\nplt.figure(figsize=(12, 10))\nshap.summary_plot(shap_values, X_shap, show=False, max_display=20)\nplt.title('SHAP Summary Plot - Top 20 Features', fontsize=14, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.savefig(os.path.join(SHAP_PATH, 'shap_summary_plot.png'), dpi=300, bbox_inches='tight')\nplt.close()\nlogger.info(f\"  \u2713 Saved: shap_summary_plot.png\")\n\n# === Plot 2: Bar Plot (Mean Absolute SHAP) ===\nlogger.info(\"  Creating bar plot...\")\nplt.figure(figsize=(12, 8))\nshap.summary_plot(shap_values, X_shap, plot_type='bar', show=False, max_display=20)\nplt.title('SHAP Feature Importance - Top 20', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(os.path.join(SHAP_PATH, 'shap_bar_plot.png'), dpi=300, bbox_inches='tight')\nplt.close()\nlogger.info(f\"  \u2713 Saved: shap_bar_plot.png\")\n\n# === Plot 3: Category Comparison ===\nlogger.info(\"  Creating category comparison...\")\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# SHAP by category\naxes[0].barh(category_shap.index, category_shap.values, color='steelblue', alpha=0.7)\naxes[0].set_xlabel('SHAP Importance (%)')\naxes[0].set_title('SHAP Importance by Feature Category')\naxes[0].grid(True, alpha=0.3, axis='x')\n\n# Gain by category\ncategory_gain = importance_df.groupby(importance_df['feature'].apply(categorize_feature))['importance_pct'].sum().sort_values(ascending=False)\naxes[1].barh(category_gain.index, category_gain.values, color='coral', alpha=0.7)\naxes[1].set_xlabel('Gain Importance (%)')\naxes[1].set_title('Gain Importance by Feature Category')\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.savefig(os.path.join(SHAP_PATH, 'category_comparison.png'), dpi=300, bbox_inches='tight')\nplt.close()\nlogger.info(f\"  \u2713 Saved: category_comparison.png\")\n\n# === Plot 4: Dependence Plots for Top 3 Features ===\nlogger.info(\"  Creating dependence plots for top features...\")\ntop_3_features = shap_importance_df.head(3)['feature'].tolist()\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, feature in enumerate(top_3_features):\n    feature_idx = X_shap.columns.get_loc(feature)\n    shap.dependence_plot(feature_idx, shap_values, X_shap, ax=axes[idx], show=False)\n    axes[idx].set_title(f'{feature}', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(os.path.join(SHAP_PATH, 'top3_dependence_plots.png'), dpi=300, bbox_inches='tight')\nplt.close()\nlogger.info(f\"  \u2713 Saved: top3_dependence_plots.png\")\n\n# ============================================================================\n# STEP 7: Save Comprehensive SHAP Results\n# ============================================================================\nlogger.info(\"\\n[7/7] Saving comprehensive SHAP analysis...\")\n\nshap_comprehensive = {\n    'analysis_info': {\n        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'samples_analyzed': len(X_shap),\n        'total_features': len(X_shap.columns),\n        'shap_gain_correlation': float(correlation)\n    },\n    'top_20_features_shap': shap_importance_df.head(20).to_dict('records'),\n    'category_importance_shap': category_shap.to_dict(),\n    'category_importance_gain': category_gain.to_dict(),\n    'importance_agreement': agreement,\n    'key_insights': {\n        'most_important_feature': shap_importance_df.iloc[0]['feature'],\n        'most_important_category': category_shap.index[0],\n        'astrological_importance': float(category_shap.get('Planetary', 0) + category_shap.get('Aspects', 0)),\n        'technical_importance': float(category_shap.get('Technical', 0))\n    }\n}\n\nwith open(os.path.join(SHAP_PATH, 'shap_comprehensive.json'), 'w') as f:\n    json.dump(shap_comprehensive, f, indent=2)\n\n# Save SHAP values for future use\nnp.save(os.path.join(SHAP_PATH, 'shap_values.npy'), shap_values)\nX_shap.to_parquet(os.path.join(SHAP_PATH, 'X_shap_samples.parquet'))\n\nlogger.info(f\"  \u2713 Saved all SHAP results to {SHAP_PATH}/\")\n\n# ============================================================================\n# FINAL ASSESSMENT\n# ============================================================================\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"SHAP ANALYSIS ASSESSMENT\")\nlogger.info(\"=\" * 70)\n\nastro_importance = float(category_shap.get('Planetary', 0) + category_shap.get('Aspects', 0))\ntech_importance = float(category_shap.get('Technical', 0))\n\nlogger.info(f\"\\n  Key Findings:\")\nlogger.info(f\"    \u2022 Most important feature: {shap_importance_df.iloc[0]['feature']}\")\nlogger.info(f\"    \u2022 Most important category: {category_shap.index[0]}\")\nlogger.info(f\"    \u2022 Astrological features: {astro_importance:.1f}% importance\")\nlogger.info(f\"    \u2022 Technical features: {tech_importance:.1f}% importance\")\nlogger.info(f\"    \u2022 SHAP-Gain correlation: {correlation:.3f} ({agreement} agreement)\")\n\nif astro_importance > 20:\n    astro_status = \"SIGNIFICANT\"\n    logger.info(f\"\\n  \u2705 {astro_status} - Astrological features contribute meaningfully\")\nelif astro_importance > 10:\n    astro_status = \"MODERATE\"\n    logger.warning(f\"\\n  \u26a0 {astro_status} - Astrological features have some influence\")\nelse:\n    astro_status = \"MINIMAL\"\n    logger.warning(f\"\\n  \u26a0 {astro_status} - Astrological features contribute little\")\n\nshap_comprehensive['astro_status'] = astro_status\n\nwith open(os.path.join(SHAP_PATH, 'shap_comprehensive.json'), 'w') as f:\n    json.dump(shap_comprehensive, f, indent=2)\n\nlogger.info(\"\\n\" + \"=\" * 70)\nlogger.info(\"PHASE 3 PART 6 (SHAP ANALYSIS) - COMPLETE \u2713\")\nlogger.info(\"=\" * 70)\n\nlogger.info(f\"\\n\ud83c\udf89 PHASE 3 COMPLETE!\")\nlogger.info(f\"\\n\ud83d\udccb All Cells Completed:\")\nlogger.info(f\"  1. \u2713 Data preparation\")\nlogger.info(f\"  2. \u2713 Global model training\")\nlogger.info(f\"  3. \u2713 Sector-specific models\")\nlogger.info(f\"  4. \u2713 Ensemble methods\")\nlogger.info(f\"  5. \u2713 Walk-forward validation\")\nlogger.info(f\"  6. \u2713 SHAP analysis\")\n\nlogger.info(f\"\\n\ud83d\udcc2 Outputs:\")\nlogger.info(f\"  \u2022 {LGBM_PATH}/\")\nlogger.info(f\"  \u2022 {SHAP_PATH}/\")\n\nlogger.info(f\"\\n\ud83d\udcca Model Interpretability Status: {astro_status}\")\nlogger.info(\"=\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmGwADE8UINr",
    "outputId": "f07558f6-791d-4f3e-c306-84c9acae2423"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "ASTRO-FINANCE PROJECT - PHASE 3: SHAP ANALYSIS\n",
      "Phase 3 Progress: Part 6 of 6 (FINAL)\n",
      "======================================================================\n",
      "\n",
      "[1/7] Loading data and models...\n",
      "  \u2713 Test data: (9491, 947)\n",
      "  \u2713 Loaded LightGBM model\n",
      "\n",
      "[2/7] Selecting representative samples for SHAP analysis...\n",
      "  \u2713 Selected 200 samples for SHAP\n",
      "    \u2022 Low confidence: 100\n",
      "    \u2022 Medium confidence: 100\n",
      "    \u2022 High confidence: 0\n",
      "\n",
      "[3/7] Computing SHAP values...\n",
      "  (This may take a few minutes...)\n",
      "  \u2713 SHAP values computed: (200, 947)\n",
      "\n",
      "[4/7] Analyzing global feature importance...\n",
      "\n",
      "  Top 20 Features by SHAP Importance:\n",
      "    volatility_20d                     : 13.78% [Other]\n",
      "    ticker_id                          :  8.54% [Categorical]\n",
      "    returns_5d                         :  5.04% [Technical]\n",
      "    jupiter_saturn_midpoint            :  2.79% [Planetary]\n",
      "    sector_id                          :  2.52% [Categorical]\n",
      "    volume_ratio                       :  2.50% [Technical]\n",
      "    saturn_longitude                   :  2.09% [Planetary]\n",
      "    day_of_month                       :  1.91% [Temporal]\n",
      "    mars_saturn_square_exact_dist      :  1.90% [Planetary]\n",
      "    returns_20d                        :  1.86% [Technical]\n",
      "    bb_position                        :  1.79% [Technical]\n",
      "    mars_dignity_score                 :  1.59% [Planetary]\n",
      "    jupiter_longitude                  :  1.57% [Planetary]\n",
      "    sma_50                             :  1.53% [Technical]\n",
      "    moon_venus_conjunction_applying    :  1.44% [Planetary]\n",
      "    sma_20                             :  1.31% [Technical]\n",
      "    rahu_longitude                     :  1.24% [Other]\n",
      "    rahu_sign                          :  1.15% [Other]\n",
      "    venus_mars_trine_strength          :  1.12% [Planetary]\n",
      "    venus_rahu_sextile_exact_dist      :  1.02% [Planetary]\n",
      "\n",
      "  SHAP Importance by Category:\n",
      "    \u2022 Planetary      :  53.7%\n",
      "    \u2022 Other          :  17.1%\n",
      "    \u2022 Technical      :  15.4%\n",
      "    \u2022 Categorical    :  11.7%\n",
      "    \u2022 Temporal       :   2.0%\n",
      "    \u2022 Aspects        :   0.0%\n",
      "\n",
      "[5/7] Comparing SHAP importance with Gain importance...\n",
      "\n",
      "  Correlation between SHAP and Gain: 0.967\n",
      "  \u2713 HIGH agreement - Both methods identify similar features\n",
      "\n",
      "  Top 10 Features with Largest Ranking Disagreement:\n",
      "    moon_mars_opposition_exact_dist: SHAP= 0.60%, Gain= 0.04%\n",
      "    moon_phase_category           : SHAP= 0.11%, Gain= 0.03%\n",
      "    moon_strength_score           : SHAP= 0.20%, Gain= 0.05%\n",
      "    mars_on_sun_venus_midpoint_exact: SHAP= 0.01%, Gain= 0.08%\n",
      "    mars_rahu_trine_exact_dist    : SHAP= 0.14%, Gain= 0.05%\n",
      "    mars_jupiter_quincunx         : SHAP= 0.01%, Gain= 0.08%\n",
      "    sun_venus_sextile_applying    : SHAP= 0.13%, Gain= 0.04%\n",
      "    jupiter_rahu_trine_applying   : SHAP= 0.05%, Gain= 0.01%\n",
      "    mars_jupiter_quincunx_exact   : SHAP= 0.01%, Gain= 0.08%\n",
      "    jupiter_saturn_square_strength: SHAP= 0.07%, Gain= 0.03%\n",
      "\n",
      "[6/7] Creating SHAP visualizations...\n",
      "  Creating summary plot...\n",
      "  \u2713 Saved: shap_summary_plot.png\n",
      "  Creating bar plot...\n",
      "  \u2713 Saved: shap_bar_plot.png\n",
      "  Creating category comparison...\n",
      "  \u2713 Saved: category_comparison.png\n",
      "  Creating dependence plots for top features...\n",
      "  \u2713 Saved: top3_dependence_plots.png\n",
      "\n",
      "[7/7] Saving comprehensive SHAP analysis...\n",
      "  \u2713 Saved all SHAP results to /content/drive/MyDrive/AstroFinanceProject/models/shap_analysis/\n",
      "\n",
      "======================================================================\n",
      "SHAP ANALYSIS ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "  Key Findings:\n",
      "    \u2022 Most important feature: volatility_20d\n",
      "    \u2022 Most important category: Planetary\n",
      "    \u2022 Astrological features: 53.7% importance\n",
      "    \u2022 Technical features: 15.4% importance\n",
      "    \u2022 SHAP-Gain correlation: 0.967 (HIGH agreement)\n",
      "\n",
      "  \u2705 SIGNIFICANT - Astrological features contribute meaningfully\n",
      "\n",
      "======================================================================\n",
      "PHASE 3 PART 6 (SHAP ANALYSIS) - COMPLETE \u2713\n",
      "======================================================================\n",
      "\n",
      "\ud83c\udf89 PHASE 3 COMPLETE!\n",
      "\n",
      "\ud83d\udccb All Cells Completed:\n",
      "  1. \u2713 Data preparation\n",
      "  2. \u2713 Global model training\n",
      "  3. \u2713 Sector-specific models\n",
      "  4. \u2713 Ensemble methods\n",
      "  5. \u2713 Walk-forward validation\n",
      "  6. \u2713 SHAP analysis\n",
      "\n",
      "\ud83d\udcc2 Outputs:\n",
      "  \u2022 /content/drive/MyDrive/AstroFinanceProject/models/lightgbm_improved/\n",
      "  \u2022 /content/drive/MyDrive/AstroFinanceProject/models/shap_analysis/\n",
      "\n",
      "\ud83d\udcca Model Interpretability Status: SIGNIFICANT\n",
      "======================================================================\n"
     ]
    }
   ],
   "id": "cell-0017"
  }
 ]
}